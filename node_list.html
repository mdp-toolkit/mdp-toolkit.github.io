
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Node List &#8212; Modular toolkit for Data Processing (MDP)</title>
    <link rel="stylesheet" href="_static/mdp.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '3.6',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="shortcut icon" href="_static/favicon.ico"/>
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Additional utilities" href="additional_utilities.html" />
    <link rel="prev" title="BiMDP Custom Inspection" href="examples/bimdp_examples/bimdp_custom_inspection.html" /> 
<meta name="viewport" content="width=740" />

  </head>
  <body>
<div id="header">
    <table width="100%">
	<tr>
	    <td class="td_header_left">
		<a href="https://mdp-toolkit.github.io">
		    Modular toolkit for<br />Data Processing
		</a>
	    </td>
	    <td class="td_header_right">
		<a href="examples/logo/logo_animation.html">
		    <img src="_static/logo.png" alt="MDP logo"
			 title="click to see the animated logo!" class="img_header"/>
		</a>
	    </td>
	</tr>
    </table>
    <div class="clear"></div>
</div>

      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<div class="navigation_title"><a href="index.html">Home</a></div>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="documentation.html">Documentation</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="tutorial/tutorial.html">Tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples/examples.html">Examples</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Node List</a></li>
<li class="toctree-l2"><a class="reference internal" href="additional_utilities.html">Additional utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="development.html">Development</a></li>
<li class="toctree-l2"><a class="reference external" href="https://mdp-toolkit.github.io/api/index.html">API documentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="talks/talks.html">Talks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="how_to_cite_mdp.html">How to cite MDP</a></li>
<li class="toctree-l1"><a class="reference internal" href="contact.html">Contact</a></li>
</ul>


        </div>
      </div>

    <div class="document">
   
   
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="module-mdp.nodes">
<span id="id1"></span><span id="node-list"></span><h1>Node List<a class="headerlink" href="#module-mdp.nodes" title="Permalink to this headline">¶</a></h1>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html">nodes</a></p>
<dl class="class">
<dt id="mdp.nodes.PCANode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">PCANode</code><a class="headerlink" href="#mdp.nodes.PCANode" title="Permalink to this definition">¶</a></dt>
<dd><p>Filter the input data through the most significatives of its
principal components.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>avg</strong> – Mean of the input data (available after training).</li>
<li><strong>v</strong> – Transposed of the projection matrix (available after training).</li>
<li><strong>d</strong> – Variance corresponding to the PCA components (eigenvalues of the
covariance matrix).</li>
<li><strong>explained_variance</strong> – When output_dim has been specified as a fraction
of the total variance, this is the fraction of the total variance that is
actually explained.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition-reference admonition">
<p class="first admonition-title">Reference</p>
<p class="last">More information about Principal Component Analysis, a.k.a. discrete
Karhunen-Loeve transform can be found among others in
I.T. Jolliffe, Principal Component Analysis, Springer-Verlag (1986).</p>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.PCANode-class.html">PCANode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.WhiteningNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">WhiteningNode</code><a class="headerlink" href="#mdp.nodes.WhiteningNode" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Whiten</em> the input data by filtering it through the most
significant of its principal components.</p>
<p>All output signals have zero mean, unit variance and are decorrelated.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>avg</strong> – Mean of the input data (available after training).</li>
<li><strong>v</strong> – Transpose of the projection matrix (available after training).</li>
<li><strong>d</strong> – Variance corresponding to the PCA components (eigenvalues of
the covariance matrix).</li>
<li><strong>explained_variance</strong> – When output_dim has been specified as a 
fraction of the total variance, this is the fraction of the total
variance that is actually explained.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.WhiteningNode-class.html">WhiteningNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.NIPALSNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">NIPALSNode</code><a class="headerlink" href="#mdp.nodes.NIPALSNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform Principal Component Analysis using the NIPALS algorithm.</p>
<p>This algorithm is particularly useful if you have more variables than
observations, or in general when the number of variables is huge and
calculating a full covariance matrix may be infeasible. It’s also more
efficient of the standard PCANode if you expect the number of significant
principal components to be a small. In this case setting output_dim to be
a certain fraction of the total variance, say 90%, may be of some help.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>avg</strong> – Mean of the input data (available after training).</li>
<li><strong>d</strong> – Variance corresponding to the PCA components.</li>
<li><strong>v</strong> – Transposed of the projection matrix (available after training).</li>
<li><strong>explained_variance</strong> – When output_dim has been specified as a fraction
of the total variance, this is the fraction of the total variance that is
actually explained.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition-reference admonition">
<p class="first admonition-title">Reference</p>
<p>Reference for <em>NIPALS (Nonlinear Iterative Partial Least Squares)</em>:
Wold, H.
Nonlinear estimation by iterative least squares procedures.
in David, F. (Editor), Research Papers in Statistics, Wiley,
New York, pp 411-444 (1966).</p>
<p>More information about Principal Component Analysis*, a.k.a. discrete
Karhunen-Loeve transform can be found among others in
I.T. Jolliffe, Principal Component Analysis, Springer-Verlag (1986).</p>
<p class="last">Original code contributed by:
Michael Schmuker, Susanne Lezius, and Farzad Farkhooi (2008).</p>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.NIPALSNode-class.html">NIPALSNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.FastICANode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">FastICANode</code><a class="headerlink" href="#mdp.nodes.FastICANode" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform Independent Component Analysis using the FastICA algorithm.</p>
<p>Note that FastICA is a batch-algorithm. This means that it needs
all input data before it can start and compute the ICs.
The algorithm is here given as a Node for convenience, but it
actually accumulates all inputs it receives. Remember that to avoid
running out of memory when you have many components and many time samples.</p>
<p>FastICA does not support the telescope mode (the convergence
criterium is not robust in telescope mode).
criterium is not robust in telescope mode).</p>
<p>History:</p>
<blockquote>
<div><ul class="simple">
<li>1.4.1998 created for Matlab by Jarmo Hurri, Hugo Gavert, Jaakko Sarela,
and Aapo Hyvarinen</li>
<li>7.3.2003  modified for Python by Thomas Wendler</li>
<li>3.6.2004  rewritten and adapted for scipy and MDP by MDP’s authors</li>
<li>25.5.2005 now independent from scipy. Requires Numeric or numarray</li>
<li>26.6.2006 converted to numpy</li>
<li>14.9.2007 updated to Matlab version 2.5</li>
</ul>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>white</strong> – The whitening node used for preprocessing.</li>
<li><strong>filters</strong> – The ICA filters matrix (this is the transposed of the
projection matrix after whitening).</li>
<li><strong>convergence</strong> – The value of the convergence threshold.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition-reference admonition">
<p class="first admonition-title">Reference</p>
<p class="last">Aapo Hyvarinen (1999).
Fast and Robust Fixed-Point Algorithms for Independent Component Analysis
IEEE Transactions on Neural Networks, 10(3):626-634.</p>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.FastICANode-class.html">FastICANode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.CuBICANode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">CuBICANode</code><a class="headerlink" href="#mdp.nodes.CuBICANode" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform Independent Component Analysis using the CuBICA algorithm.</p>
<p>Note that CuBICA is a batch-algorithm, which means that it needs
all input data before it can start and compute the ICs.  The
algorithm is here given as a Node for convenience, but it actually
accumulates all inputs it receives. Remember that to avoid running
out of memory when you have many components and many time samples.</p>
<p>As an alternative to this batch mode you might consider the telescope
mode (see the docs of the <code class="docutils literal"><span class="pre">__init__</span></code> method).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>white</strong> – The whitening node used for preprocessing.</li>
<li><strong>filters</strong> – The ICA filters matrix (this is the transposed of the
projection matrix after whitening).</li>
<li><strong>convergence</strong> – The value of the convergence threshold.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition-reference admonition">
<p class="first admonition-title">Reference</p>
<p class="last">Blaschke, T. and Wiskott, L. (2003).
CuBICA: Independent Component Analysis by Simultaneous Third- and
Fourth-Order Cumulant Diagonalization.
IEEE Transactions on Signal Processing, 52(5), pp. 1250-1256.</p>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.CuBICANode-class.html">CuBICANode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.TDSEPNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">TDSEPNode</code><a class="headerlink" href="#mdp.nodes.TDSEPNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform Independent Component Analysis using the TDSEP algorithm.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">That TDSEP, as implemented in this Node, is an online algorithm,
i.e. it is suited to be trained on huge data sets, provided that the
training is done sending small chunks of data for each time.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>white</strong> – The whitening node used for preprocessing.</li>
<li><strong>filters</strong> – The ICA filters matrix (this is the transposed of the
projection matrix after whitening).</li>
<li><strong>convergence</strong> – The value of the convergence threshold.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition-reference admonition">
<p class="first admonition-title">Reference</p>
<p class="last">Ziehe, Andreas and Muller, Klaus-Robert (1998).
TDSEP an efficient algorithm for blind separation using time structure.
in Niklasson, L, Boden, M, and Ziemke, T (Editors), Proc. 8th Int. Conf.
Artificial Neural Networks (ICANN 1998).</p>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.TDSEPNode-class.html">TDSEPNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.JADENode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">JADENode</code><a class="headerlink" href="#mdp.nodes.JADENode" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform Independent Component Analysis using the JADE algorithm.</p>
<p>Note that JADE is a batch-algorithm. This means that it needs
all input data before it can start and compute the ICs.
The algorithm is here given as a Node for convenience, but it
actually accumulates all inputs it receives. Remember that to avoid
running out of memory when you have many components and many time samples.</p>
<p>JADE does not support the telescope mode.</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition-reference admonition">
<p class="first admonition-title">Reference</p>
<p><em>Cardoso, Jean-Francois and Souloumiac, Antoine (1993).</em>
Blind beamforming for non Gaussian signals.
Radar and Signal Processing, IEE Proceedings F, 140(6): 362-370.</p>
<p><em>Cardoso, Jean-Francois (1999).</em>
High-order contrasts for independent component analysis.
Neural Computation, 11(1): 157-192.</p>
<p class="last">Original code contributed by: 
Gabriel Beckers (2008).</p>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition-history admonition">
<p class="first admonition-title">History</p>
<ul class="last simple">
<li>May 2005    version 1.8 for MATLAB released by Jean-Francois Cardoso</li>
<li>Dec 2007    MATLAB version 1.8 ported to Python/NumPy by Gabriel Beckers</li>
<li>Feb 15 2008 Python/NumPy version adapted for MDP by Gabriel Beckers</li>
</ul>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.JADENode-class.html">JADENode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.SFANode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">SFANode</code><a class="headerlink" href="#mdp.nodes.SFANode" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract the slowly varying components from the input data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>avg</strong> – Mean of the input data (available after training)</li>
<li><strong>sf</strong> – Matrix of the SFA filters (available after training)</li>
<li><strong>d</strong> – Delta values corresponding to the SFA components (generalized
eigenvalues). [See the docs of the <code class="docutils literal"><span class="pre">get_eta_values</span></code> method for
more information]</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition-reference admonition">
<p class="first admonition-title">Reference</p>
<p class="last">More information about Slow Feature Analysis can be found in
Wiskott, L. and Sejnowski, T.J., Slow Feature Analysis: Unsupervised
Learning of Invariances, Neural Computation, 14(4):715-770 (2002).</p>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.SFANode-class.html">SFANode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.SFA2Node">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">SFA2Node</code><a class="headerlink" href="#mdp.nodes.SFA2Node" title="Permalink to this definition">¶</a></dt>
<dd><p>Get an input signal, expand it in the space of
inhomogeneous polynomials of degree 2 and extract its slowly varying
components.</p>
<blockquote>
<div>The <code class="docutils literal"><span class="pre">get_quadratic_form</span></code> method returns the input-output</div></blockquote>
<p>function of one of the learned unit as a <code class="docutils literal"><span class="pre">QuadraticForm</span></code> object.
See the documentation of <code class="docutils literal"><span class="pre">mdp.utils.QuadraticForm</span></code> for additional
information.</p>
<div class="admonition-reference admonition">
<p class="first admonition-title">Reference:</p>
<p class="last">More information about Slow Feature Analysis can be found in
Wiskott, L. and Sejnowski, T.J., Slow Feature Analysis: Unsupervised
Learning of Invariances, Neural Computation, 14(4):715-770 (2002).</p>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.SFA2Node-class.html">SFA2Node</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.VartimeSFANode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">VartimeSFANode</code><a class="headerlink" href="#mdp.nodes.VartimeSFANode" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract the slowly varying components from the input data.
This node can be understood as a generalization of the <em>SFANode</em> that
allows non-constant time increments between samples.</p>
<p>In particular, this node numerically computes the integrals involved in
the SFA problem formulation by applying the trapezoid rule.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>avg</strong> – Mean of the input data (available after training)</li>
<li><strong>sf</strong> – Matrix of the SFA filters (available after training)</li>
<li><strong>d</strong> – Delta values corresponding to the SFA components (generalized
eigenvalues). [See the docs of the <code class="docutils literal"><span class="pre">get_eta_values</span></code> method for
more information]</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition-reference admonition">
<p class="first admonition-title">Reference</p>
<p class="last">More information about Slow Feature Analysis can be found in
Wiskott, L. and Sejnowski, T.J., Slow Feature Analysis: Unsupervised
Learning of Invariances, Neural Computation, 14(4):715-770 (2002).</p>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.VartimeSFANode-class.html">VartimeSFANode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.ISFANode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">ISFANode</code><a class="headerlink" href="#mdp.nodes.ISFANode" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform Independent Slow Feature Analysis on the input data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>RP</strong> – The global rotation-permutation matrix. This is the filter
applied on input_data to get output_data</li>
<li><strong>RPC</strong> – The <em>complete</em> global rotation-permutation matrix. This
is a matrix of dimension input_dim x input_dim (the ‘outer space’
is retained)</li>
<li><strong>covs</strong> – A <cite>mdp.utils.MultipleCovarianceMatrices</cite> instance 
input_data. After convergence the uppermost
<code class="docutils literal"><span class="pre">output_dim</span></code> x <code class="docutils literal"><span class="pre">output_dim</span></code> submatrices should be almost
diagonal.
<code class="docutils literal"><span class="pre">self.covs[n-1]</span></code> is the covariance matrix relative to the
<code class="docutils literal"><span class="pre">n</span></code>-th time-lag</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>They are not cleared after convergence. If you need to free
some memory, you can safely delete them with:</p>
<div class="last highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">covs</span>
</pre></div>
</div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>initial_contrast</strong> – A dictionary with the starting contrast and the
SFA and ICA parts of it.</li>
<li><strong>final_contrast</strong> – Like the above but after convergence.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If you intend to use this node for large datasets please have
a look at the <code class="docutils literal"><span class="pre">stop_training</span></code> method documentation for
speeding things up.</p>
</div>
<div class="admonition-reference admonition">
<p class="first admonition-title">Reference</p>
<p class="last">Blaschke, T. , Zito, T., and Wiskott, L. (2007).
Independent Slow Feature Analysis and Nonlinear Blind Source Separation.
Neural Computation 19(4):994-1021 (2007)
<a class="reference external" href="http://itb.biologie.hu-berlin.de/~wiskott/Publications/BlasZitoWisk2007-ISFA-NeurComp.pdf">http://itb.biologie.hu-berlin.de/~wiskott/Publications/BlasZitoWisk2007-ISFA-NeurComp.pdf</a></p>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.ISFANode-class.html">ISFANode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.XSFANode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">XSFANode</code><a class="headerlink" href="#mdp.nodes.XSFANode" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform Non-linear Blind Source Separation using Slow Feature Analysis.
This node is designed to iteratively extract statistically
independent sources from (in principle) arbitrary invertible
nonlinear mixtures. The method relies on temporal correlations in
the sources and consists of a combination of nonlinear SFA and a
projection algorithm. More details can be found in the reference
given below (once it’s published).</p>
<p>The node has multiple training phases. The number of training
phases depends on the number of sources that must be
extracted. The recommended way of training this node is through a
container flow:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">flow</span> <span class="o">=</span> <span class="n">mdp</span><span class="o">.</span><span class="n">Flow</span><span class="p">([</span><span class="n">XSFANode</span><span class="p">()])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">flow</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>doing so will automatically train all training phases. The argument
<code class="docutils literal"><span class="pre">x</span></code> to the <code class="docutils literal"><span class="pre">Flow.train</span></code> method can be an array or a list of iterables
(see the section about Iterators in the MDP tutorial for more info).
If the number of training samples is large, you may run into
memory problems: use data iterators and chunk training to reduce
memory usage.</p>
<p>If you need to debug training and/or execution of this node, the
suggested approach is to use the capabilities of BiMDP. For example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">flow</span> <span class="o">=</span> <span class="n">mdp</span><span class="o">.</span><span class="n">Flow</span><span class="p">([</span><span class="n">XSFANode</span><span class="p">()])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tr_filename</span> <span class="o">=</span> <span class="n">bimdp</span><span class="o">.</span><span class="n">show_training</span><span class="p">(</span><span class="n">flow</span><span class="o">=</span><span class="n">flow</span><span class="p">,</span> <span class="n">data_iterators</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ex_filename</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="n">bimdp</span><span class="o">.</span><span class="n">show_execution</span><span class="p">(</span><span class="n">flow</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>this will run training and execution with bimdp inspection. Snapshots
of the internal flow state for each training phase and execution step
will be opened in a web brower and presented as a slideshow.</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition-reference admonition">
<p class="first admonition-title">Reference</p>
<p class="last">Sprekeler, H., Zito, T., and Wiskott, L. (2009).
An Extension of Slow Feature Analysis for Nonlinear Blind Source Separation.
Journal of Machine Learning Research. 
<a class="reference external" href="http://cogprints.org/7056/1/SprekelerZitoWiskott-Cogprints-2010.pdf">http://cogprints.org/7056/1/SprekelerZitoWiskott-Cogprints-2010.pdf</a></p>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.XSFANode-class.html">XSFANode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.GSFANode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">GSFANode</code><a class="headerlink" href="#mdp.nodes.GSFANode" title="Permalink to this definition">¶</a></dt>
<dd><p>This node implements “Graph-Based SFA (GSFA)”, which is the main
component of hierarchical GSFA (HGSFA).</p>
<p>For further information, see: Escalante-B A.-N., Wiskott L, “How to solve
classification and regression problems on high-dimensional data with a
supervised extension of Slow Feature Analysis”. Journal of Machine
Learning Research 14:3683-3719, 2013</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.GSFANode-class.html">GSFANode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.iGSFANode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">iGSFANode</code><a class="headerlink" href="#mdp.nodes.iGSFANode" title="Permalink to this definition">¶</a></dt>
<dd><p>This node implements “information-preserving graph-based SFA (iGSFA)”,
which is the main component of hierarchical iGSFA (HiGSFA).</p>
<p>For further information, see: Escalante-B., A.-N. and Wiskott, L.,
“Improved graph-based {SFA}: Information preservation complements the
slowness principle”, e-print arXiv:1601.03945,
http://arxiv.org/abs/1601.03945, 2017.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.iGSFANode-class.html">iGSFANode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.FDANode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">FDANode</code><a class="headerlink" href="#mdp.nodes.FDANode" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform a (generalized) Fisher Discriminant Analysis of its
input. It is a supervised node that implements FDA using a
generalized eigenvalue approach.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>FDANode has two training phases and is supervised so make sure to
pay attention to the following points when you train it:</p>
<ul class="last simple">
<li>call the <code class="docutils literal"><span class="pre">train</span></code> method with <em>two</em> arguments: the input data
and the labels (see the doc string of the <code class="docutils literal"><span class="pre">train</span></code> method for details).</li>
<li>if you are training the node by hand, call the <code class="docutils literal"><span class="pre">train</span></code> method twice.</li>
<li>if you are training the node using a flow (recommended), the
only argument to <code class="docutils literal"><span class="pre">Flow.train</span></code> must be a list of
<code class="docutils literal"><span class="pre">(data_point,</span> <span class="pre">label)</span></code> tuples or an iterator returning lists of
such tuples, <em>not</em> a generator.  The <code class="docutils literal"><span class="pre">Flow.train</span></code> function can be
called just once as usual, since it takes care of <em>rewinding</em> the iterator
to perform the second training step.</li>
</ul>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>avg</strong> – Mean of the input data (available after training).</li>
<li><strong>v</strong> – Transposed of the projection matrix, so that
<code class="docutils literal"><span class="pre">output</span> <span class="pre">=</span> <span class="pre">dot(input-self.avg,</span> <span class="pre">self.v)</span></code> (available after training).</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition-reference admonition">
<p class="first admonition-title">Reference</p>
<p class="last">More information on Fisher Discriminant Analysis can be found for
example in C. Bishop, Neural Networks for Pattern Recognition,
Oxford Press, pp. 105-112.</p>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.FDANode-class.html">FDANode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.FANode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">FANode</code><a class="headerlink" href="#mdp.nodes.FANode" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform Factor Analysis.</p>
<p>The current implementation should be most efficient for long
data sets: the sufficient statistics are collected in the
training phase, and all EM-cycles are performed at
its end.</p>
<p>The <code class="docutils literal"><span class="pre">execute</span></code> method returns the Maximum A Posteriori estimate
of the latent variables. The <code class="docutils literal"><span class="pre">generate_input</span></code> method generates
observations from the prior distribution.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>mu</strong> – Mean of the input data (available after training).</li>
<li><strong>A</strong> – Generating weights (available after training).</li>
<li><strong>E_y_mtx</strong> – Weights for Maximum A Posteriori inference.</li>
<li><strong>sigma</strong> – Vector of estimated variance of the noise
for all input components.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition-reference admonition">
<p class="first admonition-title">Reference</p>
<p class="last">More information about Factor Analysis can be found in
Max Welling’s classnotes:
<a class="reference external" href="http://www.ics.uci.edu/~welling/classnotes/classnotes.html">http://www.ics.uci.edu/~welling/classnotes/classnotes.html</a> ,
in the chapter ‘Linear Models’.</p>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.FANode-class.html">FANode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.RBMNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">RBMNode</code><a class="headerlink" href="#mdp.nodes.RBMNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Restricted Boltzmann Machine node. An RBM is an undirected
probabilistic network with binary variables. The graph is
bipartite into observed (<em>visible</em>) and hidden (<em>latent</em>) variables.
By default, the <code class="docutils literal"><span class="pre">execute</span></code> method returns the <em>probability</em> of
one of the hiden variables being equal to 1 given the input.
Use the <code class="docutils literal"><span class="pre">sample_v</span></code> method to sample from the observed variables
given a setting of the hidden variables, and <code class="docutils literal"><span class="pre">sample_h</span></code> to do the
opposite. The <code class="docutils literal"><span class="pre">energy</span></code> method can be used to compute the energy
of a given setting of all variables.</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition-reference admonition">
<p class="first admonition-title">Reference</p>
<p>For more information on RBMs, see
Geoffrey E. Hinton (2007) Boltzmann machine. Scholarpedia, 2(5):1668</p>
<p class="last">The network is trained by Contrastive Divergence, as described in
Hinton, G. E. (2002). Training products of experts by minimizing
contrastive divergence. Neural Computation, 14(8):1711-1800</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>w</strong> – Generative weights between hidden and observed variables.</li>
<li><strong>bv</strong> – Bias vector of the observed variables.</li>
<li><strong>bh</strong> – Bias vector of the hidden variables.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.RBMNode-class.html">RBMNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.RBMWithLabelsNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">RBMWithLabelsNode</code><a class="headerlink" href="#mdp.nodes.RBMWithLabelsNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Restricted Boltzmann Machine with softmax labels. An RBM is an
undirected probabilistic network with binary variables. In this
case, the node is partitioned into a set of observed (<em>visible</em>)
variables, a set of hidden (<em>latent</em>) variables, and a set of
label variables (also observed), only one of which is active at
any time. The node is able to learn associations between the
visible variables and the labels.
By default, the <code class="docutils literal"><span class="pre">execute</span></code> method returns the <em>probability</em> of
one of the hiden variables being equal to 1 given the input.
Use the <code class="docutils literal"><span class="pre">sample_v</span></code> method to sample from the observed variables
(visible and labels) given a setting of the hidden variables, and
<code class="docutils literal"><span class="pre">sample_h</span></code> to do the opposite. The <code class="docutils literal"><span class="pre">energy</span></code> method can be used
to compute the energy of a given setting of all variables.</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition-reference admonition">
<p class="first admonition-title">Reference</p>
<p>The network is trained by Contrastive Divergence, as described in
Hinton, G. E. (2002). Training products of experts by minimizing
contrastive divergence. Neural Computation, 14(8):1711-1800</p>
<p>For more information on RBMs with labels, see:</p>
<ul class="last simple">
<li>Geoffrey E. Hinton (2007) Boltzmann machine. Scholarpedia, 2(5):1668.</li>
<li>Hinton, G. E, Osindero, S., and Teh, Y. W. (2006). A fast learning
algorithm for deep belief nets. Neural Computation, 18:1527-1554.</li>
</ul>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>w</strong> – Generative weights between hidden and observed variables.</li>
<li><strong>bv</strong> – Bias vector of the observed variables.</li>
<li><strong>bh</strong> – Bias vector of the hidden variables.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.RBMWithLabelsNode-class.html">RBMWithLabelsNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.GrowingNeuralGasNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">GrowingNeuralGasNode</code><a class="headerlink" href="#mdp.nodes.GrowingNeuralGasNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Learn the topological structure of the input data by building a
corresponding graph approximation.</p>
<p>The algorithm expands on the original Neural Gas algorithm
(see mdp.nodes NeuralGasNode) in that the algorithm adds new nodes are
added to the graph as more data becomes available. Im this way,
if the growth rate is appropriate, one can avoid overfitting  or
underfitting the data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><strong>graph</strong> – The corresponding <cite>mdp.graph.Graph</cite> object.</td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition-reference admonition">
<p class="first admonition-title">Reference</p>
<p class="last">More information about the Growing Neural Gas algorithm can be found in
B. Fritzke, A Growing Neural Gas Network Learns Topologies, in G. Tesauro,
D. S. Touretzky, and T. K. Leen (editors), Advances in Neural Information
Processing Systems 7, pages 625-632. MIT Press, Cambridge MA, 1995.</p>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.GrowingNeuralGasNode-class.html">GrowingNeuralGasNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.LLENode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">LLENode</code><a class="headerlink" href="#mdp.nodes.LLENode" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform a Locally Linear Embedding analysis on the data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>training_projection</strong> – The LLE projection of the training data
(defined when training finishes).</li>
<li><strong>desired_variance</strong> – Variance limit used to compute intrinsic
dimensionality.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Based on the algorithm outlined in <em>An Introduction to Locally
Linear Embedding</em> by L. Saul and S. Roweis, using improvements
suggested in <em>Locally Linear Embedding for Classification</em> by
D. deRidder and R.P.W. Duin.</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition-reference admonition">
<p class="first admonition-title">Reference</p>
<p class="last">Roweis, S. and Saul, L., Nonlinear dimensionality
reduction by locally linear embedding, Science 290 (5500), pp.
2323-2326, 2000.</p>
</div>
<p>Original code contributed by: Jake VanderPlas, University of Washington,</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.LLENode-class.html">LLENode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.HLLENode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">HLLENode</code><a class="headerlink" href="#mdp.nodes.HLLENode" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform a Hessian Locally Linear Embedding analysis on the data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>training_projection</strong> – The HLLE projection of the training data
(defined when training finishes).</li>
<li><strong>desired_variance</strong> – Variance limit used to compute intrinsic
dimensionality.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Many methods are inherited from LLENode, including _execute(),
_adjust_output_dim(), etc. The main advantage of the Hessian estimator
is to limit distortions of the input manifold.  Once the model has been
trained, it is sufficient (and much less computationally intensive)
to determine projections for new points using the LLE framework.</p>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition-reference admonition">
<p class="first admonition-title">Reference</p>
<p>Implementation based on algorithm outlined in
Donoho, D. L., and Grimes, C., Hessian Eigenmaps: new locally linear
embedding techniques for high-dimensional data, Proceedings of the
National Academy of Sciences 100(10): 5591-5596, 2003.</p>
<p class="last">Original code contributed by: Jake Vanderplas, University of Washington</p>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.HLLENode-class.html">HLLENode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.LinearRegressionNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">LinearRegressionNode</code><a class="headerlink" href="#mdp.nodes.LinearRegressionNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute least-square, multivariate linear regression on the input
data, i.e., learn coefficients <code class="docutils literal"><span class="pre">b_j</span></code> so that the linear combination
<code class="docutils literal"><span class="pre">y_i</span> <span class="pre">=</span> <span class="pre">b_0</span> <span class="pre">+</span> <span class="pre">b_1</span> <span class="pre">x_1</span> <span class="pre">+</span> <span class="pre">...</span> <span class="pre">b_N</span> <span class="pre">x_N</span></code> , for <code class="docutils literal"><span class="pre">i</span> <span class="pre">=</span> <span class="pre">1</span> <span class="pre">...</span> <span class="pre">M</span></code>, minimizes
the sum of squared error given the training <code class="docutils literal"><span class="pre">x</span></code>’s and <code class="docutils literal"><span class="pre">y</span></code>’s.</p>
<p>This is a supervised learning node, and requires input data <code class="docutils literal"><span class="pre">x</span></code> and
target data <code class="docutils literal"><span class="pre">y</span></code> to be supplied during training (see <code class="docutils literal"><span class="pre">train</span></code>
docstring).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><strong>beta</strong> – The coefficients of the linear regression.</td>
</tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.LinearRegressionNode-class.html">LinearRegressionNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.QuadraticExpansionNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">QuadraticExpansionNode</code><a class="headerlink" href="#mdp.nodes.QuadraticExpansionNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform expansion in the space formed by all linear and quadratic
monomials.
<code class="docutils literal"><span class="pre">QuadraticExpansionNode()</span></code> is equivalent to a
<code class="docutils literal"><span class="pre">PolynomialExpansionNode(2)</span></code></p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.QuadraticExpansionNode-class.html">QuadraticExpansionNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.PolynomialExpansionNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">PolynomialExpansionNode</code><a class="headerlink" href="#mdp.nodes.PolynomialExpansionNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform expansion in a polynomial space.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.PolynomialExpansionNode-class.html">PolynomialExpansionNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.RBFExpansionNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">RBFExpansionNode</code><a class="headerlink" href="#mdp.nodes.RBFExpansionNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Expand input space with Gaussian Radial Basis Functions (RBFs).</p>
<p>The input data is filtered through a set of unnormalized Gaussian
filters, i.e.:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">y_j</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="o">/</span><span class="n">s_j</span> <span class="o">*</span> <span class="o">||</span><span class="n">x</span> <span class="o">-</span> <span class="n">c_j</span><span class="o">||^</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>for isotropic RBFs, or more in general:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">y_j</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">c_j</span><span class="p">)</span><span class="o">^</span><span class="n">T</span> <span class="n">S</span><span class="o">^-</span><span class="mi">1</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">c_j</span><span class="p">))</span>
</pre></div>
</div>
<p>for anisotropic RBFs.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.RBFExpansionNode-class.html">RBFExpansionNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.GeneralExpansionNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">GeneralExpansionNode</code><a class="headerlink" href="#mdp.nodes.GeneralExpansionNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Expands the input samples by applying to them one or more functions provided.</p>
<p>The functions to be applied are specified by a list [f_0, …, f_k], where 
f_i, for 0 &lt;= i &lt;= k, denotes a particular function. 
The input data given to these functions is a two-dimensional array and
the output is another two-dimensional array. The dimensionality of the output 
should depend only on the dimensionality of the input.
Given a two-dimensional input array x, the output of the node 
is then [f_0(x), …, f_k(x)], that is, the concatenation of each one of 
the computed arrays f_i(x).</p>
<p>This node has been designed to facilitate nonlinear, fixed but arbitrary  
transformations of the data samples within MDP flows.</p>
<p>Original code contributed by Alberto Escalante.</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition-example admonition">
<p class="first admonition-title">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mdp</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mdp</span> <span class="kn">import</span> <span class="n">numx</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">identity</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">u3</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">numx</span><span class="o">.</span><span class="n">absolute</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mi">3</span> <span class="c1">#A simple nonlinear transformation</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">norm2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="c1">#Computes the norm of each sample returning an Nx1 array</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">return</span> <span class="p">((</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> 
</pre></div>
</div>
<div class="last highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">numx</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gen</span> <span class="o">=</span> <span class="n">mdp</span><span class="o">.</span><span class="n">nodes</span><span class="o">.</span><span class="n">GeneralExpansionNode</span><span class="p">(</span><span class="n">funcs</span><span class="o">=</span><span class="p">[</span><span class="n">identity</span><span class="p">,</span> <span class="n">u3</span><span class="p">,</span> <span class="n">norm2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">gen</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="o">-</span><span class="mf">2.</span>          <span class="mf">2.</span>          <span class="mf">8.</span>          <span class="mf">8.</span>          <span class="mf">2.82842712</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span> <span class="p">[</span> <span class="mf">0.2</span>         <span class="mf">0.3</span>         <span class="mf">0.008</span>       <span class="mf">0.027</span>       <span class="mf">0.36055513</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span> <span class="p">[</span> <span class="mf">0.6</span>         <span class="mf">1.2</span>         <span class="mf">0.216</span>       <span class="mf">1.728</span>       <span class="mf">1.34164079</span><span class="p">]]</span>
</pre></div>
</div>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.GeneralExpansionNode-class.html">GeneralExpansionNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.GrowingNeuralGasExpansionNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">GrowingNeuralGasExpansionNode</code><a class="headerlink" href="#mdp.nodes.GrowingNeuralGasExpansionNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform a trainable radial basis expansion, where the centers and
sizes of the basis functions are learned through a growing neural
gas.</p>
<p>The positions of RBFs correspond to position of the nodes of the neural gas
The sizes of the RBFs correspond to mean distance to the neighbouring nodes.</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Adjust the maximum number of nodes to control the
dimension of the expansion.</p>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition-reference admonition">
<p class="first admonition-title">Reference</p>
<p class="last">More information on this expansion type can be found in:
B. Fritzke.
Growing cell structures-a self-organizing network for unsupervised
and supervised learning. Neural Networks 7, p. 1441–1460 (1994).</p>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.GrowingNeuralGasExpansionNode-class.html">GrowingNeuralGasExpansionNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.NeuralGasNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">NeuralGasNode</code><a class="headerlink" href="#mdp.nodes.NeuralGasNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Learn the topological structure of the input data by building a
corresponding graph approximation (original Neural Gas algorithm).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>graph</strong> – The corresponding <cite>mdp.graph.Graph</cite> object.</li>
<li><strong>max_epochs</strong> – Maximum number of epochs until which to train.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition-reference admonition">
<p class="first admonition-title">Reference</p>
<p class="last">The Neural Gas algorithm was originally published in Martinetz, T. and
Schulten, K.: A “Neural-Gas” Network Learns Topologies. In Kohonen, T.,
Maekisara, K., Simula, O., and Kangas, J. (eds.), Artificial Neural
Networks. Elsevier, North-Holland., 1991.</p>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.NeuralGasNode-class.html">NeuralGasNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.SignumClassifier">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">SignumClassifier</code><a class="headerlink" href="#mdp.nodes.SignumClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>This classifier node classifies as <code class="docutils literal"><span class="pre">1</span></code> if the sum of the data points
is positive and as <code class="docutils literal"><span class="pre">-1</span></code> if the data point is negative.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.SignumClassifier-class.html">SignumClassifier</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.PerceptronClassifier">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">PerceptronClassifier</code><a class="headerlink" href="#mdp.nodes.PerceptronClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>A simple perceptron with input_dim input nodes.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.PerceptronClassifier-class.html">PerceptronClassifier</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.SimpleMarkovClassifier">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">SimpleMarkovClassifier</code><a class="headerlink" href="#mdp.nodes.SimpleMarkovClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>A simple version of a Markov classifier.</p>
<p>It can be trained on a vector of tuples the label being the next element
in the testing data.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.SimpleMarkovClassifier-class.html">SimpleMarkovClassifier</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.DiscreteHopfieldClassifier">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">DiscreteHopfieldClassifier</code><a class="headerlink" href="#mdp.nodes.DiscreteHopfieldClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Node for simulating a simple discrete Hopfield model</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.DiscreteHopfieldClassifier-class.html">DiscreteHopfieldClassifier</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.KMeansClassifier">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">KMeansClassifier</code><a class="headerlink" href="#mdp.nodes.KMeansClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Employs K-Means Clustering for a given number of centroids.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.KMeansClassifier-class.html">KMeansClassifier</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.NormalizeNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">NormalizeNode</code><a class="headerlink" href="#mdp.nodes.NormalizeNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Make input signal meanfree and unit variance.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.NormalizeNode-class.html">NormalizeNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.GaussianClassifier">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">GaussianClassifier</code><a class="headerlink" href="#mdp.nodes.GaussianClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform a supervised Gaussian classification.</p>
<p>Given a set of labelled data, the node fits a gaussian distribution
to each class.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.GaussianClassifier-class.html">GaussianClassifier</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.NearestMeanClassifier">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">NearestMeanClassifier</code><a class="headerlink" href="#mdp.nodes.NearestMeanClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Nearest-Mean classifier.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.NearestMeanClassifier-class.html">NearestMeanClassifier</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.KNNClassifier">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">KNNClassifier</code><a class="headerlink" href="#mdp.nodes.KNNClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>K-Nearest-Neighbour Classifier.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.KNNClassifier-class.html">KNNClassifier</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.EtaComputerNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">EtaComputerNode</code><a class="headerlink" href="#mdp.nodes.EtaComputerNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the eta values of the normalized training data.</p>
<p>The delta value of a signal is a measure of its temporal
variation, and is defined as the mean of the derivative squared,
i.e. <code class="docutils literal"><span class="pre">delta(x)</span> <span class="pre">=</span> <span class="pre">mean(dx/dt(t)^2)</span></code>.  <code class="docutils literal"><span class="pre">delta(x)</span></code> is zero if
<code class="docutils literal"><span class="pre">x</span></code> is a constant signal, and increases if the temporal variation
of the signal is bigger.</p>
<p>The eta value is a more intuitive measure of temporal variation,
defined as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">eta</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">T</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">pi</span><span class="p">)</span> <span class="o">*</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">delta</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
<p>If <code class="docutils literal"><span class="pre">x</span></code> is a signal of length <code class="docutils literal"><span class="pre">T</span></code> which consists of a sine function
that accomplishes exactly <code class="docutils literal"><span class="pre">N</span></code> oscillations, then <code class="docutils literal"><span class="pre">eta(x)=N</span></code>.</p>
<p><code class="docutils literal"><span class="pre">EtaComputerNode</span></code> normalizes the training data to have unit
variance, such that it is possible to compare the temporal
variation of two signals independently from their scaling.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<ul class="last simple">
<li>If a data chunk is tlen data points long, this node is
going to consider only the first tlen-1 points together with their
derivatives. This means in particular that the variance of the
signal is not computed on all data points. This behavior is
compatible with that of <code class="docutils literal"><span class="pre">SFANode</span></code>.</li>
<li>This is an analysis node, i.e. the data is analyzed during training
and the results are stored internally.  Use the method
<code class="docutils literal"><span class="pre">get_eta</span></code> to access them.</li>
</ul>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition-reference admonition">
<p class="first admonition-title">Reference</p>
<p class="last"><em>Wiskott, L. and Sejnowski, T.J. (2002)</em>.
Slow Feature Analysis: Unsupervised Learning of Invariances,
Neural Computation, 14(4):715-770.</p>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.EtaComputerNode-class.html">EtaComputerNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.HitParadeNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">HitParadeNode</code><a class="headerlink" href="#mdp.nodes.HitParadeNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Collect the first <code class="docutils literal"><span class="pre">n</span></code> local maxima and minima of the training signal
which are separated by a minimum gap <code class="docutils literal"><span class="pre">d</span></code>.</p>
<p>This is an analysis node, i.e. the data is analyzed during training
and the results are stored internally. Use the
<code class="docutils literal"><span class="pre">get_maxima</span></code> and <code class="docutils literal"><span class="pre">get_minima</span></code> methods to access them.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.HitParadeNode-class.html">HitParadeNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.NoiseNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">NoiseNode</code><a class="headerlink" href="#mdp.nodes.NoiseNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Inject multiplicative or additive noise into the input data.</p>
<p>Original code contributed by Mathias Franzius.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.NoiseNode-class.html">NoiseNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.NormalNoiseNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">NormalNoiseNode</code><a class="headerlink" href="#mdp.nodes.NormalNoiseNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Special version of <code class="docutils literal"><span class="pre">NoiseNode</span></code> for Gaussian additive noise.</p>
<p>Unlike <code class="docutils literal"><span class="pre">NoiseNode</span></code> it does not store a noise function reference but simply
uses <code class="docutils literal"><span class="pre">numx_rand.normal</span></code>.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.NormalNoiseNode-class.html">NormalNoiseNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.TimeFramesNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">TimeFramesNode</code><a class="headerlink" href="#mdp.nodes.TimeFramesNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Copy delayed version of the input signal on the space dimensions.</p>
<p>For example, for <code class="docutils literal"><span class="pre">time_frames=3</span></code> and <code class="docutils literal"><span class="pre">gap=2</span></code>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">[</span> <span class="n">X</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>        <span class="p">[</span> <span class="n">X</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="n">X</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="n">X</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
  <span class="n">X</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>          <span class="n">X</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="n">X</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span> <span class="n">X</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
  <span class="n">X</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>   <span class="o">--&gt;</span>    <span class="n">X</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="n">X</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="n">X</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
  <span class="n">X</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>          <span class="n">X</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span> <span class="n">X</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span> <span class="n">X</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span>
  <span class="n">X</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>          <span class="o">...</span>  <span class="o">...</span>  <span class="o">...</span>  <span class="o">...</span>  <span class="o">...</span>  <span class="o">...</span> <span class="p">]</span>
  <span class="n">X</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
  <span class="n">X</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
  <span class="n">X</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span>
  <span class="o">...</span>  <span class="o">...</span>  <span class="p">]</span>
</pre></div>
</div>
<p>It is not always possible to invert this transformation (the
transformation is not surjective. However, the <code class="docutils literal"><span class="pre">pseudo_inverse</span></code>
method does the correct thing when it is indeed possible.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.TimeFramesNode-class.html">TimeFramesNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.TimeDelayNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">TimeDelayNode</code><a class="headerlink" href="#mdp.nodes.TimeDelayNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Copy delayed version of the input signal on the space dimensions.</p>
<p>For example, for <code class="docutils literal"><span class="pre">time_frames=3</span></code> and <code class="docutils literal"><span class="pre">gap=2</span></code>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">[</span> <span class="n">X</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>        <span class="p">[</span> <span class="n">X</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>   <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>
  <span class="n">X</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>          <span class="n">X</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>   <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>
  <span class="n">X</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>   <span class="o">--&gt;</span>    <span class="n">X</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="n">X</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>   <span class="mi">0</span>    <span class="mi">0</span>
  <span class="n">X</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>          <span class="n">X</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span> <span class="n">X</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>   <span class="mi">0</span>    <span class="mi">0</span>
  <span class="n">X</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>          <span class="n">X</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="n">X</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="n">X</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">X</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>          <span class="o">...</span>  <span class="o">...</span>  <span class="o">...</span>  <span class="o">...</span>  <span class="o">...</span>  <span class="o">...</span> <span class="p">]</span>
  <span class="n">X</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
  <span class="n">X</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span> <span class="n">Y</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span>
  <span class="o">...</span>  <span class="o">...</span>  <span class="p">]</span>
</pre></div>
</div>
<p>This node provides similar functionality as the <code class="docutils literal"><span class="pre">TimeFramesNode</span></code>, only
that it performs a time embedding into the past rather than into the future.</p>
<p>See <code class="docutils literal"><span class="pre">TimeDelaySlidingWindowNode</span></code> for a sliding window delay node for
application in a non-batch manner.</p>
<p>Original code contributed by Sebastian Hoefer.
Dec 31, 2010</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.TimeDelayNode-class.html">TimeDelayNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.TimeDelaySlidingWindowNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">TimeDelaySlidingWindowNode</code><a class="headerlink" href="#mdp.nodes.TimeDelaySlidingWindowNode" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal"><span class="pre">TimeDelaySlidingWindowNode</span></code> is an alternative to <code class="docutils literal"><span class="pre">TimeDelayNode</span></code>
which should be used for online learning/execution. Whereas the
<code class="docutils literal"><span class="pre">TimeDelayNode</span></code> works in a batch manner, for online application
a sliding window is necessary which yields only one row per call.</p>
<p>Applied to the same data the collection of all returned rows of the
<code class="docutils literal"><span class="pre">TimeDelaySlidingWindowNode</span></code> is equivalent to the result of the
<code class="docutils literal"><span class="pre">TimeDelayNode</span></code>.</p>
<p>Original code contributed by Sebastian Hoefer.
Dec 31, 2010</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.TimeDelaySlidingWindowNode-class.html">TimeDelaySlidingWindowNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.CutoffNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">CutoffNode</code><a class="headerlink" href="#mdp.nodes.CutoffNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Node to cut off values at specified bounds.</p>
<p>Works similar to <code class="docutils literal"><span class="pre">numpy.clip</span></code>, but also works when only a lower or upper
bound is specified.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.CutoffNode-class.html">CutoffNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.AdaptiveCutoffNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">AdaptiveCutoffNode</code><a class="headerlink" href="#mdp.nodes.AdaptiveCutoffNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Node which uses the data history during training to learn cutoff values.</p>
<p>As opposed to the simple <code class="docutils literal"><span class="pre">CutoffNode</span></code>, a different cutoff value is learned
for each data coordinate. For example if an upper cutoff fraction of
0.05 is specified, then the upper cutoff bound is set so that the upper
5% of the training data would have been clipped (in each dimension).
The cutoff bounds are then applied during execution.
This node also works as a <code class="docutils literal"><span class="pre">HistogramNode</span></code>, so the histogram data is stored.</p>
<p>When <code class="docutils literal"><span class="pre">stop_training</span></code> is called the cutoff values for each coordinate are
calculated based on the collected histogram data.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.AdaptiveCutoffNode-class.html">AdaptiveCutoffNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.HistogramNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">HistogramNode</code><a class="headerlink" href="#mdp.nodes.HistogramNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Node which stores a history of the data during its training phase.</p>
<p>The data history is stored in <code class="docutils literal"><span class="pre">self.data_hist</span></code> and can also be deleted to
free memory. Alternatively it can be automatically pickled to disk.</p>
<p>Note that data is only stored during training.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.HistogramNode-class.html">HistogramNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.IdentityNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">IdentityNode</code><a class="headerlink" href="#mdp.nodes.IdentityNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Execute returns the input data and the node is not trainable.</p>
<p>This node can be instantiated and is for example useful in
complex network layouts.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.IdentityNode-class.html">IdentityNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.OnlineCenteringNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">OnlineCenteringNode</code><a class="headerlink" href="#mdp.nodes.OnlineCenteringNode" title="Permalink to this definition">¶</a></dt>
<dd><p>OnlineCenteringNode centers the input data, that is, subtracts the arithmetic mean (average) from the
input data. This is an online learnable node.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>The node’s train method updates the average (avg) according to the update rule:</p>
<p>avg &lt;- (1 / n) * x + (1-1/n) * avg, where n is the total number of samples observed while training.</p>
<p>The node’s execute method subtracts the updated average from the input and returns it.</p>
<p>This node also supports centering via an exponentially weighted moving average that resembles a leaky
integrator:</p>
<p>avg &lt;- alpha * x + (1-alpha) * avg, where alpha = 2. / (avg_n + 1).</p>
<p class="last">avg_n intuitively denotes a “window size”. For a large avg_n, ‘avg_n’-samples represent about 86% of
the total weight.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><strong>avg</strong> – The updated average of the input data.</td>
</tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.OnlineCenteringNode-class.html">OnlineCenteringNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.OnlineTimeDiffNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">OnlineTimeDiffNode</code><a class="headerlink" href="#mdp.nodes.OnlineTimeDiffNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the discrete time derivative of the input using backward difference approximation:</p>
<p>dx(n) = x(n) - x(n-1), where n is the total number of input samples observed during training.</p>
<p>This is an online learnable node that uses a buffer to store the previous input sample = x(n-1). The node’s train
method updates the buffer. The node’s execute method returns the time difference using the stored buffer
as its previous input sample x(n-1).</p>
<p>This node supports both “incremental” and “batch” training types.</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition-example admonition">
<p class="first admonition-title">Example</p>
<dl class="docutils">
<dt>If the training and execute methods are called sample by sample incrementally::</dt>
<dd>train(x[1]), y[1]=execute(x[1]), train(x[2]), y[2]=execute(x[2]), …,</dd>
<dt>then::</dt>
<dd>y[1] = x[1]
y[2] = x[2] - x[1]
y[3] = x[3] - x[2]
…</dd>
<dt>If training and execute methods are called block by block::</dt>
<dd>train([x[1], x[2], x[3]]), [y[3], y[4], y[5]] = execute([x[3], x[4], x[5]])</dd>
<dt>then::</dt>
<dd>y[3] = x[3] - x[2]
y[4] = x[4] - x[3]
y[5] = x[5] - x[4]</dd>
</dl>
<p class="last">Note that the stored buffer is still = x[2]. Only train() method changes the state of the node.
execute’s input data is always assumed to start at get_current_train_iteration() time step.</p>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.OnlineTimeDiffNode-class.html">OnlineTimeDiffNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.CCIPCANode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">CCIPCANode</code><a class="headerlink" href="#mdp.nodes.CCIPCANode" title="Permalink to this definition">¶</a></dt>
<dd><p>Candid-Covariance free Incremental Principal Component Analysis (CCIPCA)
extracts the principal components from the input data incrementally.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>v</strong> – Eigenvectors</li>
<li><strong>d</strong> – Eigenvalues</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition-reference admonition">
<p class="first admonition-title">Reference</p>
<p class="last">More information about Candid-Covariance free Incremental Principal
Component Analysis can be found in Weng J., Zhang Y. and Hwang W.,
Candid covariance-free incremental principal component analysis,
IEEE Trans. Pattern Analysis and Machine Intelligence,
vol. 25, 1034–1040, 2003.</p>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.CCIPCANode-class.html">CCIPCANode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.CCIPCAWhiteningNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">CCIPCAWhiteningNode</code><a class="headerlink" href="#mdp.nodes.CCIPCAWhiteningNode" title="Permalink to this definition">¶</a></dt>
<dd><dl class="docutils">
<dt>Incrementally updates whitening vectors for the input data using CCIPCA.</dt>
<dd></dd>
</dl>
<p>Candid-Covariance free Incremental Principal Component Analysis (CCIPCA)
extracts the principal components from the input data incrementally.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>v</strong> – Eigenvectors</li>
<li><strong>d</strong> – Eigenvalues</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition-reference admonition">
<p class="first admonition-title">Reference</p>
<p class="last">More information about Candid-Covariance free Incremental Principal
Component Analysis can be found in Weng J., Zhang Y. and Hwang W.,
Candid covariance-free incremental principal component analysis,
IEEE Trans. Pattern Analysis and Machine Intelligence,
vol. 25, 1034–1040, 2003.</p>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.CCIPCAWhiteningNode-class.html">CCIPCAWhiteningNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.MCANode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">MCANode</code><a class="headerlink" href="#mdp.nodes.MCANode" title="Permalink to this definition">¶</a></dt>
<dd><p>Minor Component Analysis (MCA) extracts minor components (dual of principal
components) from the input data incrementally.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>v</strong> – Eigenvectors</li>
<li><strong>d</strong> – Eigenvalues</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition-reference admonition">
<p class="first admonition-title">Reference</p>
<p class="last">More information about MCA can be found in
Peng, D. and Yi, Z, A new algorithm for sequential minor component
analysis, International Journal of Computational Intelligence Research,
2(2):207–215, 2006.</p>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.MCANode-class.html">MCANode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.IncSFANode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">IncSFANode</code><a class="headerlink" href="#mdp.nodes.IncSFANode" title="Permalink to this definition">¶</a></dt>
<dd><p>Incremental Slow Feature Analysis (IncSFA) extracts the slowly varying
components from the input data incrementally.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>sf</strong> – Slow feature vectors</li>
<li><strong>wv</strong> – Whitening vectors</li>
<li><strong>sf_change</strong> – Difference in slow features after update</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition-reference admonition">
<p class="first admonition-title">Reference</p>
<p class="last">More information about IncSFA
can be found in Kompella V.R, Luciw M. and Schmidhuber J., Incremental Slow
Feature Analysis: Adaptive Low-Complexity Slow Feature Updating from
High-Dimensional Input Streams, Neural Computation, 2012.</p>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.IncSFANode-class.html">IncSFANode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.RecursiveExpansionNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">RecursiveExpansionNode</code><a class="headerlink" href="#mdp.nodes.RecursiveExpansionNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Recursively computable (orthogonal) expansions.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>lower</strong> – The lower bound of the domain on which the recursion function
is defined or orthogonal.</li>
<li><strong>upper</strong> – The upper bound of the domain on which the recursion function
is defined or orthogonal.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.RecursiveExpansionNode-class.html">RecursiveExpansionNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.NormalizingRecursiveExpansionNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">NormalizingRecursiveExpansionNode</code><a class="headerlink" href="#mdp.nodes.NormalizingRecursiveExpansionNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Recursively computable (orthogonal) expansions and a
trainable transformation to the domain of the expansions.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>lower</strong> – The lower bound of the domain on which the recursion
function is defined or orthogonal.</li>
<li><strong>upper</strong> – The upper bound of the domain on which the recursion function
is defined or orthogonal.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.NormalizingRecursiveExpansionNode-class.html">NormalizingRecursiveExpansionNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.Convolution2DNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">Convolution2DNode</code><a class="headerlink" href="#mdp.nodes.Convolution2DNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Convolve input data with filter banks.</p>
<p>Convolution can be selected to be executed by linear filtering of the data, or
in the frequency domain using a Discrete Fourier Transform.</p>
<p>Input data can be given as 3D data, each row being a 2D array
to be convolved with the filters, or as 2D data, in which case
the <code class="docutils literal"><span class="pre">input_shape</span></code> argument must be specified.</p>
<p>This node depends on <code class="docutils literal"><span class="pre">scipy</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><strong>filters</strong> – Specifies a set of 2D filters that are
convolved with the input data during execution.</td>
</tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.Convolution2DNode-class.html">Convolution2DNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.SGDRegressorScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">SGDRegressorScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.SGDRegressorScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Linear model fitted by minimizing a regularized empirical loss with SGD
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.stochastic_gradient.SGDRegressor</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
SGD stands for Stochastic Gradient Descent: the gradient of the loss is
estimated each sample at a time and the model is updated along the way with
a decreasing strength schedule (aka learning rate).</p>
<p>The regularizer is a penalty added to the loss function that shrinks model
parameters towards the zero vector using either the squared euclidean norm
L2 or the absolute norm L1 or a combination of both (Elastic Net). If the
parameter update crosses the 0.0 value because of the regularizer, the
update is truncated to 0.0 to allow for learning sparse models and achieve
online feature selection.</p>
<p>This implementation works with data represented as dense numpy arrays of
floating point values for the features.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>loss <span class="classifier-delimiter">:</span> <span class="classifier">str, default: ‘squared_loss’</span></dt>
<dd><p class="first">The loss function to be used. The possible values are ‘squared_loss’,
‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’</p>
<p class="last">The ‘squared_loss’ refers to the ordinary least squares fit.
‘huber’ modifies ‘squared_loss’ to focus less on getting outliers
correct by switching from squared to linear loss past a distance of
epsilon. ‘epsilon_insensitive’ ignores errors less than epsilon and is
linear past that; this is the loss function used in SVR.
‘squared_epsilon_insensitive’ is the same but becomes squared loss past
a tolerance of epsilon.</p>
</dd>
<dt>penalty <span class="classifier-delimiter">:</span> <span class="classifier">str, ‘none’, ‘l2’, ‘l1’, or ‘elasticnet’</span></dt>
<dd>The penalty (aka regularization term) to be used. Defaults to ‘l2’
which is the standard regularizer for linear SVM models. ‘l1’ and
‘elasticnet’ might bring sparsity to the model (feature selection)
not achievable with ‘l2’.</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Constant that multiplies the regularization term. Defaults to 0.0001
Also used to compute learning_rate when set to ‘optimal’.</dd>
<dt>l1_ratio <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The Elastic Net mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.
l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.
Defaults to 0.15.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Whether the intercept should be estimated or not. If False, the
data is assumed to be already centered. Defaults to True.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd><p class="first">The maximum number of passes over the training data (aka epochs).
It only impacts the behavior in the <code class="docutils literal"><span class="pre">fit</span></code> method, and not the
<cite>partial_fit</cite>.
Defaults to 5. Defaults to 1000 from 0.21, or if tol is not None.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19.</span></p>
</div>
</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float or None, optional</span></dt>
<dd><p class="first">The stopping criterion. If it is not None, the iterations will stop
when (loss &gt; previous_loss - tol). Defaults to None.
Defaults to 1e-3 from 0.21.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19.</span></p>
</div>
</dd>
<dt>shuffle <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd>Whether or not the training data should be shuffled after each epoch.
Defaults to True.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span></dt>
<dd>The verbosity level.</dd>
<dt>epsilon <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Epsilon in the epsilon-insensitive loss functions; only if <cite>loss</cite> is
‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’.
For ‘huber’, determines the threshold at which it becomes less
important to get the prediction exactly right.
For epsilon-insensitive, any differences between the current prediction
and the correct label are ignored if they are less than this threshold.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>The seed of the pseudo random number generator to use when shuffling
the data.  If int, random_state is the seed used by the random number
generator; If RandomState instance, random_state is the random number
generator; If None, the random number generator is the RandomState
instance used by <cite>np.random</cite>.</dd>
<dt>learning_rate <span class="classifier-delimiter">:</span> <span class="classifier">string, optional</span></dt>
<dd><p class="first">The learning rate schedule:</p>
<p>‘constant’:</p>
<blockquote>
<div><ul class="simple">
<li>eta = eta0</li>
</ul>
</div></blockquote>
<p>‘optimal’:</p>
<blockquote>
<div><ul class="simple">
<li>eta = 1.0 / (alpha * (t + t0))</li>
<li>where t0 is chosen by a heuristic proposed by Leon Bottou.</li>
</ul>
</div></blockquote>
<dl class="docutils">
<dt>‘invscaling’: [default]</dt>
<dd>eta = eta0 / pow(t, power_t)</dd>
</dl>
<p>‘adaptive’:</p>
<blockquote class="last">
<div><ul class="simple">
<li>eta = eta0, as long as the training keeps decreasing.</li>
<li>Each time n_iter_no_change consecutive epochs fail to decrease the</li>
<li>training loss by tol or fail to increase validation score by tol if</li>
<li>early_stopping is True, the current learning rate is divided by 5.</li>
</ul>
</div></blockquote>
</dd>
<dt>eta0 <span class="classifier-delimiter">:</span> <span class="classifier">double</span></dt>
<dd>The initial learning rate for the ‘constant’, ‘invscaling’ or
‘adaptive’ schedules. The default value is 0.0 as eta0 is not used by
the default schedule ‘optimal’.</dd>
<dt>power_t <span class="classifier-delimiter">:</span> <span class="classifier">double</span></dt>
<dd>The exponent for inverse scaling learning rate [default 0.5].</dd>
<dt>early_stopping <span class="classifier-delimiter">:</span> <span class="classifier">bool, default=False</span></dt>
<dd><p class="first">Whether to use early stopping to terminate training when validation
score is not improving. If set to True, it will automatically set aside
a fraction of training data as validation and terminate training when
validation score is not improving by at least tol for
n_iter_no_change consecutive epochs.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
<dt>validation_fraction <span class="classifier-delimiter">:</span> <span class="classifier">float, default=0.1</span></dt>
<dd><p class="first">The proportion of training data to set aside as validation set for
early stopping. Must be between 0 and 1.
Only used if early_stopping is True.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
<dt>n_iter_no_change <span class="classifier-delimiter">:</span> <span class="classifier">int, default=5</span></dt>
<dd><p class="first">Number of iterations with no improvement to wait before early stopping.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd><p class="first">When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.
See <span class="xref std std-term">the Glossary</span>.</p>
<p class="last">Repeatedly calling fit or partial_fit when warm_start is True can
result in a different solution than when calling fit a single time
because of the way the data is shuffled.
If a dynamic learning rate is used, the learning rate is adapted
depending on the number of samples already seen. Calling <code class="docutils literal"><span class="pre">fit</span></code> resets
this counter, while <code class="docutils literal"><span class="pre">partial_fit</span></code>  will result in increasing the
existing counter.</p>
</dd>
<dt>average <span class="classifier-delimiter">:</span> <span class="classifier">bool or int, optional</span></dt>
<dd>When set to True, computes the averaged SGD weights and stores the
result in the <code class="docutils literal"><span class="pre">coef_</span></code> attribute. If set to an int greater than 1,
averaging will begin once the total number of samples seen reaches
average. So <code class="docutils literal"><span class="pre">average=10</span></code> will begin averaging after seeing 10
samples.</dd>
<dt>n_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd><p class="first">The number of passes over the training data (aka epochs).
Defaults to None. Deprecated, will be removed in 0.21.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.19: </span>Deprecated</p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,)</span></dt>
<dd>Weights assigned to the features.</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (1,)</span></dt>
<dd>The intercept term.</dd>
<dt><code class="docutils literal"><span class="pre">average_coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,)</span></dt>
<dd>Averaged weights assigned to the features.</dd>
<dt><code class="docutils literal"><span class="pre">average_intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (1,)</span></dt>
<dd>The averaged intercept term.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The actual number of iterations to reach the stopping criterion.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">SGDRegressor</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">SGDRegressor(alpha=0.0001, average=False, early_stopping=False,</span>
<span class="go">       epsilon=0.1, eta0=0.01, fit_intercept=True, l1_ratio=0.15,</span>
<span class="go">       learning_rate=&#39;invscaling&#39;, loss=&#39;squared_loss&#39;, max_iter=1000,</span>
<span class="go">       n_iter=None, n_iter_no_change=5, penalty=&#39;l2&#39;, power_t=0.25,</span>
<span class="go">       random_state=None, shuffle=True, tol=0.001, validation_fraction=0.1,</span>
<span class="go">       verbose=0, warm_start=False)</span>
</pre></div>
</div>
<p>See also</p>
<p>Ridge, ElasticNet, Lasso, sklearn.svm.SVR</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.SGDRegressorScikitsLearnNode-class.html">SGDRegressorScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.PatchExtractorScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">PatchExtractorScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.PatchExtractorScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Extracts patches from a collection of images
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.feature_extraction.image.PatchExtractor</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>patch_size <span class="classifier-delimiter">:</span> <span class="classifier">tuple of ints (patch_height, patch_width)</span></dt>
<dd>the dimensions of one patch</dd>
<dt>max_patches <span class="classifier-delimiter">:</span> <span class="classifier">integer or float, optional default is None</span></dt>
<dd>The maximum number of patches per image to extract. If max_patches is a
float in (0, 1), it is taken to mean a proportion of the total number
of patches.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
</dl>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#PatchExtractorScikitsLearnNode">PatchExtractorScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.TheilSenRegressorScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">TheilSenRegressorScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.TheilSenRegressorScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Theil-Sen Estimator: robust multivariate regression model.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.theil_sen.TheilSenRegressor</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
The algorithm calculates least square solutions on subsets with size
n_subsamples of the samples in X. Any value of n_subsamples between the
number of features and samples leads to an estimator with a compromise
between robustness and efficiency. Since the number of least square
solutions is “n_samples choose n_subsamples”, it can be extremely large
and can therefore be limited with max_subpopulation. If this limit is
reached, the subsets are chosen randomly. In a final step, the spatial
median (or L1 median) is calculated of all least square solutions.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations.</dd>
<dt>copy_X <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>If True, X will be copied; else, it may be overwritten.</dd>
<dt>max_subpopulation <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default 1e4</span></dt>
<dd>Instead of computing with a set of cardinality ‘n choose k’, where n is
the number of samples and k is the number of subsamples (at least
number of features), consider only a stochastic subpopulation of a
given maximal size if ‘n choose k’ is larger than max_subpopulation.
For other than small problem sizes this parameter will determine
memory usage and runtime if n_subsamples is not changed.</dd>
<dt>n_subsamples <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default None</span></dt>
<dd>Number of samples to calculate the parameters. This is at least the
number of features (plus 1 if fit_intercept=True) and the number of
samples as a maximum. A lower number leads to a higher breakdown
point and a low efficiency while a high number leads to a low
breakdown point and a high efficiency. If None, take the
minimum number of subsamples leading to maximal robustness.
If n_subsamples is set to n_samples, Theil-Sen is identical to least
squares.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default 300</span></dt>
<dd>Maximum number of iterations for the calculation of spatial median.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default 1.e-3</span></dt>
<dd>Tolerance when calculating spatial median.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional, default None</span></dt>
<dd>A random number generator instance to define the state of the random
permutations generator.  If int, random_state is the seed used by the
random number generator; If RandomState instance, random_state is the
random number generator; If None, the random number generator is the
RandomState instance used by <cite>np.random</cite>.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Number of CPUs to use during the cross validation.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span></dt>
<dd>Verbose mode when fitting the model.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = (n_features)</span></dt>
<dd>Coefficients of the regression model (median of distribution).</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Estimated intercept of regression model.</dd>
<dt><code class="docutils literal"><span class="pre">breakdown_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Approximated breakdown point.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of iterations needed for the spatial median.</dd>
<dt><code class="docutils literal"><span class="pre">n_subpopulation_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of combinations taken into account from ‘n choose k’, where n is
the number of samples and k is the number of subsamples.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">TheilSenRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">TheilSenRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">0.9884...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">1</span><span class="p">,])</span>
<span class="go">array([-31.5871...])</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<ul class="simple">
<li>Theil-Sen Estimators in a Multiple Linear Regression Model, 2009
Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang
<a class="reference external" href="http://home.olemiss.edu/~xdang/papers/MTSE.pdf">http://home.olemiss.edu/~xdang/papers/MTSE.pdf</a></li>
</ul>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#TheilSenRegressorScikitsLearnNode">TheilSenRegressorScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.SparseRandomProjectionScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">SparseRandomProjectionScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.SparseRandomProjectionScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Reduce dimensionality through sparse random projection
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.random_projection.SparseRandomProjection</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Sparse random matrix is an alternative to dense random
projection matrix that guarantees similar embedding quality while being
much more memory efficient and allowing faster computation of the
projected data.</p>
<p>If we note <cite>s = 1 / density</cite> the components of the random matrix are
drawn from:</p>
<blockquote>
<div><ul class="simple">
<li>-sqrt(s) / sqrt(n_components)   with probability 1 / 2s</li>
<li>0                              with probability 1 - 1 / s</li>
<li>+sqrt(s) / sqrt(n_components)   with probability 1 / 2s</li>
</ul>
</div></blockquote>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int or ‘auto’, optional (default = ‘auto’)</span></dt>
<dd><p class="first">Dimensionality of the target projection space.</p>
<p>n_components can be automatically adjusted according to the
number of samples in the dataset and the bound given by the
Johnson-Lindenstrauss lemma. In that case the quality of the
embedding is controlled by the <code class="docutils literal"><span class="pre">eps</span></code> parameter.</p>
<p class="last">It should be noted that Johnson-Lindenstrauss lemma can yield
very conservative estimated of the required number of components
as it makes no assumption on the structure of the dataset.</p>
</dd>
<dt>density <span class="classifier-delimiter">:</span> <span class="classifier">float in range ]0, 1], optional (default=’auto’)</span></dt>
<dd><p class="first">Ratio of non-zero component in the random projection matrix.</p>
<p>If density = ‘auto’, the value is set to the minimum density
as recommended by Ping Li et al.: 1 / sqrt(n_features).</p>
<p class="last">Use density = 1 / 3.0 if you want to reproduce the results from
Achlioptas, 2001.</p>
</dd>
<dt>eps <span class="classifier-delimiter">:</span> <span class="classifier">strictly positive float, optional, (default=0.1)</span></dt>
<dd><p class="first">Parameter to control the quality of the embedding according to
the Johnson-Lindenstrauss lemma when n_components is set to
‘auto’.</p>
<p class="last">Smaller values lead to better embedding and higher number of
dimensions (n_components) in the target projection space.</p>
</dd>
<dt>dense_output <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=False)</span></dt>
<dd><p class="first">If True, ensure that the output of the random projection is a
dense numpy array even if the input and random projection matrix
are both sparse. In practice, if the number of components is
small the number of zero components in the projected data will
be very small and it will be more CPU and memory efficient to
use a dense representation.</p>
<p class="last">If False, the projected data uses a sparse representation if
the input is sparse.</p>
</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>Control the pseudo random number generator used to generate the matrix
at fit time.  If int, random_state is the seed used by the random
number generator; If RandomState instance, random_state is the random
number generator; If None, the random number generator is the
RandomState instance used by <cite>np.random</cite>.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">n_component_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Concrete number of components computed when n_components=”auto”.</dd>
<dt><code class="docutils literal"><span class="pre">components_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">CSR matrix with shape [n_components, n_features]</span></dt>
<dd>Random matrix used for the projection.</dd>
<dt><code class="docutils literal"><span class="pre">density_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float in range 0.0 - 1.0</span></dt>
<dd>Concrete density computed from when density = “auto”.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.random_projection</span> <span class="kn">import</span> <span class="n">SparseRandomProjection</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span> <span class="o">=</span> <span class="n">SparseRandomProjection</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_new</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_new</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(100, 3947)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># very few components are non-zero</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">transformer</span><span class="o">.</span><span class="n">components_</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> 
<span class="go">0.0100...</span>
</pre></div>
</div>
<p>See Also</p>
<p>GaussianRandomProjection</p>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Ping Li, T. Hastie and K. W. Church, 2006,
“Very Sparse Random Projections”.
<a class="reference external" href="http://web.stanford.edu/~hastie/Papers/Ping/KDD06_rp.pdf">http://web.stanford.edu/~hastie/Papers/Ping/KDD06_rp.pdf</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>D. Achlioptas, 2001, “Database-friendly random projections”,
<a class="reference external" href="https://users.soe.ucsc.edu/~optas/papers/jl.pdf">https://users.soe.ucsc.edu/~optas/papers/jl.pdf</a></td></tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#SparseRandomProjectionScikitsLearnNode">SparseRandomProjectionScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.LinearModelCVScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">LinearModelCVScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.LinearModelCVScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.coordinate_descent.LinearModelCV</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.LinearModelCVScikitsLearnNode-class.html">LinearModelCVScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.DictionaryLearningScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">DictionaryLearningScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.DictionaryLearningScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Dictionary learning
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.decomposition.dict_learning.DictionaryLearning</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Finds a dictionary (a set of atoms) that can best be used to represent data
using a sparse code.</p>
<p>Solves the optimization problem:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">U</span><span class="o">^*</span><span class="p">,</span><span class="n">V</span><span class="o">^*</span><span class="p">)</span> <span class="o">=</span> <span class="n">argmin</span> <span class="mf">0.5</span> <span class="o">||</span> <span class="n">Y</span> <span class="o">-</span> <span class="n">U</span> <span class="n">V</span> <span class="o">||</span><span class="n">_2</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span> <span class="n">U</span> <span class="o">||</span><span class="n">_1</span>
            <span class="p">(</span><span class="n">U</span><span class="p">,</span><span class="n">V</span><span class="p">)</span>
            <span class="k">with</span> <span class="o">||</span> <span class="n">V_k</span> <span class="o">||</span><span class="n">_2</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">for</span> <span class="nb">all</span>  <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">n_components</span>
</pre></div>
</div>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>number of dictionary elements to extract</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float,</span></dt>
<dd>sparsity controlling parameter</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>maximum number of iterations to perform</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float,</span></dt>
<dd>tolerance for numerical error</dd>
<dt>fit_algorithm <span class="classifier-delimiter">:</span> <span class="classifier">{‘lars’, ‘cd’}</span></dt>
<dd><p class="first">lars: uses the least angle regression method to solve the lasso problem
(linear_model.lars_path)
cd: uses the coordinate descent method to compute the
Lasso solution (linear_model.Lasso). Lars will be faster if
the estimated components are sparse.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>cd</em> coordinate descent method to improve speed.</p>
</div>
</dd>
<dt>transform_algorithm <span class="classifier-delimiter">:</span> <span class="classifier">{‘lasso_lars’, ‘lasso_cd’, ‘lars’, ‘omp’,     ‘threshold’}</span></dt>
<dd><p class="first">Algorithm used to transform the data
lars: uses the least angle regression method (linear_model.lars_path)
lasso_lars: uses Lars to compute the Lasso solution
lasso_cd: uses the coordinate descent method to compute the
Lasso solution (linear_model.Lasso). lasso_lars will be faster if
the estimated components are sparse.
omp: uses orthogonal matching pursuit to estimate the sparse solution
threshold: squashes to zero all coefficients less than alpha from
the projection <code class="docutils literal"><span class="pre">dictionary</span> <span class="pre">*</span> <span class="pre">X'</span></code></p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>lasso_cd</em> coordinate descent method to improve speed.</p>
</div>
</dd>
<dt>transform_n_nonzero_coefs <span class="classifier-delimiter">:</span> <span class="classifier">int, <code class="docutils literal"><span class="pre">0.1</span> <span class="pre">*</span> <span class="pre">n_features</span></code> by default</span></dt>
<dd>Number of nonzero coefficients to target in each column of the
solution. This is only used by <cite>algorithm=’lars’</cite> and <cite>algorithm=’omp’</cite>
and is overridden by <cite>alpha</cite> in the <cite>omp</cite> case.</dd>
<dt>transform_alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, 1. by default</span></dt>
<dd>If <cite>algorithm=’lasso_lars’</cite> or <cite>algorithm=’lasso_cd’</cite>, <cite>alpha</cite> is the
penalty applied to the L1 norm.
If <cite>algorithm=’threshold’</cite>, <cite>alpha</cite> is the absolute value of the
threshold below which coefficients will be squashed to zero.
If <cite>algorithm=’omp’</cite>, <cite>alpha</cite> is the tolerance parameter: the value of
the reconstruction error targeted. In this case, it overrides
<cite>n_nonzero_coefs</cite>.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Number of parallel jobs to run.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>code_init <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_samples, n_components),</span></dt>
<dd>initial value for the code, for warm restart</dd>
<dt>dict_init <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_components, n_features),</span></dt>
<dd>initial values for the dictionary, for warm restart</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default: False)</span></dt>
<dd>To control the verbosity of the procedure.</dd>
<dt>split_sign <span class="classifier-delimiter">:</span> <span class="classifier">bool, False by default</span></dt>
<dd>Whether to split the sparse feature vector into the concatenation of
its negative part and its positive part. This can improve the
performance of downstream classifiers.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>positive_code <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd><p class="first">Whether to enforce positivity when finding the code.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
<dt>positive_dict <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd><p class="first">Whether to enforce positivity when finding the dictionary</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">components_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>dictionary atoms extracted from the data</dd>
<dt><code class="docutils literal"><span class="pre">error_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array</span></dt>
<dd>vector of errors at each iteration</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of iterations run.</dd>
</dl>
<p><strong>Notes</strong></p>
<p><strong>References:</strong></p>
<p>J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning
for sparse coding (<a class="reference external" href="http://www.di.ens.fr/sierra/pdfs/icml09.pdf">http://www.di.ens.fr/sierra/pdfs/icml09.pdf</a>)</p>
<p>See also</p>
<p>SparseCoder
MiniBatchDictionaryLearning
SparsePCA
MiniBatchSparsePCA</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#DictionaryLearningScikitsLearnNode">DictionaryLearningScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.MinMaxScalerScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">MinMaxScalerScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.MinMaxScalerScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Transforms features by scaling each feature to a given range.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.preprocessing.data.MinMaxScaler</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
This estimator scales and translates each feature individually such
that it is in the given range on the training set, e.g. between
zero and one.</p>
<p>The transformation is given by:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">X_std</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">X_std</span> <span class="o">*</span> <span class="p">(</span><span class="nb">max</span> <span class="o">-</span> <span class="nb">min</span><span class="p">)</span> <span class="o">+</span> <span class="nb">min</span>
</pre></div>
</div>
<p>where min, max = feature_range.</p>
<p>The transformation is calculated as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="nb">min</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span>
<span class="n">where</span> <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="nb">max</span> <span class="o">-</span> <span class="nb">min</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
<p>This transformation is often used as an alternative to zero mean,
unit variance scaling.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>feature_range <span class="classifier-delimiter">:</span> <span class="classifier">tuple (min, max), default=(0, 1)</span></dt>
<dd>Desired range of transformed data.</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>Set to False to perform inplace row normalization and avoid a
copy (if the input is already a numpy array).</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">min_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">ndarray, shape (n_features,)</span></dt>
<dd>Per feature adjustment for minimum. Equivalent to
<code class="docutils literal"><span class="pre">min</span> <span class="pre">-</span> <span class="pre">X.min(axis=0)</span> <span class="pre">*</span> <span class="pre">self.scale_</span></code></dd>
<dt><code class="docutils literal"><span class="pre">scale_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">ndarray, shape (n_features,)</span></dt>
<dd><p class="first">Per feature relative scaling of the data. Equivalent to
<code class="docutils literal"><span class="pre">(max</span> <span class="pre">-</span> <span class="pre">min)</span> <span class="pre">/</span> <span class="pre">(X.max(axis=0)</span> <span class="pre">-</span> <span class="pre">X.min(axis=0))</span></code></p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>scale_</em> attribute.</p>
</div>
</dd>
<dt><code class="docutils literal"><span class="pre">data_min_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">ndarray, shape (n_features,)</span></dt>
<dd><p class="first">Per feature minimum seen in the data</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>data_min_</em></p>
</div>
</dd>
<dt><code class="docutils literal"><span class="pre">data_max_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">ndarray, shape (n_features,)</span></dt>
<dd><p class="first">Per feature maximum seen in the data</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>data_max_</em></p>
</div>
</dd>
<dt><code class="docutils literal"><span class="pre">data_range_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">ndarray, shape (n_features,)</span></dt>
<dd><p class="first">Per feature range <code class="docutils literal"><span class="pre">(data_max_</span> <span class="pre">-</span> <span class="pre">data_min_)</span></code> seen in the data</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>data_range_</em></p>
</div>
</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">[[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">18</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
<span class="go">MinMaxScaler(copy=True, feature_range=(0, 1))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">data_max_</span><span class="p">)</span>
<span class="go">[ 1. 18.]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
<span class="go">[[0.   0.  ]</span>
<span class="go"> [0.25 0.25]</span>
<span class="go"> [0.5  0.5 ]</span>
<span class="go"> [1.   1.  ]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]))</span>
<span class="go">[[1.5 0. ]]</span>
</pre></div>
</div>
<p>See also</p>
<p>minmax_scale: Equivalent function without the estimator API.</p>
<p><strong>Notes</strong></p>
<p>NaNs are treated as missing values: disregarded in fit, and maintained in
transform.</p>
<p>For a comparison of the different scalers, transformers, and normalizers,
see <span class="xref std std-ref">examples/preprocessing/plot_all_scaling.py</span>.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#MinMaxScalerScikitsLearnNode">MinMaxScalerScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.ElasticNetCVScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">ElasticNetCVScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.ElasticNetCVScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Elastic Net model with iterative fitting along a regularization path.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.coordinate_descent.ElasticNetCV</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
See glossary entry for <span class="xref std std-term">cross-validation estimator</span>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>l1_ratio <span class="classifier-delimiter">:</span> <span class="classifier">float or array of floats, optional</span></dt>
<dd>float between 0 and 1 passed to ElasticNet (scaling between
l1 and l2 penalties). For <code class="docutils literal"><span class="pre">l1_ratio</span> <span class="pre">=</span> <span class="pre">0</span></code>
the penalty is an L2 penalty. For <code class="docutils literal"><span class="pre">l1_ratio</span> <span class="pre">=</span> <span class="pre">1</span></code> it is an L1 penalty.
For <code class="docutils literal"><span class="pre">0</span> <span class="pre">&lt;</span> <span class="pre">l1_ratio</span> <span class="pre">&lt;</span> <span class="pre">1</span></code>, the penalty is a combination of L1 and L2
This parameter can be a list, in which case the different
values are tested by cross-validation and the one giving the best
prediction score is used. Note that a good choice of list of
values for l1_ratio is often to put more values close to 1
(i.e. Lasso) and less close to 0 (i.e. Ridge), as in <code class="docutils literal"><span class="pre">[.1,</span> <span class="pre">.5,</span> <span class="pre">.7,</span>
<span class="pre">.9,</span> <span class="pre">.95,</span> <span class="pre">.99,</span> <span class="pre">1]</span></code></dd>
<dt>eps <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Length of the path. <code class="docutils literal"><span class="pre">eps=1e-3</span></code> means that
<code class="docutils literal"><span class="pre">alpha_min</span> <span class="pre">/</span> <span class="pre">alpha_max</span> <span class="pre">=</span> <span class="pre">1e-3</span></code>.</dd>
<dt>n_alphas <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Number of alphas along the regularization path, used for each l1_ratio.</dd>
<dt>alphas <span class="classifier-delimiter">:</span> <span class="classifier">numpy array, optional</span></dt>
<dd>List of alphas where to compute the models.
If None alphas are set automatically</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span></dt>
<dd>This parameter is ignored when <code class="docutils literal"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<code class="xref py py-class docutils literal"><span class="pre">sklearn.preprocessing.StandardScaler</span></code> before calling <code class="docutils literal"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal"><span class="pre">normalize=False</span></code>.</dd>
<dt>precompute <span class="classifier-delimiter">:</span> <span class="classifier">True | False | ‘auto’ | array-like</span></dt>
<dd>Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal"><span class="pre">'auto'</span></code> let us decide. The Gram
matrix can also be passed as argument.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>The maximum number of iterations</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The tolerance for the optimization: if the updates are
smaller than <code class="docutils literal"><span class="pre">tol</span></code>, the optimization code checks the
dual gap for optimality and continues until it is smaller
than <code class="docutils literal"><span class="pre">tol</span></code>.</dd>
<dt>cv <span class="classifier-delimiter">:</span> <span class="classifier">int, cross-validation generator or an iterable, optional</span></dt>
<dd><p class="first">Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul class="simple">
<li>None, to use the default 3-fold cross-validation,</li>
<li>integer, to specify the number of folds.</li>
<li><span class="xref std std-term">CV splitter</span>,</li>
<li>An iterable yielding (train, test) splits as arrays of indices.</li>
</ul>
<p>For integer/None inputs, <code class="xref py py-class docutils literal"><span class="pre">KFold</span></code> is used.</p>
<p>Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.20: </span><code class="docutils literal"><span class="pre">cv</span></code> default value if None will change from 3-fold to 5-fold
in v0.22.</p>
</div>
</dd>
<dt>copy_X <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>If <code class="docutils literal"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">bool or integer</span></dt>
<dd>Amount of verbosity.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Number of CPUs to use during the cross validation.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>positive <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd>When set to <code class="docutils literal"><span class="pre">True</span></code>, forces the coefficients to be positive.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional, default None</span></dt>
<dd>The seed of the pseudo random number generator that selects a random
feature to update.  If int, random_state is the seed used by the random
number generator; If RandomState instance, random_state is the random
number generator; If None, the random number generator is the
RandomState instance used by <cite>np.random</cite>. Used when <code class="docutils literal"><span class="pre">selection</span></code> ==
‘random’.</dd>
<dt>selection <span class="classifier-delimiter">:</span> <span class="classifier">str, default ‘cyclic’</span></dt>
<dd>If set to ‘random’, a random coefficient is updated every iteration
rather than looping over features sequentially by default. This
(setting to ‘random’) often leads to significantly faster convergence
especially when tol is higher than 1e-4.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">alpha_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The amount of penalization chosen by cross validation</dd>
<dt><code class="docutils literal"><span class="pre">l1_ratio_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The compromise between l1 and l2 penalization chosen by
cross validation</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,) | (n_targets, n_features)</span></dt>
<dd>Parameter vector (w in the cost function formula),</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float | array, shape (n_targets, n_features)</span></dt>
<dd>Independent term in the decision function.</dd>
<dt><code class="docutils literal"><span class="pre">mse_path_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_l1_ratio, n_alpha, n_folds)</span></dt>
<dd>Mean square error for the test set on each fold, varying l1_ratio and
alpha.</dd>
<dt><code class="docutils literal"><span class="pre">alphas_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">numpy array, shape (n_alphas,) or (n_l1_ratio, n_alphas)</span></dt>
<dd>The grid of alphas used for fitting, for each l1_ratio.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>number of iterations run by the coordinate descent solver to reach
the specified tolerance for the optimal alpha.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNetCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span> <span class="o">=</span> <span class="n">ElasticNetCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">ElasticNetCV(alphas=None, copy_X=True, cv=5, eps=0.001, fit_intercept=True,</span>
<span class="go">       l1_ratio=0.5, max_iter=1000, n_alphas=100, n_jobs=None,</span>
<span class="go">       normalize=False, positive=False, precompute=&#39;auto&#39;, random_state=0,</span>
<span class="go">       selection=&#39;cyclic&#39;, tol=0.0001, verbose=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">alpha_</span><span class="p">)</span> 
<span class="go">0.1994727942696716</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span> 
<span class="go">0.398...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span> 
<span class="go">[0.398...]</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>For an example, see
<span class="xref std std-ref">examples/linear_model/plot_lasso_model_selection.py</span>.</p>
<p>To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.</p>
<p>The parameter l1_ratio corresponds to alpha in the glmnet R package
while alpha corresponds to the lambda parameter in glmnet.
More specifically, the optimization objective is:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2_2</span>
<span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
<span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||^</span><span class="mi">2_2</span>
</pre></div>
</div>
<p>If you are interested in controlling the L1 and L2 penalty
separately, keep in mind that this is equivalent to:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">*</span> <span class="n">L1</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">L2</span>
</pre></div>
</div>
<p>for:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="ow">and</span> <span class="n">l1_ratio</span> <span class="o">=</span> <span class="n">a</span> <span class="o">/</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span><span class="o">.</span>
</pre></div>
</div>
<p>See also</p>
<p>enet_path
ElasticNet</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.ElasticNetCVScikitsLearnNode-class.html">ElasticNetCVScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.RBFSamplerScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">RBFSamplerScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.RBFSamplerScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Approximates feature map of an RBF kernel by Monte Carlo approximation
of its Fourier transform.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.kernel_approximation.RBFSampler</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
It implements a variant of Random Kitchen Sinks.[1]</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>gamma <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Parameter of RBF kernel: exp(-gamma * x^2)</dd>
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of Monte Carlo samples per original feature.
Equals the dimensionality of the computed feature space.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.kernel_approximation</span> <span class="kn">import</span> <span class="n">RBFSampler</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rbf_feature</span> <span class="o">=</span> <span class="n">RBFSampler</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_features</span> <span class="o">=</span> <span class="n">rbf_feature</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_features</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">SGDClassifier(alpha=0.0001, average=False, class_weight=None,</span>
<span class="go">       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,</span>
<span class="go">       l1_ratio=0.15, learning_rate=&#39;optimal&#39;, loss=&#39;hinge&#39;, max_iter=5,</span>
<span class="go">       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty=&#39;l2&#39;,</span>
<span class="go">       power_t=0.5, random_state=None, shuffle=True, tol=0.001,</span>
<span class="go">       validation_fraction=0.1, verbose=0, warm_start=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_features</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">1.0</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>See “Random Features for Large-Scale Kernel Machines” by A. Rahimi and
Benjamin Recht.</p>
<p>[1] “Weighted Sums of Random Kitchen Sinks: Replacing
minimization with randomization in learning” by A. Rahimi and
Benjamin Recht.
(<a class="reference external" href="http://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf">http://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf</a>)</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#RBFSamplerScikitsLearnNode">RBFSamplerScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.OrthogonalMatchingPursuitCVScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">OrthogonalMatchingPursuitCVScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.OrthogonalMatchingPursuitCVScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Cross-validated Orthogonal Matching Pursuit model (OMP).
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.omp.OrthogonalMatchingPursuitCV</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
See glossary entry for <span class="xref std std-term">cross-validation estimator</span>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd>Whether the design matrix X must be copied by the algorithm. A false
value is only helpful if X is already Fortran-ordered, otherwise a
copy is made anyway.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>This parameter is ignored when <code class="docutils literal"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<code class="xref py py-class docutils literal"><span class="pre">sklearn.preprocessing.StandardScaler</span></code> before calling <code class="docutils literal"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal"><span class="pre">normalize=False</span></code>.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span></dt>
<dd>Maximum numbers of iterations to perform, therefore maximum features
to include. 10% of <code class="docutils literal"><span class="pre">n_features</span></code> but at least 5 if available.</dd>
<dt>cv <span class="classifier-delimiter">:</span> <span class="classifier">int, cross-validation generator or an iterable, optional</span></dt>
<dd><p class="first">Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul class="simple">
<li>None, to use the default 3-fold cross-validation,</li>
<li>integer, to specify the number of folds.</li>
<li><span class="xref std std-term">CV splitter</span>,</li>
<li>An iterable yielding (train, test) splits as arrays of indices.</li>
</ul>
<p>For integer/None inputs, <code class="xref py py-class docutils literal"><span class="pre">KFold</span></code> is used.</p>
<p>Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.20: </span><code class="docutils literal"><span class="pre">cv</span></code> default value if None will change from 3-fold to 5-fold
in v0.22.</p>
</div>
</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Number of CPUs to use during the cross validation.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">boolean or integer, optional</span></dt>
<dd>Sets the verbosity amount</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float or array, shape (n_targets,)</span></dt>
<dd>Independent term in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,) or (n_targets, n_features)</span></dt>
<dd>Parameter vector (w in the problem formulation).</dd>
<dt><code class="docutils literal"><span class="pre">n_nonzero_coefs_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Estimated number of non-zero coefficients giving the best mean squared
error over the cross-validation folds.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int or array-like</span></dt>
<dd>Number of active features across every target for the model refit with
the best hyperparameters got by cross-validating across all folds.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">OrthogonalMatchingPursuitCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">noise</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">OrthogonalMatchingPursuitCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">0.9991...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">n_nonzero_coefs_</span>
<span class="go">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">1</span><span class="p">,])</span>
<span class="go">array([-78.3854...])</span>
</pre></div>
</div>
<p>See also</p>
<p>orthogonal_mp
orthogonal_mp_gram
lars_path
Lars
LassoLars
OrthogonalMatchingPursuit
LarsCV
LassoLarsCV
decomposition.sparse_encode</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#OrthogonalMatchingPursuitCVScikitsLearnNode">OrthogonalMatchingPursuitCVScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.SkewedChi2SamplerScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">SkewedChi2SamplerScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.SkewedChi2SamplerScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Approximates feature map of the “skewed chi-squared” kernel by Monte
Carlo approximation of its Fourier transform.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.kernel_approximation.SkewedChi2Sampler</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>skewedness <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>“skewedness” parameter of the kernel. Needs to be cross-validated.</dd>
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>number of Monte Carlo samples per original feature.
Equals the dimensionality of the computed feature space.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.kernel_approximation</span> <span class="kn">import</span> <span class="n">SkewedChi2Sampler</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">chi2_feature</span> <span class="o">=</span> <span class="n">SkewedChi2Sampler</span><span class="p">(</span><span class="n">skewedness</span><span class="o">=.</span><span class="mi">01</span><span class="p">,</span>
<span class="gp">... </span>                                 <span class="n">n_components</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="gp">... </span>                                 <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_features</span> <span class="o">=</span> <span class="n">chi2_feature</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_features</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">SGDClassifier(alpha=0.0001, average=False, class_weight=None,</span>
<span class="go">       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,</span>
<span class="go">       l1_ratio=0.15, learning_rate=&#39;optimal&#39;, loss=&#39;hinge&#39;, max_iter=10,</span>
<span class="go">       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty=&#39;l2&#39;,</span>
<span class="go">       power_t=0.5, random_state=None, shuffle=True, tol=0.001,</span>
<span class="go">       validation_fraction=0.1, verbose=0, warm_start=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_features</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">1.0</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>See “Random Fourier Approximations for Skewed Multiplicative Histogram
Kernels” by Fuxin Li, Catalin Ionescu and Cristian Sminchisescu.</p>
<p>See also</p>
<dl class="docutils">
<dt>AdditiveChi2Sampler <span class="classifier-delimiter">:</span> <span class="classifier">A different approach for approximating an additive</span></dt>
<dd>variant of the chi squared kernel.</dd>
</dl>
<p>sklearn.metrics.pairwise.chi2_kernel : The exact chi squared kernel.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#SkewedChi2SamplerScikitsLearnNode">SkewedChi2SamplerScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.RandomTreesEmbeddingScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">RandomTreesEmbeddingScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.RandomTreesEmbeddingScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>An ensemble of totally random trees.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.ensemble.forest.RandomTreesEmbedding</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
An unsupervised transformation of a dataset to a high-dimensional
sparse representation. A datapoint is coded according to which leaf of
each tree it is sorted into. Using a one-hot encoding of the leaves,
this leads to a binary coding with as many ones as there are trees in
the forest.</p>
<p>The dimensionality of the resulting representation is
<code class="docutils literal"><span class="pre">n_out</span> <span class="pre">&lt;=</span> <span class="pre">n_estimators</span> <span class="pre">*</span> <span class="pre">max_leaf_nodes</span></code>. If <code class="docutils literal"><span class="pre">max_leaf_nodes</span> <span class="pre">==</span> <span class="pre">None</span></code>,
the number of leaf nodes is at most <code class="docutils literal"><span class="pre">n_estimators</span> <span class="pre">*</span> <span class="pre">2</span> <span class="pre">**</span> <span class="pre">max_depth</span></code>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_estimators <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=10)</span></dt>
<dd><p class="first">Number of trees in the forest.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.20: </span>The default value of <code class="docutils literal"><span class="pre">n_estimators</span></code> will change from 10 in
version 0.20 to 100 in version 0.22.</p>
</div>
</dd>
<dt>max_depth <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=5)</span></dt>
<dd>The maximum depth of each tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</dd>
<dt>min_samples_split <span class="classifier-delimiter">:</span> <span class="classifier">int, float, optional (default=2)</span></dt>
<dd><p class="first">The minimum number of samples required to split an internal node:</p>
<ul class="simple">
<li>If int, then consider <cite>min_samples_split</cite> as the minimum number.</li>
<li>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> is the minimum
number of samples for each split.</li>
</ul>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</dd>
<dt>min_samples_leaf <span class="classifier-delimiter">:</span> <span class="classifier">int, float, optional (default=1)</span></dt>
<dd><p class="first">The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul class="simple">
<li>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</li>
<li>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> is the minimum
number of samples for each node.</li>
</ul>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</dd>
<dt>min_weight_fraction_leaf <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span></dt>
<dd>The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</dd>
<dt>max_leaf_nodes <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Grow trees with <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</dd>
<dt>min_impurity_decrease <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span></dt>
<dd><p class="first">A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal"><span class="pre">N</span></code>, <code class="docutils literal"><span class="pre">N_t</span></code>, <code class="docutils literal"><span class="pre">N_t_R</span></code> and <code class="docutils literal"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19.</span></p>
</div>
</dd>
<dt>min_impurity_split <span class="classifier-delimiter">:</span> <span class="classifier">float, (default=1e-7)</span></dt>
<dd><p class="first">Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="last deprecated">
<p><span class="versionmodified">Deprecated since version 0.19: </span><code class="docutils literal"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal"><span class="pre">min_impurity_split</span></code> will change from 1e-7 to 0 in 0.23 and it
will be removed in 0.25. Use <code class="docutils literal"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</dd>
<dt>sparse_output <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default=True)</span></dt>
<dd>Whether or not to return a sparse CSR matrix, as default behavior,
or to return a dense array compatible with dense pipeline operators.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The number of jobs to run in parallel for both <cite>fit</cite> and <cite>predict</cite>.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=0)</span></dt>
<dd>Controls the verbosity when fitting and predicting.</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default=False)</span></dt>
<dd>When set to <code class="docutils literal"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit a whole
new forest. See <span class="xref std std-term">the Glossary</span>.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">estimators_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list of DecisionTreeClassifier</span></dt>
<dd>The collection of fitted sub-estimators.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized trees”,
Machine Learning, 63(1), 3-42, 2006.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>Moosmann, F. and Triggs, B. and Jurie, F.  “Fast discriminative
visual codebooks using randomized clustering forests”
NIPS 2007</td></tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#RandomTreesEmbeddingScikitsLearnNode">RandomTreesEmbeddingScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.PerceptronScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">PerceptronScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.PerceptronScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Perceptron
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.perceptron.Perceptron</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>penalty <span class="classifier-delimiter">:</span> <span class="classifier">None, ‘l2’ or ‘l1’ or ‘elasticnet’</span></dt>
<dd>The penalty (aka regularization term) to be used. Defaults to None.</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Constant that multiplies the regularization term if regularization is
used. Defaults to 0.0001</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Whether the intercept should be estimated or not. If False, the
data is assumed to be already centered. Defaults to True.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd><p class="first">The maximum number of passes over the training data (aka epochs).
It only impacts the behavior in the <code class="docutils literal"><span class="pre">fit</span></code> method, and not the
<cite>partial_fit</cite>.
Defaults to 5. Defaults to 1000 from 0.21, or if tol is not None.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19.</span></p>
</div>
</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float or None, optional</span></dt>
<dd><p class="first">The stopping criterion. If it is not None, the iterations will stop
when (loss &gt; previous_loss - tol). Defaults to None.
Defaults to 1e-3 from 0.21.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19.</span></p>
</div>
</dd>
<dt>shuffle <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional, default True</span></dt>
<dd>Whether or not the training data should be shuffled after each epoch.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span></dt>
<dd>The verbosity level</dd>
<dt>eta0 <span class="classifier-delimiter">:</span> <span class="classifier">double</span></dt>
<dd>Constant by which the updates are multiplied. Defaults to 1.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The number of CPUs to use to do the OVA (One Versus All, for
multi-class problems) computation.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional, default None</span></dt>
<dd>The seed of the pseudo random number generator to use when shuffling
the data.  If int, random_state is the seed used by the random number
generator; If RandomState instance, random_state is the random number
generator; If None, the random number generator is the RandomState
instance used by <cite>np.random</cite>.</dd>
<dt>early_stopping <span class="classifier-delimiter">:</span> <span class="classifier">bool, default=False</span></dt>
<dd><p class="first">Whether to use early stopping to terminate training when validation.
score is not improving. If set to True, it will automatically set aside
a fraction of training data as validation and terminate training when
validation score is not improving by at least tol for
n_iter_no_change consecutive epochs.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
<dt>validation_fraction <span class="classifier-delimiter">:</span> <span class="classifier">float, default=0.1</span></dt>
<dd><p class="first">The proportion of training data to set aside as validation set for
early stopping. Must be between 0 and 1.
Only used if early_stopping is True.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
<dt>n_iter_no_change <span class="classifier-delimiter">:</span> <span class="classifier">int, default=5</span></dt>
<dd><p class="first">Number of iterations with no improvement to wait before early stopping.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
<dt>class_weight <span class="classifier-delimiter">:</span> <span class="classifier">dict, {class_label: weight} or “balanced” or None, optional</span></dt>
<dd><p class="first">Preset for the class_weight fit parameter.</p>
<p>Weights associated with classes. If not given, all classes
are supposed to have weight one.</p>
<p class="last">The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd>When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution. See
<span class="xref std std-term">the Glossary</span>.</dd>
<dt>n_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd><p class="first">The number of passes over the training data (aka epochs).
Defaults to None. Deprecated, will be removed in 0.21.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.19: </span>Deprecated</p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]</span></dt>
<dd>Weights assigned to the features.</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1] if n_classes == 2 else [n_classes]</span></dt>
<dd>Constants in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The actual number of iterations to reach the stopping criterion.
For multiclass fits, it is the maximum over every binary fit.</dd>
</dl>
<p><strong>Notes</strong></p>
<p><code class="docutils literal"><span class="pre">Perceptron</span></code> is a classification algorithm which shares the same
underlying implementation with <code class="docutils literal"><span class="pre">SGDClassifier</span></code>. In fact,
<code class="docutils literal"><span class="pre">Perceptron()</span></code> is equivalent to <cite>SGDClassifier(loss=”perceptron”,
eta0=1, learning_rate=”constant”, penalty=None)</cite>.</p>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Perceptron</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">Perceptron</span><span class="p">(</span><span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,</span>
<span class="go">      fit_intercept=True, max_iter=None, n_iter=None, n_iter_no_change=5,</span>
<span class="go">      n_jobs=None, penalty=None, random_state=0, shuffle=True, tol=0.001,</span>
<span class="go">      validation_fraction=0.1, verbose=0, warm_start=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">0.946...</span>
</pre></div>
</div>
<p>See also</p>
<p>SGDClassifier</p>
<p><strong>References</strong></p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Perceptron">https://en.wikipedia.org/wiki/Perceptron</a> and references therein.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#PerceptronScikitsLearnNode">PerceptronScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.RidgeClassifierScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">RidgeClassifierScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.RidgeClassifierScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Classifier using Ridge regression.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.ridge.RidgeClassifier</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Regularization strength; must be a positive float. Regularization
improves the conditioning of the problem and reduces the variance of
the estimates. Larger values specify stronger regularization.
Alpha corresponds to <code class="docutils literal"><span class="pre">C^-1</span></code> in other linear models such as
LogisticRegression or LinearSVC.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>Whether to calculate the intercept for this model. If set to false, no
intercept will be used in calculations (e.g. data is expected to be
already centered).</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span></dt>
<dd>This parameter is ignored when <code class="docutils literal"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<code class="xref py py-class docutils literal"><span class="pre">sklearn.preprocessing.StandardScaler</span></code> before calling <code class="docutils literal"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal"><span class="pre">normalize=False</span></code>.</dd>
<dt>copy_X <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>If True, X will be copied; else, it may be overwritten.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Maximum number of iterations for conjugate gradient solver.
The default value is determined by scipy.sparse.linalg.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Precision of the solution.</dd>
<dt>class_weight <span class="classifier-delimiter">:</span> <span class="classifier">dict or ‘balanced’, optional</span></dt>
<dd><p class="first">Weights associated with classes in the form <code class="docutils literal"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
If not given, all classes are supposed to have weight one.</p>
<p class="last">The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
</dd>
<dt>solver <span class="classifier-delimiter">:</span> <span class="classifier">{‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’, ‘sag’, ‘saga’}</span></dt>
<dd><p class="first">Solver to use in the computational routines:</p>
<ul class="last">
<li><p class="first">‘auto’ chooses the solver automatically based on the type of data.</p>
</li>
<li><p class="first">‘svd’ uses a Singular Value Decomposition of X to compute the Ridge
coefficients. More stable for singular matrices than
‘cholesky’.</p>
</li>
<li><p class="first">‘cholesky’ uses the standard scipy.linalg.solve function to
obtain a closed-form solution.</p>
</li>
<li><p class="first">‘sparse_cg’ uses the conjugate gradient solver as found in
scipy.sparse.linalg.cg. As an iterative algorithm, this solver is
more appropriate than ‘cholesky’ for large-scale data
(possibility to set <cite>tol</cite> and <cite>max_iter</cite>).</p>
</li>
<li><p class="first">‘lsqr’ uses the dedicated regularized least-squares routine
scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative
procedure.</p>
</li>
<li><p class="first">‘sag’ uses a Stochastic Average Gradient descent, and ‘saga’ uses
its unbiased and more flexible version named SAGA. Both methods
use an iterative procedure, and are often faster than other solvers
when both n_samples and n_features are large. Note that ‘sag’ and
‘saga’ fast convergence is only guaranteed on features with
approximately the same scale. You can preprocess the data with a
scaler from sklearn.preprocessing.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17: </span>Stochastic Average Gradient descent solver.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.19: </span>SAGA solver.</p>
</div>
</li>
</ul>
</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional, default None</span></dt>
<dd>The seed of the pseudo random number generator to use when shuffling
the data.  If int, random_state is the seed used by the random number
generator; If RandomState instance, random_state is the random number
generator; If None, the random number generator is the RandomState
instance used by <cite>np.random</cite>. Used when <code class="docutils literal"><span class="pre">solver</span></code> == ‘sag’.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,) or (n_classes, n_features)</span></dt>
<dd>Weight vector(s).</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float | array, shape = (n_targets,)</span></dt>
<dd>Independent term in decision function. Set to 0.0 if
<code class="docutils literal"><span class="pre">fit_intercept</span> <span class="pre">=</span> <span class="pre">False</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array or None, shape (n_targets,)</span></dt>
<dd>Actual number of iterations for each target. Available only for
sag and lsqr solvers. Other solvers will return None.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RidgeClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">RidgeClassifier</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">0.9595...</span>
</pre></div>
</div>
<p>See also</p>
<p>Ridge : Ridge regression
RidgeClassifierCV :  Ridge classifier with built-in cross validation</p>
<p><strong>Notes</strong></p>
<p>For multi-class classification, n_class classifiers are trained in
a one-versus-all approach. Concretely, this is implemented by taking
advantage of the multi-variate response support in Ridge.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.RidgeClassifierScikitsLearnNode-class.html">RidgeClassifierScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.LinearSVRScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">LinearSVRScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.LinearSVRScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Linear Support Vector Regression.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.svm.classes.LinearSVR</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Similar to SVR with parameter kernel=’linear’, but implemented in terms of
liblinear rather than libsvm, so it has more flexibility in the choice of
penalties and loss functions and should scale better to large numbers of
samples.</p>
<p>This class supports both dense and sparse input.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>epsilon <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.1)</span></dt>
<dd>Epsilon parameter in the epsilon-insensitive loss function. Note
that the value of this parameter depends on the scale of the target
variable y. If unsure, set <code class="docutils literal"><span class="pre">epsilon=0</span></code>.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1e-4)</span></dt>
<dd>Tolerance for stopping criteria.</dd>
<dt>C <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span></dt>
<dd>Penalty parameter C of the error term. The penalty is a squared
l2 penalty. The bigger this parameter, the less regularization is used.</dd>
<dt>loss <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=’epsilon_insensitive’)</span></dt>
<dd>Specifies the loss function. The epsilon-insensitive loss
(standard SVR) is the L1 loss, while the squared epsilon-insensitive
loss (‘squared_epsilon_insensitive’) is the L2 loss.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span></dt>
<dd>Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(i.e. data is expected to be already centered).</dd>
<dt>intercept_scaling <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1)</span></dt>
<dd>When self.fit_intercept is True, instance vector x becomes
[x, self.intercept_scaling],
i.e. a “synthetic” feature with constant value equals to
intercept_scaling is appended to the instance vector.
The intercept becomes intercept_scaling * synthetic feature weight
Note! the synthetic feature weight is subject to l1/l2 regularization
as all other features.
To lessen the effect of regularization on synthetic feature weight
(and therefore on the intercept) intercept_scaling has to be increased.</dd>
<dt>dual <span class="classifier-delimiter">:</span> <span class="classifier">bool, (default=True)</span></dt>
<dd>Select the algorithm to either solve the dual or primal
optimization problem. Prefer dual=False when n_samples &gt; n_features.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, (default=0)</span></dt>
<dd>Enable verbose output. Note that this setting takes advantage of a
per-process runtime setting in liblinear that, if enabled, may not work
properly in a multithreaded context.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>The seed of the pseudo random number generator to use when shuffling
the data.  If int, random_state is the seed used by the random number
generator; If RandomState instance, random_state is the random number
generator; If None, the random number generator is the RandomState
instance used by <cite>np.random</cite>.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, (default=1000)</span></dt>
<dd>The maximum number of iterations to be run.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features] if n_classes == 2 else [n_classes, n_features]</span></dt>
<dd><p class="first">Weights assigned to the features (coefficients in the primal
problem). This is only available in the case of a linear kernel.</p>
<p class="last"><cite>coef_</cite> is a readonly property derived from <cite>raw_coef_</cite> that
follows the internal memory layout of liblinear.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1] if n_classes == 2 else [n_classes]</span></dt>
<dd>Constants in decision function.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVR</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span> <span class="o">=</span> <span class="n">LinearSVR</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,</span>
<span class="go">     intercept_scaling=1.0, loss=&#39;epsilon_insensitive&#39;, max_iter=1000,</span>
<span class="go">     random_state=0, tol=1e-05, verbose=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">[16.35... 26.91... 42.30... 60.47...]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="go">[-4.29...]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>
<span class="go">[-4.29...]</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>LinearSVC</dt>
<dd>Implementation of Support Vector Machine classifier using the
same library as this class (liblinear).</dd>
<dt>SVR</dt>
<dd><p class="first">Implementation of Support Vector Machine regression using libsvm:</p>
<ul class="last simple">
<li>the kernel can be non-linear but its SMO algorithm does not</li>
<li>scale to large number of samples as LinearSVC does.</li>
</ul>
</dd>
<dt>sklearn.linear_model.SGDRegressor</dt>
<dd>SGDRegressor can optimize the same cost function as LinearSVR
by adjusting the penalty and loss parameters. In addition it requires
less memory, allows incremental (online) learning, and implements
various loss functions and regularization regimes.</dd>
</dl>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#LinearSVRScikitsLearnNode">LinearSVRScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.OrdinalEncoderScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">OrdinalEncoderScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.OrdinalEncoderScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Encode categorical features as an integer array.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.preprocessing._encoders.OrdinalEncoder</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
The input to this transformer should be an array-like of integers or
strings, denoting the values taken on by categorical (discrete) features.
The features are converted to ordinal integers. This results in
a single column of integers (0 to n_categories - 1) per feature.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>categories <span class="classifier-delimiter">:</span> <span class="classifier">‘auto’ or a list of lists/arrays of values.</span></dt>
<dd><p class="first">Categories (unique values) per feature:</p>
<ul class="simple">
<li>‘auto’ : Determine categories automatically from the training data.</li>
<li>list : <code class="docutils literal"><span class="pre">categories[i]</span></code> holds the categories expected in the ith
column. The passed categories should not mix strings and numeric
values, and should be sorted in case of numeric values.</li>
</ul>
<p class="last">The used categories can be found in the <code class="docutils literal"><span class="pre">categories_</span></code> attribute.</p>
</dd>
<dt>dtype <span class="classifier-delimiter">:</span> <span class="classifier">number type, default np.float64</span></dt>
<dd>Desired dtype of output.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">categories_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list of arrays</span></dt>
<dd>The categories of each feature determined during fitting
(in order of the features in X and corresponding with the output
of <code class="docutils literal"><span class="pre">transform</span></code>).</dd>
</dl>
<p><strong>Examples</strong></p>
<p>Given a dataset with two features, we let the encoder find the unique
values per feature and transform the data to an ordinal encoding.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OrdinalEncoder</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span> <span class="o">=</span> <span class="n">OrdinalEncoder</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">&#39;Male&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;Female&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;Female&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">OrdinalEncoder(categories=&#39;auto&#39;, dtype=&lt;... &#39;numpy.float64&#39;&gt;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span><span class="o">.</span><span class="n">categories_</span>
<span class="go">[array([&#39;Female&#39;, &#39;Male&#39;], dtype=object), array([1, 2, 3], dtype=object)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span><span class="o">.</span><span class="n">transform</span><span class="p">([[</span><span class="s1">&#39;Female&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;Male&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="go">array([[0., 2.],</span>
<span class="go">       [1., 0.]])</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="go">array([[&#39;Male&#39;, 1],</span>
<span class="go">       [&#39;Female&#39;, 2]], dtype=object)</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>sklearn.preprocessing.OneHotEncoder <span class="classifier-delimiter">:</span> <span class="classifier">performs a one-hot encoding of</span></dt>
<dd>categorical features.</dd>
<dt>sklearn.preprocessing.LabelEncoder <span class="classifier-delimiter">:</span> <span class="classifier">encodes target labels with values</span></dt>
<dd>between 0 and n_classes-1.</dd>
</dl>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#OrdinalEncoderScikitsLearnNode">OrdinalEncoderScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.QuadraticDiscriminantAnalysisScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">QuadraticDiscriminantAnalysisScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.QuadraticDiscriminantAnalysisScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Quadratic Discriminant Analysis
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
A classifier with a quadratic decision boundary, generated
by fitting class conditional densities to the data
and using Bayes’ rule.</p>
<p>The model fits a Gaussian density to each class.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>QuadraticDiscriminantAnalysis</em></p>
</div>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>priors <span class="classifier-delimiter">:</span> <span class="classifier">array, optional, shape = [n_classes]</span></dt>
<dd>Priors on classes</dd>
<dt>reg_param <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Regularizes the covariance estimate as
<code class="docutils literal"><span class="pre">(1-reg_param)*Sigma</span> <span class="pre">+</span> <span class="pre">reg_param*np.eye(n_features)</span></code></dd>
<dt>store_covariance <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd><p class="first">If True the covariance matrices are computed and stored in the
<cite>self.covariance_</cite> attribute.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17.</span></p>
</div>
</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default 1.0e-4</span></dt>
<dd><p class="first">Threshold used for rank estimation.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17.</span></p>
</div>
</dd>
<dt>store_covariances <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>Deprecated, use <cite>store_covariance</cite>.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">covariance_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list of array-like, shape = [n_features, n_features]</span></dt>
<dd>Covariance matrices of each class.</dd>
<dt><code class="docutils literal"><span class="pre">means_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_classes, n_features]</span></dt>
<dd>Class means.</dd>
<dt><code class="docutils literal"><span class="pre">priors_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_classes]</span></dt>
<dd>Class priors (sum to 1).</dd>
<dt><code class="docutils literal"><span class="pre">rotations_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list of arrays</span></dt>
<dd>For each class k an array of shape [n_features, n_k], with
<code class="docutils literal"><span class="pre">n_k</span> <span class="pre">=</span> <span class="pre">min(n_features,</span> <span class="pre">number</span> <span class="pre">of</span> <span class="pre">elements</span> <span class="pre">in</span> <span class="pre">class</span> <span class="pre">k)</span></code>
It is the rotation of the Gaussian distribution, i.e. its
principal axis.</dd>
<dt><code class="docutils literal"><span class="pre">scalings_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list of arrays</span></dt>
<dd>For each class k an array of shape [n_k]. It contains the scaling
of the Gaussian distributions along its principal axes, i.e. the
variance in the rotated coordinate system.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.discriminant_analysis</span> <span class="kn">import</span> <span class="n">QuadraticDiscriminantAnalysis</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">QuadraticDiscriminantAnalysis</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,</span>
<span class="go">                              store_covariance=False,</span>
<span class="go">                              store_covariances=None, tol=0.0001)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>sklearn.discriminant_analysis.LinearDiscriminantAnalysis: Linear</dt>
<dd>Discriminant Analysis</dd>
</dl>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#QuadraticDiscriminantAnalysisScikitsLearnNode">QuadraticDiscriminantAnalysisScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.MLPClassifierScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">MLPClassifierScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.MLPClassifierScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Multi-layer Perceptron classifier.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.neural_network.multilayer_perceptron.MLPClassifier</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
This model optimizes the log-loss function using LBFGS or stochastic
gradient descent.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.18.</span></p>
</div>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>hidden_layer_sizes <span class="classifier-delimiter">:</span> <span class="classifier">tuple, length = n_layers - 2, default (100,)</span></dt>
<dd>The ith element represents the number of neurons in the ith
hidden layer.</dd>
<dt>activation <span class="classifier-delimiter">:</span> <span class="classifier">{‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default ‘relu’</span></dt>
<dd><p class="first">Activation function for the hidden layer.</p>
<ul class="last simple">
<li>‘identity’, no-op activation, useful to implement linear bottleneck,
returns f(x) = x</li>
<li>‘logistic’, the logistic sigmoid function,
returns f(x) = 1 / (1 + exp(-x)).</li>
<li>‘tanh’, the hyperbolic tan function,
returns f(x) = tanh(x).</li>
<li>‘relu’, the rectified linear unit function,
returns f(x) = max(0, x)</li>
</ul>
</dd>
<dt>solver <span class="classifier-delimiter">:</span> <span class="classifier">{‘lbfgs’, ‘sgd’, ‘adam’}, default ‘adam’</span></dt>
<dd><p class="first">The solver for weight optimization.</p>
<ul class="simple">
<li>‘lbfgs’ is an optimizer in the family of quasi-Newton methods.</li>
<li>‘sgd’ refers to stochastic gradient descent.</li>
<li>‘adam’ refers to a stochastic gradient-based optimizer proposed
by Kingma, Diederik, and Jimmy Ba</li>
</ul>
<p class="last">Note: The default solver ‘adam’ works pretty well on relatively
large datasets (with thousands of training samples or more) in terms of
both training time and validation score.
For small datasets, however, ‘lbfgs’ can converge faster and perform
better.</p>
</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default 0.0001</span></dt>
<dd>L2 penalty (regularization term) parameter.</dd>
<dt>batch_size <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default ‘auto’</span></dt>
<dd>Size of minibatches for stochastic optimizers.
If the solver is ‘lbfgs’, the classifier will not use minibatch.
When set to “auto”, <cite>batch_size=min(200, n_samples)</cite></dd>
<dt>learning_rate <span class="classifier-delimiter">:</span> <span class="classifier">{‘constant’, ‘invscaling’, ‘adaptive’}, default ‘constant’</span></dt>
<dd><p class="first">Learning rate schedule for weight updates.</p>
<ul class="simple">
<li>‘constant’ is a constant learning rate given by
‘learning_rate_init’.</li>
<li>‘invscaling’ gradually decreases the learning rate at each
time step ‘t’ using an inverse scaling exponent of ‘power_t’.
effective_learning_rate = learning_rate_init / pow(t, power_t)</li>
<li>‘adaptive’ keeps the learning rate constant to
‘learning_rate_init’ as long as training loss keeps decreasing.
Each time two consecutive epochs fail to decrease training loss by at
least tol, or fail to increase validation score by at least tol if
‘early_stopping’ is on, the current learning rate is divided by 5.</li>
</ul>
<p class="last">Only used when <code class="docutils literal"><span class="pre">solver='sgd'</span></code>.</p>
</dd>
<dt>learning_rate_init <span class="classifier-delimiter">:</span> <span class="classifier">double, optional, default 0.001</span></dt>
<dd>The initial learning rate used. It controls the step-size
in updating the weights. Only used when solver=’sgd’ or ‘adam’.</dd>
<dt>power_t <span class="classifier-delimiter">:</span> <span class="classifier">double, optional, default 0.5</span></dt>
<dd>The exponent for inverse scaling learning rate.
It is used in updating effective learning rate when the learning_rate
is set to ‘invscaling’. Only used when solver=’sgd’.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default 200</span></dt>
<dd>Maximum number of iterations. The solver iterates until convergence
(determined by ‘tol’) or this number of iterations. For stochastic
solvers (‘sgd’, ‘adam’), note that this determines the number of epochs
(how many times each data point will be used), not the number of
gradient steps.</dd>
<dt>shuffle <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional, default True</span></dt>
<dd>Whether to shuffle samples in each iteration. Only used when
solver=’sgd’ or ‘adam’.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional, default None</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default 1e-4</span></dt>
<dd>Tolerance for the optimization. When the loss or score is not improving
by at least <code class="docutils literal"><span class="pre">tol</span></code> for <code class="docutils literal"><span class="pre">n_iter_no_change</span></code> consecutive iterations,
unless <code class="docutils literal"><span class="pre">learning_rate</span></code> is set to ‘adaptive’, convergence is
considered to be reached and training stops.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional, default False</span></dt>
<dd>Whether to print progress messages to stdout.</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional, default False</span></dt>
<dd>When set to True, reuse the solution of the previous
call to fit as initialization, otherwise, just erase the
previous solution. See <span class="xref std std-term">the Glossary</span>.</dd>
<dt>momentum <span class="classifier-delimiter">:</span> <span class="classifier">float, default 0.9</span></dt>
<dd>Momentum for gradient descent update. Should be between 0 and 1. Only
used when solver=’sgd’.</dd>
<dt>nesterovs_momentum <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default True</span></dt>
<dd>Whether to use Nesterov’s momentum. Only used when solver=’sgd’ and
momentum &gt; 0.</dd>
<dt>early_stopping <span class="classifier-delimiter">:</span> <span class="classifier">bool, default False</span></dt>
<dd>Whether to use early stopping to terminate training when validation
score is not improving. If set to true, it will automatically set
aside 10% of training data as validation and terminate training when
validation score is not improving by at least tol for
<code class="docutils literal"><span class="pre">n_iter_no_change</span></code> consecutive epochs.
Only effective when solver=’sgd’ or ‘adam’</dd>
<dt>validation_fraction <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default 0.1</span></dt>
<dd>The proportion of training data to set aside as validation set for
early stopping. Must be between 0 and 1.
Only used if early_stopping is True</dd>
<dt>beta_1 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default 0.9</span></dt>
<dd>Exponential decay rate for estimates of first moment vector in adam,
should be in [0, 1). Only used when solver=’adam’</dd>
<dt>beta_2 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default 0.999</span></dt>
<dd>Exponential decay rate for estimates of second moment vector in adam,
should be in [0, 1). Only used when solver=’adam’</dd>
<dt>epsilon <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default 1e-8</span></dt>
<dd>Value for numerical stability in adam. Only used when solver=’adam’</dd>
<dt>n_iter_no_change <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default 10</span></dt>
<dd><p class="first">Maximum number of epochs to not meet <code class="docutils literal"><span class="pre">tol</span></code> improvement.
Only effective when solver=’sgd’ or ‘adam’</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">classes_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array or list of array of shape (n_classes,)</span></dt>
<dd>Class labels for each output.</dd>
<dt><code class="docutils literal"><span class="pre">loss_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The current loss computed with the loss function.</dd>
<dt><code class="docutils literal"><span class="pre">coefs_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list, length n_layers - 1</span></dt>
<dd>The ith element in the list represents the weight matrix corresponding
to layer i.</dd>
<dt><code class="docutils literal"><span class="pre">intercepts_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list, length n_layers - 1</span></dt>
<dd>The ith element in the list represents the bias vector corresponding to
layer i + 1.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>The number of iterations the solver has ran.</dd>
<dt><code class="docutils literal"><span class="pre">n_layers_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of layers.</dd>
<dt><code class="docutils literal"><span class="pre">n_outputs_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of outputs.</dd>
<dt><code class="docutils literal"><span class="pre">out_activation_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">string</span></dt>
<dd>Name of the output activation function.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>MLPClassifier trains iteratively since at each time step
the partial derivatives of the loss function with respect to the model
parameters are computed to update the parameters.</p>
<p>It can also have a regularization term added to the loss function
that shrinks model parameters to prevent overfitting.</p>
<p>This implementation works with data represented as dense numpy arrays or
sparse scipy arrays of floating point values.</p>
<p><strong>References</strong></p>
<dl class="docutils">
<dt>Hinton, Geoffrey E.</dt>
<dd>“Connectionist learning procedures.” Artificial intelligence 40.1
(1989): 185-234.</dd>
<dt>Glorot, Xavier, and Yoshua Bengio. “Understanding the difficulty of</dt>
<dd>training deep feedforward neural networks.” International Conference
on Artificial Intelligence and Statistics. 2010.</dd>
<dt>He, Kaiming, et al. “Delving deep into rectifiers: Surpassing human-level</dt>
<dd>performance on imagenet classification.” arXiv preprint
arXiv:1502.01852 (2015).</dd>
<dt>Kingma, Diederik, and Jimmy Ba. “Adam: A method for stochastic</dt>
<dd>optimization.” arXiv preprint arXiv:1412.6980 (2014).</dd>
</dl>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#MLPClassifierScikitsLearnNode">MLPClassifierScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.KNeighborsClassifierScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">KNeighborsClassifierScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.KNeighborsClassifierScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Classifier implementing the k-nearest neighbors vote.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.neighbors.classification.KNeighborsClassifier</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_neighbors <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default = 5)</span></dt>
<dd>Number of neighbors to use by default for <code class="xref py py-meth docutils literal"><span class="pre">kneighbors()</span></code> queries.</dd>
<dt>weights <span class="classifier-delimiter">:</span> <span class="classifier">str or callable, optional (default = ‘uniform’)</span></dt>
<dd><p class="first">weight function used in prediction.  Possible values:</p>
<ul class="last simple">
<li>‘uniform’ : uniform weights.  All points in each neighborhood
are weighted equally.</li>
<li>‘distance’ : weight points by the inverse of their distance.
in this case, closer neighbors of a query point will have a
greater influence than neighbors which are further away.</li>
<li>[callable] : a user-defined function which accepts an
array of distances, and returns an array of the same shape
containing the weights.</li>
</ul>
</dd>
<dt>algorithm <span class="classifier-delimiter">:</span> <span class="classifier">{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, optional</span></dt>
<dd><p class="first">Algorithm used to compute the nearest neighbors:</p>
<ul class="simple">
<li>‘ball_tree’ will use <code class="xref py py-class docutils literal"><span class="pre">BallTree</span></code></li>
<li>‘kd_tree’ will use <code class="xref py py-class docutils literal"><span class="pre">KDTree</span></code></li>
<li>‘brute’ will use a brute-force search.</li>
<li>‘auto’ will attempt to decide the most appropriate algorithm
based on the values passed to <code class="xref py py-meth docutils literal"><span class="pre">fit()</span></code> method.</li>
</ul>
<p class="last">Note: fitting on sparse input will override the setting of
this parameter, using brute force.</p>
</dd>
<dt>leaf_size <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default = 30)</span></dt>
<dd>Leaf size passed to BallTree or KDTree.  This can affect the
speed of the construction and query, as well as the memory
required to store the tree.  The optimal value depends on the
nature of the problem.</dd>
<dt>p <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default = 2)</span></dt>
<dd>Power parameter for the Minkowski metric. When p = 1, this is
equivalent to using manhattan_distance (l1), and euclidean_distance
(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.</dd>
<dt>metric <span class="classifier-delimiter">:</span> <span class="classifier">string or callable, default ‘minkowski’</span></dt>
<dd>the distance metric to use for the tree.  The default metric is
minkowski, and with p=2 is equivalent to the standard Euclidean
metric. See the documentation of the DistanceMetric class for a
list of available metrics.</dd>
<dt>metric_params <span class="classifier-delimiter">:</span> <span class="classifier">dict, optional (default = None)</span></dt>
<dd>Additional keyword arguments for the metric function.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The number of parallel jobs to run for neighbors search.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.
Doesn’t affect <code class="xref py py-meth docutils literal"><span class="pre">fit()</span></code> method.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">KNeighborsClassifier(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">neigh</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">1.1</span><span class="p">]]))</span>
<span class="go">[0]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">neigh</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">([[</span><span class="mf">0.9</span><span class="p">]]))</span>
<span class="go">[[0.66666667 0.33333333]]</span>
</pre></div>
</div>
<p>See also</p>
<p>RadiusNeighborsClassifier
KNeighborsRegressor
RadiusNeighborsRegressor
NearestNeighbors</p>
<p><strong>Notes</strong></p>
<p>See <span class="xref std std-ref">Nearest Neighbors</span> in the online documentation
for a discussion of the choice of <code class="docutils literal"><span class="pre">algorithm</span></code> and <code class="docutils literal"><span class="pre">leaf_size</span></code>.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Regarding the Nearest Neighbors algorithms, if it is found that two
neighbors, neighbor <cite>k+1</cite> and <cite>k</cite>, have identical distances
but different labels, the results will depend on the ordering of the
training data.</p>
</div>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm">https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm</a></p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#KNeighborsClassifierScikitsLearnNode">KNeighborsClassifierScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.PowerTransformerScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">PowerTransformerScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.PowerTransformerScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply a power transform featurewise to make data more Gaussian-like.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.preprocessing.data.PowerTransformer</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Power transforms are a family of parametric, monotonic transformations
that are applied to make data more Gaussian-like. This is useful for
modeling issues related to heteroscedasticity (non-constant variance),
or other situations where normality is desired.</p>
<p>Currently, PowerTransformer supports the Box-Cox transform and the
Yeo-Johnson transform. The optimal parameter for stabilizing variance and
minimizing skewness is estimated through maximum likelihood.</p>
<p>Box-Cox requires input data to be strictly positive, while Yeo-Johnson
supports both positive or negative data.</p>
<p>By default, zero-mean, unit-variance normalization is applied to the
transformed data.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>method <span class="classifier-delimiter">:</span> <span class="classifier">str, (default=’yeo-johnson’)</span></dt>
<dd><p class="first">The power transform method. Available methods are:</p>
<ul class="last simple">
<li>‘yeo-johnson’ <a href="#id83"><span class="problematic" id="id6">[1]_</span></a>, works with positive and negative values</li>
<li>‘box-cox’ <a href="#id84"><span class="problematic" id="id7">[2]_</span></a>, only works with strictly positive values</li>
</ul>
</dd>
<dt>standardize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=True</span></dt>
<dd>Set to True to apply zero-mean, unit-variance normalization to the
transformed output.</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default=True</span></dt>
<dd>Set to False to perform inplace computation during transformation.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">lambdas_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of float, shape (n_features,)</span></dt>
<dd>The parameters of the power transformation for the selected features.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PowerTransformer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pt</span> <span class="o">=</span> <span class="n">PowerTransformer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">pt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
<span class="go">PowerTransformer(copy=True, method=&#39;yeo-johnson&#39;, standardize=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">pt</span><span class="o">.</span><span class="n">lambdas_</span><span class="p">)</span>
<span class="go">[ 1.386... -3.100...]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">pt</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
<span class="go">[[-1.316... -0.707...]</span>
<span class="go"> [ 0.209... -0.707...]</span>
<span class="go"> [ 1.106...  1.414...]]</span>
</pre></div>
</div>
<p>See also</p>
<p>power_transform : Equivalent function without the estimator API.</p>
<dl class="docutils">
<dt>QuantileTransformer <span class="classifier-delimiter">:</span> <span class="classifier">Maps data to a standard normal distribution with</span></dt>
<dd>the parameter <cite>output_distribution=’normal’</cite>.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>NaNs are treated as missing values: disregarded in <code class="docutils literal"><span class="pre">fit</span></code>, and maintained
in <code class="docutils literal"><span class="pre">transform</span></code>.</p>
<p>For a comparison of the different scalers, transformers, and normalizers,
see <span class="xref std std-ref">examples/preprocessing/plot_all_scaling.py</span>.</p>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id8" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>I.K. Yeo and R.A. Johnson, “A new family of power transformations to
improve normality or symmetry.” Biometrika, 87(4), pp.954-959,
(2000).</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id9" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>G.E.P. Box and D.R. Cox, “An Analysis of Transformations”, Journal
of the Royal Statistical Society B, 26, 211-252 (1964).</td></tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#PowerTransformerScikitsLearnNode">PowerTransformerScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.SparsePCAScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">SparsePCAScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.SparsePCAScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Sparse Principal Components Analysis (SparsePCA)
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.decomposition.sparse_pca.SparsePCA</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Finds the set of sparse components that can optimally reconstruct
the data.  The amount of sparseness is controllable by the coefficient
of the L1 penalty, given by the parameter alpha.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>Number of sparse atoms to extract.</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float,</span></dt>
<dd>Sparsity controlling parameter. Higher values lead to sparser
components.</dd>
<dt>ridge_alpha <span class="classifier-delimiter">:</span> <span class="classifier">float,</span></dt>
<dd>Amount of ridge shrinkage to apply in order to improve
conditioning when calling the transform method.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>Maximum number of iterations to perform.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float,</span></dt>
<dd>Tolerance for the stopping condition.</dd>
<dt>method <span class="classifier-delimiter">:</span> <span class="classifier">{‘lars’, ‘cd’}</span></dt>
<dd>lars: uses the least angle regression method to solve the lasso problem
(linear_model.lars_path)
cd: uses the coordinate descent method to compute the
Lasso solution (linear_model.Lasso). Lars will be faster if
the estimated components are sparse.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Number of parallel jobs to run.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>U_init <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_samples, n_components),</span></dt>
<dd>Initial values for the loadings for warm restart scenarios.</dd>
<dt>V_init <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_components, n_features),</span></dt>
<dd>Initial values for the components for warm restart scenarios.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Controls the verbosity; the higher, the more messages. Defaults to 0.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>normalize_components <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=False)</span></dt>
<dd><ul class="first simple">
<li>if False, use a version of Sparse PCA without components
normalization and without data centering. This is likely a bug and
even though it’s the default for backward compatibility,
this should not be used.</li>
<li>if True, use a version of Sparse PCA with components normalization
and data centering.</li>
</ul>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
<div class="last deprecated">
<p><span class="versionmodified">Deprecated since version 0.22: </span><code class="docutils literal"><span class="pre">normalize_components</span></code> was added and set to <code class="docutils literal"><span class="pre">False</span></code> for
backward compatibility. It would be set to <code class="docutils literal"><span class="pre">True</span></code> from 0.22
onwards.</p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">components_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>Sparse components extracted from the data.</dd>
<dt><code class="docutils literal"><span class="pre">error_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array</span></dt>
<dd>Vector of errors at each iteration.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of iterations run.</dd>
<dt><code class="docutils literal"><span class="pre">mean_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,)</span></dt>
<dd>Per-feature empirical mean, estimated from the training set.
Equal to <code class="docutils literal"><span class="pre">X.mean(axis=0)</span></code>.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_friedman1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">SparsePCA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">make_friedman1</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span> <span class="o">=</span> <span class="n">SparsePCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">normalize_components</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> 
<span class="go">SparsePCA(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_transformed</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_transformed</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(200, 5)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># most values in the ``components_`` are zero (sparsity)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">transformer</span><span class="o">.</span><span class="n">components_</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> 
<span class="go">0.9666...</span>
</pre></div>
</div>
<p>See also</p>
<p>PCA
MiniBatchSparsePCA
DictionaryLearning</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#SparsePCAScikitsLearnNode">SparsePCAScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.ExtraTreeRegressorScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">ExtraTreeRegressorScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.ExtraTreeRegressorScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>An extremely randomized tree regressor.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.tree.tree.ExtraTreeRegressor</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Extra-trees differ from classic decision trees in the way they are built.
When looking for the best split to separate the samples of a node into two
groups, random splits are drawn for each of the <cite>max_features</cite> randomly
selected features and the best split among those is chosen. When
<cite>max_features</cite> is set 1, this amounts to building a totally random
decision tree.</p>
<p>Warning: Extra-trees should only be used within ensemble methods.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>criterion <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=”mse”)</span></dt>
<dd><p class="first">The function to measure the quality of a split. Supported criteria
are “mse” for the mean squared error, which is equal to variance
reduction as feature selection criterion, and “mae” for the mean
absolute error.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.18: </span>Mean Absolute Error (MAE) criterion.</p>
</div>
</dd>
<dt>splitter <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=”random”)</span></dt>
<dd>The strategy used to choose the split at each node. Supported
strategies are “best” to choose the best split and “random” to choose
the best random split.</dd>
<dt>max_depth <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</dd>
<dt>min_samples_split <span class="classifier-delimiter">:</span> <span class="classifier">int, float, optional (default=2)</span></dt>
<dd><p class="first">The minimum number of samples required to split an internal node:</p>
<ul class="simple">
<li>If int, then consider <cite>min_samples_split</cite> as the minimum number.</li>
<li>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</li>
</ul>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</dd>
<dt>min_samples_leaf <span class="classifier-delimiter">:</span> <span class="classifier">int, float, optional (default=1)</span></dt>
<dd><p class="first">The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul class="simple">
<li>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</li>
<li>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</li>
</ul>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</dd>
<dt>min_weight_fraction_leaf <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span></dt>
<dd>The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int, float, string or None, optional (default=”auto”)</span></dt>
<dd><p class="first">The number of features to consider when looking for the best split:</p>
<ul class="simple">
<li>If int, then consider <cite>max_features</cite> features at each split.</li>
<li>If float, then <cite>max_features</cite> is a fraction and
<cite>int(max_features * n_features)</cite> features are considered at each
split.</li>
<li>If “auto”, then <cite>max_features=n_features</cite>.</li>
<li>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite>.</li>
<li>If “log2”, then <cite>max_features=log2(n_features)</cite>.</li>
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
<p class="last">Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal"><span class="pre">max_features</span></code> features.</p>
</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>min_impurity_decrease <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span></dt>
<dd><p class="first">A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal"><span class="pre">N</span></code>, <code class="docutils literal"><span class="pre">N_t</span></code>, <code class="docutils literal"><span class="pre">N_t_R</span></code> and <code class="docutils literal"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19.</span></p>
</div>
</dd>
<dt>min_impurity_split <span class="classifier-delimiter">:</span> <span class="classifier">float, (default=1e-7)</span></dt>
<dd><p class="first">Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="last deprecated">
<p><span class="versionmodified">Deprecated since version 0.19: </span><code class="docutils literal"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal"><span class="pre">min_impurity_split</span></code> will change from 1e-7 to 0 in 0.23 and it
will be removed in 0.25. Use <code class="docutils literal"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</dd>
<dt>max_leaf_nodes <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Grow a tree with <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</dd>
</dl>
<p>See also</p>
<p>ExtraTreeClassifier, sklearn.ensemble.ExtraTreesClassifier,
sklearn.ensemble.ExtraTreesRegressor</p>
<p><strong>Notes</strong></p>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code class="docutils literal"><span class="pre">max_depth</span></code>, <code class="docutils literal"><span class="pre">min_samples_leaf</span></code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id10" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized trees”,
Machine Learning, 63(1), 3-42, 2006.</td></tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#ExtraTreeRegressorScikitsLearnNode">ExtraTreeRegressorScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.ExtraTreesClassifierScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">ExtraTreesClassifierScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.ExtraTreesClassifierScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>An extra-trees classifier.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.ensemble.forest.ExtraTreesClassifier</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
This class implements a meta estimator that fits a number of
randomized decision trees (a.k.a. extra-trees) on various sub-samples
of the dataset and uses averaging to improve the predictive accuracy
and control over-fitting.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_estimators <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=10)</span></dt>
<dd><p class="first">The number of trees in the forest.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.20: </span>The default value of <code class="docutils literal"><span class="pre">n_estimators</span></code> will change from 10 in
version 0.20 to 100 in version 0.22.</p>
</div>
</dd>
<dt>criterion <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=”gini”)</span></dt>
<dd>The function to measure the quality of a split. Supported criteria are
“gini” for the Gini impurity and “entropy” for the information gain.</dd>
<dt>max_depth <span class="classifier-delimiter">:</span> <span class="classifier">integer or None, optional (default=None)</span></dt>
<dd>The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</dd>
<dt>min_samples_split <span class="classifier-delimiter">:</span> <span class="classifier">int, float, optional (default=2)</span></dt>
<dd><p class="first">The minimum number of samples required to split an internal node:</p>
<ul class="simple">
<li>If int, then consider <cite>min_samples_split</cite> as the minimum number.</li>
<li>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</li>
</ul>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</dd>
<dt>min_samples_leaf <span class="classifier-delimiter">:</span> <span class="classifier">int, float, optional (default=1)</span></dt>
<dd><p class="first">The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul class="simple">
<li>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</li>
<li>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</li>
</ul>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</dd>
<dt>min_weight_fraction_leaf <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span></dt>
<dd>The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int, float, string or None, optional (default=”auto”)</span></dt>
<dd><p class="first">The number of features to consider when looking for the best split:</p>
<ul class="simple">
<li>If int, then consider <cite>max_features</cite> features at each split.</li>
<li>If float, then <cite>max_features</cite> is a fraction and
<cite>int(max_features * n_features)</cite> features are considered at each
split.</li>
<li>If “auto”, then <cite>max_features=sqrt(n_features)</cite>.</li>
<li>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite>.</li>
<li>If “log2”, then <cite>max_features=log2(n_features)</cite>.</li>
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
<p class="last">Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal"><span class="pre">max_features</span></code> features.</p>
</dd>
<dt>max_leaf_nodes <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Grow trees with <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</dd>
<dt>min_impurity_decrease <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span></dt>
<dd><p class="first">A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal"><span class="pre">N</span></code>, <code class="docutils literal"><span class="pre">N_t</span></code>, <code class="docutils literal"><span class="pre">N_t_R</span></code> and <code class="docutils literal"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19.</span></p>
</div>
</dd>
<dt>min_impurity_split <span class="classifier-delimiter">:</span> <span class="classifier">float, (default=1e-7)</span></dt>
<dd><p class="first">Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="last deprecated">
<p><span class="versionmodified">Deprecated since version 0.19: </span><code class="docutils literal"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal"><span class="pre">min_impurity_split</span></code> will change from 1e-7 to 0 in 0.23 and it
will be removed in 0.25. Use <code class="docutils literal"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</dd>
<dt>bootstrap <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=False)</span></dt>
<dd>Whether bootstrap samples are used when building trees. If False, the
whole datset is used to build each tree.</dd>
<dt>oob_score <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default=False)</span></dt>
<dd>Whether to use out-of-bag samples to estimate
the generalization accuracy.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The number of jobs to run in parallel for both <cite>fit</cite> and <cite>predict</cite>.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=0)</span></dt>
<dd>Controls the verbosity when fitting and predicting.</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default=False)</span></dt>
<dd>When set to <code class="docutils literal"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit a whole
new forest. See <span class="xref std std-term">the Glossary</span>.</dd>
<dt>class_weight <span class="classifier-delimiter">:</span> <span class="classifier">dict, list of dicts, “balanced”, “balanced_subsample” or     None, optional (default=None)</span></dt>
<dd><p class="first">Weights associated with classes in the form <code class="docutils literal"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
If not given, all classes are supposed to have weight one. For
multi-output problems, a list of dicts can be provided in the same
order as the columns of y.</p>
<p>Note that for multioutput (including multilabel) weights should be
defined for each class of every column in its own dict. For example,
for four-class multilabel classification weights should be
[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
[{1:1}, {2:5}, {3:1}, {4:1}].</p>
<p>The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
<p>The “balanced_subsample” mode is the same as “balanced” except that weights are
computed based on the bootstrap sample for every tree grown.</p>
<p>For multi-output, the weights of each column of y will be multiplied.</p>
<p class="last">Note that these weights will be multiplied with sample_weight (passed
through the fit method) if sample_weight is specified.</p>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">estimators_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list of DecisionTreeClassifier</span></dt>
<dd>The collection of fitted sub-estimators.</dd>
<dt><code class="docutils literal"><span class="pre">classes_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_classes] or a list of such arrays</span></dt>
<dd>The classes labels (single output problem), or a list of arrays of
class labels (multi-output problem).</dd>
<dt><code class="docutils literal"><span class="pre">n_classes_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int or list</span></dt>
<dd>The number of classes (single output problem), or a list containing the
number of classes for each output (multi-output problem).</dd>
<dt><code class="docutils literal"><span class="pre">feature_importances_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span></dt>
<dd>The feature importances (the higher, the more important the feature).</dd>
<dt><code class="docutils literal"><span class="pre">n_features_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The number of features when <code class="docutils literal"><span class="pre">fit</span></code> is performed.</dd>
<dt><code class="docutils literal"><span class="pre">n_outputs_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The number of outputs when <code class="docutils literal"><span class="pre">fit</span></code> is performed.</dd>
<dt><code class="docutils literal"><span class="pre">oob_score_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Score of the training dataset obtained using an out-of-bag estimate.</dd>
<dt><code class="docutils literal"><span class="pre">oob_decision_function_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_samples, n_classes]</span></dt>
<dd>Decision function computed with out-of-bag estimate on the training
set. If n_estimators is small it might be possible that a data point
was never left out during the bootstrap. In this case,
<cite>oob_decision_function_</cite> might contain NaN.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code class="docutils literal"><span class="pre">max_depth</span></code>, <code class="docutils literal"><span class="pre">min_samples_leaf</span></code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id11" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized
trees”, Machine Learning, 63(1), 3-42, 2006.</td></tr>
</tbody>
</table>
<p>See also</p>
<p>sklearn.tree.ExtraTreeClassifier : Base classifier for this ensemble.
RandomForestClassifier : Ensemble Classifier based on trees with optimal</p>
<blockquote>
<div>splits.</div></blockquote>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#ExtraTreesClassifierScikitsLearnNode">ExtraTreesClassifierScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.GridSearchCVScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">GridSearchCVScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.GridSearchCVScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Exhaustive search over specified parameter values for an estimator.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.model_selection._search.GridSearchCV</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Important members are fit, predict.</p>
<p>GridSearchCV implements a “fit” and a “score” method.
It also implements “predict”, “predict_proba”, “decision_function”,
“transform” and “inverse_transform” if they are implemented in the
estimator used.</p>
<p>The parameters of the estimator used to apply these methods are optimized
by cross-validated grid-search over a parameter grid.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>estimator <span class="classifier-delimiter">:</span> <span class="classifier">estimator object.</span></dt>
<dd>This is assumed to implement the scikit-learn estimator interface.
Either estimator needs to provide a <code class="docutils literal"><span class="pre">score</span></code> function,
or <code class="docutils literal"><span class="pre">scoring</span></code> must be passed.</dd>
<dt>param_grid <span class="classifier-delimiter">:</span> <span class="classifier">dict or list of dictionaries</span></dt>
<dd>Dictionary with parameters names (string) as keys and lists of
parameter settings to try as values, or a list of such
dictionaries, in which case the grids spanned by each dictionary
in the list are explored. This enables searching over any sequence
of parameter settings.</dd>
<dt>scoring <span class="classifier-delimiter">:</span> <span class="classifier">string, callable, list/tuple, dict or None, default: None</span></dt>
<dd><p class="first">A single string (see <span class="xref std std-ref">scoring_parameter</span>) or a callable
(see <span class="xref std std-ref">scoring</span>) to evaluate the predictions on the test set.</p>
<p>For evaluating multiple metrics, either give a list of (unique) strings
or a dict with names as keys and callables as values.</p>
<p>NOTE that when using custom scorers, each scorer should return a single
value. Metric functions returning a list/array of values can be wrapped
into multiple scorers that return one value each.</p>
<p>See <span class="xref std std-ref">multimetric_grid_search</span> for an example.</p>
<p class="last">If None, the estimator’s default scorer (if available) is used.</p>
</dd>
<dt>fit_params <span class="classifier-delimiter">:</span> <span class="classifier">dict, optional</span></dt>
<dd><p class="first">Parameters to pass to the fit method.</p>
<div class="last deprecated">
<p><span class="versionmodified">Deprecated since version 0.19: </span><code class="docutils literal"><span class="pre">fit_params</span></code> as a constructor argument was deprecated in version
0.19 and will be removed in version 0.21. Pass fit parameters to
the <code class="docutils literal"><span class="pre">fit</span></code> method instead.</p>
</div>
</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Number of jobs to run in parallel.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>pre_dispatch <span class="classifier-delimiter">:</span> <span class="classifier">int, or string, optional</span></dt>
<dd><p class="first">Controls the number of jobs that get dispatched during parallel
execution. Reducing this number can be useful to avoid an
explosion of memory consumption when more jobs get dispatched
than CPUs can process. This parameter can be:</p>
<blockquote class="last">
<div><ul class="simple">
<li>None, in which case all the jobs are immediately
created and spawned. Use this for lightweight and
fast-running jobs, to avoid delays due to on-demand
spawning of the jobs</li>
<li>An int, giving the exact number of total jobs that are
spawned</li>
<li>A string, giving an expression as a function of n_jobs,
as in ‘2*n_jobs’</li>
</ul>
</div></blockquote>
</dd>
<dt>iid <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=’warn’</span></dt>
<dd><p class="first">If True, return the average score across folds, weighted by the number
of samples in each test set. In this case, the data is assumed to be
identically distributed across the folds, and the loss minimized is
the total loss per sample, and not the mean loss across the folds. If
False, return the average score across folds. Default is True, but
will change to False in version 0.21, to correspond to the standard
definition of cross-validation.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.20: </span>Parameter <code class="docutils literal"><span class="pre">iid</span></code> will change from True to False by default in
version 0.22, and will be removed in 0.24.</p>
</div>
</dd>
<dt>cv <span class="classifier-delimiter">:</span> <span class="classifier">int, cross-validation generator or an iterable, optional</span></dt>
<dd><p class="first">Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul class="simple">
<li>None, to use the default 3-fold cross validation,</li>
<li>integer, to specify the number of folds in a <cite>(Stratified)KFold</cite>,</li>
<li><span class="xref std std-term">CV splitter</span>,</li>
<li>An iterable yielding (train, test) splits as arrays of indices.</li>
</ul>
<p>For integer/None inputs, if the estimator is a classifier and <code class="docutils literal"><span class="pre">y</span></code> is
either binary or multiclass, <code class="xref py py-class docutils literal"><span class="pre">StratifiedKFold</span></code> is used. In all
other cases, <code class="xref py py-class docutils literal"><span class="pre">KFold</span></code> is used.</p>
<p>Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.20: </span><code class="docutils literal"><span class="pre">cv</span></code> default value if None will change from 3-fold to 5-fold
in v0.22.</p>
</div>
</dd>
<dt>refit <span class="classifier-delimiter">:</span> <span class="classifier">boolean, or string, default=True</span></dt>
<dd><p class="first">Refit an estimator using the best found parameters on the whole
dataset.</p>
<p>For multiple metric evaluation, this needs to be a string denoting the
scorer is used to find the best parameters for refitting the estimator
at the end.</p>
<p>The refitted estimator is made available at the <code class="docutils literal"><span class="pre">best_estimator_</span></code>
attribute and permits using <code class="docutils literal"><span class="pre">predict</span></code> directly on this
<code class="docutils literal"><span class="pre">GridSearchCV</span></code> instance.</p>
<p>Also for multiple metric evaluation, the attributes <code class="docutils literal"><span class="pre">best_index_</span></code>,
<code class="docutils literal"><span class="pre">best_score_</span></code> and <code class="docutils literal"><span class="pre">best_params_</span></code> will only be available if
<code class="docutils literal"><span class="pre">refit</span></code> is set and all of them will be determined w.r.t this specific
scorer.</p>
<p class="last">See <code class="docutils literal"><span class="pre">scoring</span></code> parameter to know more about multiple metric
evaluation.</p>
</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd>Controls the verbosity: the higher, the more messages.</dd>
<dt>error_score <span class="classifier-delimiter">:</span> <span class="classifier">‘raise’ or numeric</span></dt>
<dd>Value to assign to the score if an error occurs in estimator fitting.
If set to ‘raise’, the error is raised. If a numeric value is given,
FitFailedWarning is raised. This parameter does not affect the refit
step, which will always raise the error. Default is ‘raise’ but from
version 0.22 it will change to np.nan.</dd>
<dt>return_train_score <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd><p class="first">If <code class="docutils literal"><span class="pre">False</span></code>, the <code class="docutils literal"><span class="pre">cv_results_</span></code> attribute will not include training
scores.</p>
<p class="last">Current default is <code class="docutils literal"><span class="pre">'warn'</span></code>, which behaves as <code class="docutils literal"><span class="pre">True</span></code> in addition
to raising a warning when a training score is looked up.
That default will be changed to <code class="docutils literal"><span class="pre">False</span></code> in 0.21.
Computing training scores is used to get insights on how different
parameter settings impact the overfitting/underfitting trade-off.
However computing the scores on the training set can be computationally
expensive and is not strictly required to select the parameters that
yield the best generalization performance.</p>
</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span><span class="p">,</span> <span class="n">datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;kernel&#39;</span><span class="p">:(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="s1">&#39;rbf&#39;</span><span class="p">),</span> <span class="s1">&#39;C&#39;</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">]}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="s2">&quot;scale&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">svc</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="gp">... </span>                            
<span class="go">GridSearchCV(cv=5, error_score=...,</span>
<span class="go">       estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,</span>
<span class="go">                     decision_function_shape=&#39;ovr&#39;, degree=..., gamma=...,</span>
<span class="go">                     kernel=&#39;rbf&#39;, max_iter=-1, probability=False,</span>
<span class="go">                     random_state=None, shrinking=True, tol=...,</span>
<span class="go">                     verbose=False),</span>
<span class="go">       fit_params=None, iid=..., n_jobs=None,</span>
<span class="go">       param_grid=..., pre_dispatch=..., refit=..., return_train_score=...,</span>
<span class="go">       scoring=..., verbose=...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">cv_results_</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="gp">... </span>                            
<span class="go">[&#39;mean_fit_time&#39;, &#39;mean_score_time&#39;, &#39;mean_test_score&#39;,...</span>
<span class="go"> &#39;mean_train_score&#39;, &#39;param_C&#39;, &#39;param_kernel&#39;, &#39;params&#39;,...</span>
<span class="go"> &#39;rank_test_score&#39;, &#39;split0_test_score&#39;,...</span>
<span class="go"> &#39;split0_train_score&#39;, &#39;split1_test_score&#39;, &#39;split1_train_score&#39;,...</span>
<span class="go"> &#39;split2_test_score&#39;, &#39;split2_train_score&#39;,...</span>
<span class="go"> &#39;std_fit_time&#39;, &#39;std_score_time&#39;, &#39;std_test_score&#39;, &#39;std_train_score&#39;...]</span>
</pre></div>
</div>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">cv_results_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">dict of numpy (masked) ndarrays</span></dt>
<dd><p class="first">A dict with keys as column headers and values as columns, that can be
imported into a pandas <code class="docutils literal"><span class="pre">DataFrame</span></code>.</p>
<p>For instance the below given table</p>
<table border="1" class="docutils">
<colgroup>
<col width="19%" />
<col width="17%" />
<col width="19%" />
<col width="27%" />
<col width="5%" />
<col width="14%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">param_kernel</th>
<th class="head">param_gamma</th>
<th class="head">param_degree</th>
<th class="head">split0_test_score</th>
<th class="head">…</th>
<th class="head">rank_t…</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>‘poly’</td>
<td>–</td>
<td>2</td>
<td>0.80</td>
<td>…</td>
<td>2</td>
</tr>
<tr class="row-odd"><td>‘poly’</td>
<td>–</td>
<td>3</td>
<td>0.70</td>
<td>…</td>
<td>4</td>
</tr>
<tr class="row-even"><td>‘rbf’</td>
<td>0.1</td>
<td>–</td>
<td>0.80</td>
<td>…</td>
<td>3</td>
</tr>
<tr class="row-odd"><td>‘rbf’</td>
<td>0.2</td>
<td>–</td>
<td>0.93</td>
<td>…</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>will be represented by a <code class="docutils literal"><span class="pre">cv_results_</span></code> dict of:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="s1">&#39;param_kernel&#39;</span><span class="p">:</span> <span class="n">masked_array</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="s1">&#39;rbf&#39;</span><span class="p">],</span>
                             <span class="n">mask</span> <span class="o">=</span> <span class="p">[</span><span class="kc">False</span> <span class="kc">False</span> <span class="kc">False</span> <span class="kc">False</span><span class="p">]</span><span class="o">...</span><span class="p">)</span>
<span class="s1">&#39;param_gamma&#39;</span><span class="p">:</span> <span class="n">masked_array</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="o">--</span> <span class="o">--</span> <span class="mf">0.1</span> <span class="mf">0.2</span><span class="p">],</span>
                            <span class="n">mask</span> <span class="o">=</span> <span class="p">[</span> <span class="kc">True</span>  <span class="kc">True</span> <span class="kc">False</span> <span class="kc">False</span><span class="p">]</span><span class="o">...</span><span class="p">),</span>
<span class="s1">&#39;param_degree&#39;</span><span class="p">:</span> <span class="n">masked_array</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.0</span> <span class="mf">3.0</span> <span class="o">--</span> <span class="o">--</span><span class="p">],</span>
                             <span class="n">mask</span> <span class="o">=</span> <span class="p">[</span><span class="kc">False</span> <span class="kc">False</span>  <span class="kc">True</span>  <span class="kc">True</span><span class="p">]</span><span class="o">...</span><span class="p">),</span>
<span class="s1">&#39;split0_test_score&#39;</span>  <span class="p">:</span> <span class="p">[</span><span class="mf">0.80</span><span class="p">,</span> <span class="mf">0.70</span><span class="p">,</span> <span class="mf">0.80</span><span class="p">,</span> <span class="mf">0.93</span><span class="p">],</span>
<span class="s1">&#39;split1_test_score&#39;</span>  <span class="p">:</span> <span class="p">[</span><span class="mf">0.82</span><span class="p">,</span> <span class="mf">0.50</span><span class="p">,</span> <span class="mf">0.70</span><span class="p">,</span> <span class="mf">0.78</span><span class="p">],</span>
<span class="s1">&#39;mean_test_score&#39;</span>    <span class="p">:</span> <span class="p">[</span><span class="mf">0.81</span><span class="p">,</span> <span class="mf">0.60</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.85</span><span class="p">],</span>
<span class="s1">&#39;std_test_score&#39;</span>     <span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.08</span><span class="p">],</span>
<span class="s1">&#39;rank_test_score&#39;</span>    <span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="s1">&#39;split0_train_score&#39;</span> <span class="p">:</span> <span class="p">[</span><span class="mf">0.80</span><span class="p">,</span> <span class="mf">0.92</span><span class="p">,</span> <span class="mf">0.70</span><span class="p">,</span> <span class="mf">0.93</span><span class="p">],</span>
<span class="s1">&#39;split1_train_score&#39;</span> <span class="p">:</span> <span class="p">[</span><span class="mf">0.82</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.70</span><span class="p">,</span> <span class="mf">0.87</span><span class="p">],</span>
<span class="s1">&#39;mean_train_score&#39;</span>   <span class="p">:</span> <span class="p">[</span><span class="mf">0.81</span><span class="p">,</span> <span class="mf">0.74</span><span class="p">,</span> <span class="mf">0.70</span><span class="p">,</span> <span class="mf">0.90</span><span class="p">],</span>
<span class="s1">&#39;std_train_score&#39;</span>    <span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.19</span><span class="p">,</span> <span class="mf">0.00</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">],</span>
<span class="s1">&#39;mean_fit_time&#39;</span>      <span class="p">:</span> <span class="p">[</span><span class="mf">0.73</span><span class="p">,</span> <span class="mf">0.63</span><span class="p">,</span> <span class="mf">0.43</span><span class="p">,</span> <span class="mf">0.49</span><span class="p">],</span>
<span class="s1">&#39;std_fit_time&#39;</span>       <span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">],</span>
<span class="s1">&#39;mean_score_time&#39;</span>    <span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.06</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">],</span>
<span class="s1">&#39;std_score_time&#39;</span>     <span class="p">:</span> <span class="p">[</span><span class="mf">0.00</span><span class="p">,</span> <span class="mf">0.00</span><span class="p">,</span> <span class="mf">0.00</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">],</span>
<span class="s1">&#39;params&#39;</span>             <span class="p">:</span> <span class="p">[{</span><span class="s1">&#39;kernel&#39;</span><span class="p">:</span> <span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="s1">&#39;degree&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span> <span class="o">...</span><span class="p">],</span>
<span class="p">}</span>
</pre></div>
</div>
<p>NOTE</p>
<p>The key <code class="docutils literal"><span class="pre">'params'</span></code> is used to store a list of parameter
settings dicts for all the parameter candidates.</p>
<p>The <code class="docutils literal"><span class="pre">mean_fit_time</span></code>, <code class="docutils literal"><span class="pre">std_fit_time</span></code>, <code class="docutils literal"><span class="pre">mean_score_time</span></code> and
<code class="docutils literal"><span class="pre">std_score_time</span></code> are all in seconds.</p>
<p class="last">For multi-metric evaluation, the scores for all the scorers are
available in the <code class="docutils literal"><span class="pre">cv_results_</span></code> dict at the keys ending with that
scorer’s name (<code class="docutils literal"><span class="pre">'_&lt;scorer_name&gt;'</span></code>) instead of <code class="docutils literal"><span class="pre">'_score'</span></code> shown
above. (‘split0_test_precision’, ‘mean_train_precision’ etc.)</p>
</dd>
<dt><code class="docutils literal"><span class="pre">best_estimator_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">estimator or dict</span></dt>
<dd><p class="first">Estimator that was chosen by the search, i.e. estimator
which gave highest score (or smallest loss if specified)
on the left out data. Not available if <code class="docutils literal"><span class="pre">refit=False</span></code>.</p>
<p class="last">See <code class="docutils literal"><span class="pre">refit</span></code> parameter for more information on allowed values.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">best_score_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd><p class="first">Mean cross-validated score of the best_estimator</p>
<p class="last">For multi-metric evaluation, this is present only if <code class="docutils literal"><span class="pre">refit</span></code> is
specified.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">best_params_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd><p class="first">Parameter setting that gave the best results on the hold out data.</p>
<p class="last">For multi-metric evaluation, this is present only if <code class="docutils literal"><span class="pre">refit</span></code> is
specified.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">best_index_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd><p class="first">The index (of the <code class="docutils literal"><span class="pre">cv_results_</span></code> arrays) which corresponds to the best
candidate parameter setting.</p>
<p>The dict at <code class="docutils literal"><span class="pre">search.cv_results_['params'][search.best_index_]</span></code> gives
the parameter setting for the best model, that gives the highest
mean score (<code class="docutils literal"><span class="pre">search.best_score_</span></code>).</p>
<p class="last">For multi-metric evaluation, this is present only if <code class="docutils literal"><span class="pre">refit</span></code> is
specified.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">scorer_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">function or a dict</span></dt>
<dd><p class="first">Scorer function used on the held out data to choose the best
parameters for the model.</p>
<p class="last">For multi-metric evaluation, this attribute holds the validated
<code class="docutils literal"><span class="pre">scoring</span></code> dict which maps the scorer key to the scorer callable.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">n_splits_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The number of cross-validation splits (folds/iterations).</dd>
<dt><code class="docutils literal"><span class="pre">refit_time_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd><p class="first">Seconds used for refitting the best model on the whole dataset.</p>
<p class="last">This is present only if <code class="docutils literal"><span class="pre">refit</span></code> is not False.</p>
</dd>
</dl>
<p><strong>Notes</strong></p>
<p>The parameters selected are those that maximize the score of the left out
data, unless an explicit score is passed in which case it is used instead.</p>
<p>If <cite>n_jobs</cite> was set to a value higher than one, the data is copied for each
point in the grid (and not <cite>n_jobs</cite> times). This is done for efficiency
reasons if individual jobs take very little time, but may raise errors if
the dataset is large and not enough memory is available.  A workaround in
this case is to set <cite>pre_dispatch</cite>. Then, the memory is copied only
<cite>pre_dispatch</cite> many times. A reasonable value for <cite>pre_dispatch</cite> is <cite>2 *
n_jobs</cite>.</p>
<p>See Also</p>
<p><code class="xref py py-class docutils literal"><span class="pre">ParameterGrid</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li>generates all the combinations of a hyperparameter grid.</li>
</ul>
</div></blockquote>
<p><code class="xref py py-func docutils literal"><span class="pre">sklearn.model_selection.train_test_split()</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li>utility function to split the data into a development set usable</li>
<li>for fitting a GridSearchCV instance and an evaluation set for</li>
<li>its final evaluation.</li>
</ul>
</div></blockquote>
<p><code class="xref py py-func docutils literal"><span class="pre">sklearn.metrics.make_scorer()</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li>Make a scorer from a performance metric or loss function.</li>
</ul>
</div></blockquote>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#GridSearchCVScikitsLearnNode">GridSearchCVScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.LassoCVScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">LassoCVScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.LassoCVScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Lasso linear model with iterative fitting along a regularization path.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.coordinate_descent.LassoCV</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
See glossary entry for <span class="xref std std-term">cross-validation estimator</span>.</p>
<p>The best model is selected by cross-validation.</p>
<p>The optimization objective for Lasso is:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2_2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
</pre></div>
</div>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>eps <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Length of the path. <code class="docutils literal"><span class="pre">eps=1e-3</span></code> means that
<code class="docutils literal"><span class="pre">alpha_min</span> <span class="pre">/</span> <span class="pre">alpha_max</span> <span class="pre">=</span> <span class="pre">1e-3</span></code>.</dd>
<dt>n_alphas <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Number of alphas along the regularization path</dd>
<dt>alphas <span class="classifier-delimiter">:</span> <span class="classifier">numpy array, optional</span></dt>
<dd>List of alphas where to compute the models.
If <code class="docutils literal"><span class="pre">None</span></code> alphas are set automatically</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default True</span></dt>
<dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span></dt>
<dd>This parameter is ignored when <code class="docutils literal"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<code class="xref py py-class docutils literal"><span class="pre">sklearn.preprocessing.StandardScaler</span></code> before calling <code class="docutils literal"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal"><span class="pre">normalize=False</span></code>.</dd>
<dt>precompute <span class="classifier-delimiter">:</span> <span class="classifier">True | False | ‘auto’ | array-like</span></dt>
<dd>Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal"><span class="pre">'auto'</span></code> let us decide. The Gram
matrix can also be passed as argument.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>The maximum number of iterations</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The tolerance for the optimization: if the updates are
smaller than <code class="docutils literal"><span class="pre">tol</span></code>, the optimization code checks the
dual gap for optimality and continues until it is smaller
than <code class="docutils literal"><span class="pre">tol</span></code>.</dd>
<dt>copy_X <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>If <code class="docutils literal"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</dd>
<dt>cv <span class="classifier-delimiter">:</span> <span class="classifier">int, cross-validation generator or an iterable, optional</span></dt>
<dd><p class="first">Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul class="simple">
<li>None, to use the default 3-fold cross-validation,</li>
<li>integer, to specify the number of folds.</li>
<li><span class="xref std std-term">CV splitter</span>,</li>
<li>An iterable yielding (train, test) splits as arrays of indices.</li>
</ul>
<p>For integer/None inputs, <code class="xref py py-class docutils literal"><span class="pre">KFold</span></code> is used.</p>
<p>Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.20: </span><code class="docutils literal"><span class="pre">cv</span></code> default value if None will change from 3-fold to 5-fold
in v0.22.</p>
</div>
</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">bool or integer</span></dt>
<dd>Amount of verbosity.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Number of CPUs to use during the cross validation.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>positive <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd>If positive, restrict regression coefficients to be positive</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional, default None</span></dt>
<dd>The seed of the pseudo random number generator that selects a random
feature to update.  If int, random_state is the seed used by the random
number generator; If RandomState instance, random_state is the random
number generator; If None, the random number generator is the
RandomState instance used by <cite>np.random</cite>. Used when <code class="docutils literal"><span class="pre">selection</span></code> ==
‘random’.</dd>
<dt>selection <span class="classifier-delimiter">:</span> <span class="classifier">str, default ‘cyclic’</span></dt>
<dd>If set to ‘random’, a random coefficient is updated every iteration
rather than looping over features sequentially by default. This
(setting to ‘random’) often leads to significantly faster convergence
especially when tol is higher than 1e-4.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">alpha_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The amount of penalization chosen by cross validation</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,) | (n_targets, n_features)</span></dt>
<dd>parameter vector (w in the cost function formula)</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float | array, shape (n_targets,)</span></dt>
<dd>independent term in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">mse_path_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_alphas, n_folds)</span></dt>
<dd>mean square error for the test set on each fold, varying alpha</dd>
<dt><code class="docutils literal"><span class="pre">alphas_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">numpy array, shape (n_alphas,)</span></dt>
<dd>The grid of alphas used for fitting</dd>
<dt><code class="docutils literal"><span class="pre">dual_gap_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">ndarray, shape ()</span></dt>
<dd>The dual gap at the end of the optimization for the optimal alpha
(<code class="docutils literal"><span class="pre">alpha_</span></code>).</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>number of iterations run by the coordinate descent solver to reach
the specified tolerance for the optimal alpha.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LassoCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">noise</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">LassoCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">0.9993...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">1</span><span class="p">,])</span>
<span class="go">array([-78.4951...])</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>For an example, see
<span class="xref std std-ref">examples/linear_model/plot_lasso_model_selection.py</span>.</p>
<p>To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.</p>
<p>See also</p>
<p>lars_path
lasso_path
LassoLars
Lasso
LassoLarsCV</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.LassoCVScikitsLearnNode-class.html">LassoCVScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.OneClassSVMScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">OneClassSVMScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.OneClassSVMScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Unsupervised Outlier Detection.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.svm.classes.OneClassSVM</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Estimate the support of a high-dimensional distribution.</p>
<p>The implementation is based on libsvm.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>kernel <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=’rbf’)</span></dt>
<dd>Specifies the kernel type to be used in the algorithm.
It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or
a callable.
If none is given, ‘rbf’ will be used. If a callable is given it is
used to precompute the kernel matrix.</dd>
<dt>degree <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=3)</span></dt>
<dd>Degree of the polynomial kernel function (‘poly’).
Ignored by all other kernels.</dd>
<dt>gamma <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=’auto’)</span></dt>
<dd><p class="first">Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.</p>
<p class="last">Current default is ‘auto’ which uses 1 / n_features,
if <code class="docutils literal"><span class="pre">gamma='scale'</span></code> is passed then it uses 1 / (n_features * X.var())
as value of gamma. The current default of gamma, ‘auto’, will change
to ‘scale’ in version 0.22. ‘auto_deprecated’, a deprecated version of
‘auto’ is used as a default indicating that no explicit value of gamma
was passed.</p>
</dd>
<dt>coef0 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.0)</span></dt>
<dd>Independent term in kernel function.
It is only significant in ‘poly’ and ‘sigmoid’.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Tolerance for stopping criterion.</dd>
<dt>nu <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>An upper bound on the fraction of training
errors and a lower bound of the fraction of support
vectors. Should be in the interval (0, 1]. By default 0.5
will be taken.</dd>
<dt>shrinking <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>Whether to use the shrinking heuristic.</dd>
<dt>cache_size <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Specify the size of the kernel cache (in MB).</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">bool, default: False</span></dt>
<dd>Enable verbose output. Note that this setting takes advantage of a
per-process runtime setting in libsvm that, if enabled, may not work
properly in a multithreaded context.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=-1)</span></dt>
<dd>Hard limit on iterations within solver, or -1 for no limit.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd><p class="first">Ignored.</p>
<div class="last deprecated">
<p><span class="versionmodified">Deprecated since version 0.20: </span><code class="docutils literal"><span class="pre">random_state</span></code> has been deprecated in 0.20 and will be removed in
0.22.</p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">support_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_SV]</span></dt>
<dd>Indices of support vectors.</dd>
<dt><code class="docutils literal"><span class="pre">support_vectors_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [nSV, n_features]</span></dt>
<dd>Support vectors.</dd>
<dt><code class="docutils literal"><span class="pre">dual_coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1, n_SV]</span></dt>
<dd>Coefficients of the support vectors in the decision function.</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1, n_features]</span></dt>
<dd><p class="first">Weights assigned to the features (coefficients in the primal
problem). This is only available in the case of a linear kernel.</p>
<p class="last"><cite>coef_</cite> is readonly property derived from <cite>dual_coef_</cite> and
<cite>support_vectors_</cite></p>
</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1,]</span></dt>
<dd>Constant in the decision function.</dd>
<dt><code class="docutils literal"><span class="pre">offset_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Offset used to define the decision function from the raw scores.
We have the relation: decision_function = score_samples - <cite>offset_</cite>.
The offset is the opposite of <cite>intercept_</cite> and is provided for
consistency with other outlier detection algorithms.</dd>
</dl>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.OneClassSVMScikitsLearnNode-class.html">OneClassSVMScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.RidgeCVScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">RidgeCVScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.RidgeCVScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Ridge regression with built-in cross-validation.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.ridge.RidgeCV</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
See glossary entry for <span class="xref std std-term">cross-validation estimator</span>.</p>
<p>By default, it performs Generalized Cross-Validation, which is a form of
efficient Leave-One-Out cross-validation.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alphas <span class="classifier-delimiter">:</span> <span class="classifier">numpy array of shape [n_alphas]</span></dt>
<dd>Array of alpha values to try.
Regularization strength; must be a positive float. Regularization
improves the conditioning of the problem and reduces the variance of
the estimates. Larger values specify stronger regularization.
Alpha corresponds to <code class="docutils literal"><span class="pre">C^-1</span></code> in other linear models such as
LogisticRegression or LinearSVC.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span></dt>
<dd>This parameter is ignored when <code class="docutils literal"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<code class="xref py py-class docutils literal"><span class="pre">sklearn.preprocessing.StandardScaler</span></code> before calling <code class="docutils literal"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal"><span class="pre">normalize=False</span></code>.</dd>
<dt>scoring <span class="classifier-delimiter">:</span> <span class="classifier">string, callable or None, optional, default: None</span></dt>
<dd>A string (see model evaluation documentation) or
a scorer callable object / function with signature
<code class="docutils literal"><span class="pre">scorer(estimator,</span> <span class="pre">X,</span> <span class="pre">y)</span></code>.</dd>
<dt>cv <span class="classifier-delimiter">:</span> <span class="classifier">int, cross-validation generator or an iterable, optional</span></dt>
<dd><p class="first">Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul class="simple">
<li>None, to use the efficient Leave-One-Out cross-validation</li>
<li>integer, to specify the number of folds.</li>
<li><span class="xref std std-term">CV splitter</span>,</li>
<li>An iterable yielding (train, test) splits as arrays of indices.</li>
</ul>
<p>For integer/None inputs, if <code class="docutils literal"><span class="pre">y</span></code> is binary or multiclass,
<code class="xref py py-class docutils literal"><span class="pre">sklearn.model_selection.StratifiedKFold</span></code> is used, else,
<code class="xref py py-class docutils literal"><span class="pre">sklearn.model_selection.KFold</span></code> is used.</p>
<p class="last">Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
</dd>
<dt>gcv_mode <span class="classifier-delimiter">:</span> <span class="classifier">{None, ‘auto’, ‘svd’, eigen’}, optional</span></dt>
<dd><p class="first">Flag indicating which strategy to use when performing
Generalized Cross-Validation. Options are:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="s1">&#39;auto&#39;</span> <span class="p">:</span> <span class="n">use</span> <span class="n">svd</span> <span class="k">if</span> <span class="n">n_samples</span> <span class="o">&gt;</span> <span class="n">n_features</span> <span class="ow">or</span> <span class="n">when</span> <span class="n">X</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">sparse</span>
         <span class="n">matrix</span><span class="p">,</span> <span class="n">otherwise</span> <span class="n">use</span> <span class="n">eigen</span>
<span class="s1">&#39;svd&#39;</span> <span class="p">:</span> <span class="n">force</span> <span class="n">computation</span> <span class="n">via</span> <span class="n">singular</span> <span class="n">value</span> <span class="n">decomposition</span> <span class="n">of</span> <span class="n">X</span>
        <span class="p">(</span><span class="n">does</span> <span class="ow">not</span> <span class="n">work</span> <span class="k">for</span> <span class="n">sparse</span> <span class="n">matrices</span><span class="p">)</span>
<span class="s1">&#39;eigen&#39;</span> <span class="p">:</span> <span class="n">force</span> <span class="n">computation</span> <span class="n">via</span> <span class="n">eigendecomposition</span> <span class="n">of</span> <span class="n">X</span><span class="o">^</span><span class="n">T</span> <span class="n">X</span>
</pre></div>
</div>
<p class="last">The ‘auto’ mode is the default and is intended to pick the cheaper
option of the two depending upon the shape and format of the training
data.</p>
</dd>
<dt>store_cv_values <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=False</span></dt>
<dd>Flag indicating if the cross-validation values corresponding to
each alpha should be stored in the <code class="docutils literal"><span class="pre">cv_values_</span></code> attribute (see
below). This flag is only compatible with <code class="docutils literal"><span class="pre">cv=None</span></code> (i.e. using
Generalized Cross-Validation).</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">cv_values_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_samples, n_alphas] or         shape = [n_samples, n_targets, n_alphas], optional</span></dt>
<dd>Cross-validation values for each alpha (if <code class="docutils literal"><span class="pre">store_cv_values=True</span></code>        and <code class="docutils literal"><span class="pre">cv=None</span></code>). After <code class="docutils literal"><span class="pre">fit()</span></code> has been called, this attribute         will contain the mean squared errors (by default) or the values         of the <code class="docutils literal"><span class="pre">{loss,score}_func</span></code> function (if provided in the constructor).</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features] or [n_targets, n_features]</span></dt>
<dd>Weight vector(s).</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float | array, shape = (n_targets,)</span></dt>
<dd>Independent term in decision function. Set to 0.0 if
<code class="docutils literal"><span class="pre">fit_intercept</span> <span class="pre">=</span> <span class="pre">False</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">alpha_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Estimated regularization parameter.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_diabetes</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RidgeCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">RidgeCV</span><span class="p">(</span><span class="n">alphas</span><span class="o">=</span><span class="p">[</span><span class="mf">1e-3</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">0.5166...</span>
</pre></div>
</div>
<p>See also</p>
<p>Ridge : Ridge regression
RidgeClassifier : Ridge classifier
RidgeClassifierCV : Ridge classifier with built-in cross validation</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.RidgeCVScikitsLearnNode-class.html">RidgeCVScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.LinearDiscriminantAnalysisScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">LinearDiscriminantAnalysisScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.LinearDiscriminantAnalysisScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Linear Discriminant Analysis
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.discriminant_analysis.LinearDiscriminantAnalysis</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
A classifier with a linear decision boundary, generated by fitting class
conditional densities to the data and using Bayes’ rule.</p>
<p>The model fits a Gaussian density to each class, assuming that all classes
share the same covariance matrix.</p>
<p>The fitted model can also be used to reduce the dimensionality of the input
by projecting it to the most discriminative directions.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>LinearDiscriminantAnalysis</em>.</p>
</div>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>solver <span class="classifier-delimiter">:</span> <span class="classifier">string, optional</span></dt>
<dd><p class="first">Solver to use, possible values:</p>
<blockquote class="last">
<div><ul class="simple">
<li><ul class="first">
<li>‘svd’: Singular value decomposition (default).</li>
</ul>
</li>
<li>Does not compute the covariance matrix, therefore this solver is</li>
<li>recommended for data with a large number of features.</li>
<li><ul class="first">
<li>‘lsqr’: Least squares solution, can be combined with shrinkage.</li>
</ul>
</li>
<li><ul class="first">
<li>‘eigen’: Eigenvalue decomposition, can be combined with shrinkage.</li>
</ul>
</li>
</ul>
</div></blockquote>
</dd>
<dt>shrinkage <span class="classifier-delimiter">:</span> <span class="classifier">string or float, optional</span></dt>
<dd><p class="first">Shrinkage parameter, possible values:</p>
<blockquote>
<div><ul class="simple">
<li><ul class="first">
<li>None: no shrinkage (default).</li>
</ul>
</li>
<li><ul class="first">
<li>‘auto’: automatic shrinkage using the Ledoit-Wolf lemma.</li>
</ul>
</li>
<li><ul class="first">
<li>float between 0 and 1: fixed shrinkage parameter.</li>
</ul>
</li>
</ul>
</div></blockquote>
<p class="last">Note that shrinkage works only with ‘lsqr’ and ‘eigen’ solvers.</p>
</dd>
<dt>priors <span class="classifier-delimiter">:</span> <span class="classifier">array, optional, shape (n_classes,)</span></dt>
<dd>Class priors.</dd>
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Number of components (&lt; n_classes - 1) for dimensionality reduction.</dd>
<dt>store_covariance <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd><p class="first">Additionally compute class covariance matrix (default False), used
only in ‘svd’ solver.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17.</span></p>
</div>
</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, (default 1.0e-4)</span></dt>
<dd><p class="first">Threshold used for rank estimation in SVD solver.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17.</span></p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,) or (n_classes, n_features)</span></dt>
<dd>Weight vector(s).</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,)</span></dt>
<dd>Intercept term.</dd>
<dt><code class="docutils literal"><span class="pre">covariance_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_features, n_features)</span></dt>
<dd>Covariance matrix (shared by all classes).</dd>
<dt><code class="docutils literal"><span class="pre">explained_variance_ratio_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_components,)</span></dt>
<dd>Percentage of variance explained by each of the selected components.
If <code class="docutils literal"><span class="pre">n_components</span></code> is not set then all components are stored and the
sum of explained variances is equal to 1.0. Only available when eigen
or svd solver is used.</dd>
<dt><code class="docutils literal"><span class="pre">means_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_classes, n_features)</span></dt>
<dd>Class means.</dd>
<dt><code class="docutils literal"><span class="pre">priors_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_classes,)</span></dt>
<dd>Class priors (sum to 1).</dd>
<dt><code class="docutils literal"><span class="pre">scalings_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (rank, n_classes - 1)</span></dt>
<dd>Scaling of the features in the space spanned by the class centroids.</dd>
<dt><code class="docutils literal"><span class="pre">xbar_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_features,)</span></dt>
<dd>Overall mean.</dd>
<dt><code class="docutils literal"><span class="pre">classes_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_classes,)</span></dt>
<dd>Unique class labels.</dd>
</dl>
<p>See also</p>
<dl class="docutils">
<dt>sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis: Quadratic</dt>
<dd>Discriminant Analysis</dd>
</dl>
<p><strong>Notes</strong></p>
<p>The default solver is ‘svd’. It can perform both classification and
transform, and it does not rely on the calculation of the covariance
matrix. This can be an advantage in situations where the number of features
is large. However, the ‘svd’ solver cannot be used with shrinkage.</p>
<p>The ‘lsqr’ solver is an efficient algorithm that only works for
classification. It supports shrinkage.</p>
<p>The ‘eigen’ solver is based on the optimization of the between class
scatter to within class scatter ratio. It can be used for both
classification and transform, and it supports shrinkage. However, the
‘eigen’ solver needs to compute the covariance matrix, so it might not be
suitable for situations with a high number of features.</p>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.discriminant_analysis</span> <span class="kn">import</span> <span class="n">LinearDiscriminantAnalysis</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">LinearDiscriminantAnalysis</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,</span>
<span class="go">              solver=&#39;svd&#39;, store_covariance=False, tol=0.0001)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#LinearDiscriminantAnalysisScikitsLearnNode">LinearDiscriminantAnalysisScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.PriorProbabilityEstimatorScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">PriorProbabilityEstimatorScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.PriorProbabilityEstimatorScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>An estimator predicting the probability of each
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.ensemble.gradient_boosting.PriorProbabilityEstimator</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#PriorProbabilityEstimatorScikitsLearnNode">PriorProbabilityEstimatorScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.ARDRegressionScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">ARDRegressionScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.ARDRegressionScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bayesian ARD regression.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.bayes.ARDRegression</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Fit the weights of a regression model, using an ARD prior. The weights of
the regression model are assumed to be in Gaussian distributions.
Also estimate the parameters lambda (precisions of the distributions of the
weights) and alpha (precision of the distribution of the noise).
The estimation is done by an iterative procedures (Evidence Maximization)</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Maximum number of iterations. Default is 300</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Stop the algorithm if w has converged. Default is 1.e-3.</dd>
<dt>alpha_1 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Hyper-parameter : shape parameter for the Gamma distribution prior
over the alpha parameter. Default is 1.e-6.</dd>
<dt>alpha_2 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Hyper-parameter : inverse scale parameter (rate parameter) for the
Gamma distribution prior over the alpha parameter. Default is 1.e-6.</dd>
<dt>lambda_1 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Hyper-parameter : shape parameter for the Gamma distribution prior
over the lambda parameter. Default is 1.e-6.</dd>
<dt>lambda_2 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Hyper-parameter : inverse scale parameter (rate parameter) for the
Gamma distribution prior over the lambda parameter. Default is 1.e-6.</dd>
<dt>compute_score <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>If True, compute the objective function at each step of the model.
Default is False.</dd>
<dt>threshold_lambda <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>threshold for removing (pruning) weights with high precision from
the computation. Default is 1.e+4.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).
Default is True.</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span></dt>
<dd>This parameter is ignored when <code class="docutils literal"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<code class="xref py py-class docutils literal"><span class="pre">sklearn.preprocessing.StandardScaler</span></code> before calling <code class="docutils literal"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal"><span class="pre">normalize=False</span></code>.</dd>
<dt>copy_X <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True.</span></dt>
<dd>If True, X will be copied; else, it may be overwritten.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span></dt>
<dd>Verbose mode when fitting the model.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = (n_features)</span></dt>
<dd>Coefficients of the regression model (mean of distribution)</dd>
<dt><code class="docutils literal"><span class="pre">alpha_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>estimated precision of the noise.</dd>
<dt><code class="docutils literal"><span class="pre">lambda_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = (n_features)</span></dt>
<dd>estimated precisions of the weights.</dd>
<dt><code class="docutils literal"><span class="pre">sigma_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = (n_features, n_features)</span></dt>
<dd>estimated variance-covariance matrix of the weights</dd>
<dt><code class="docutils literal"><span class="pre">scores_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>if computed, value of the objective function (to be maximized)</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">ARDRegression</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">... </span>
<span class="go">ARDRegression(alpha_1=1e-06, alpha_2=1e-06, compute_score=False,</span>
<span class="go">        copy_X=True, fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06,</span>
<span class="go">        n_iter=300, normalize=False, threshold_lambda=10000.0, tol=0.001,</span>
<span class="go">        verbose=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="go">array([1.])</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>For an example, see <span class="xref std std-ref">examples/linear_model/plot_ard.py</span>.</p>
<p><strong>References</strong></p>
<p>D. J. C. MacKay, Bayesian nonlinear modeling for the prediction
competition, ASHRAE Transactions, 1994.</p>
<p>R. Salakhutdinov, Lecture notes on Statistical Machine Learning,
<a class="reference external" href="http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15">http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15</a>
Their beta is our <code class="docutils literal"><span class="pre">self.alpha_</span></code>
Their alpha is our <code class="docutils literal"><span class="pre">self.lambda_</span></code>
ARD is a little different than the slide: only dimensions/features for
which <code class="docutils literal"><span class="pre">self.lambda_</span> <span class="pre">&lt;</span> <span class="pre">self.threshold_lambda</span></code> are kept and the rest are
discarded.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.ARDRegressionScikitsLearnNode-class.html">ARDRegressionScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.ImputerScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">ImputerScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.ImputerScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Imputation transformer for completing missing values.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.preprocessing.imputation.Imputer</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>missing_values <span class="classifier-delimiter">:</span> <span class="classifier">integer or “NaN”, optional (default=”NaN”)</span></dt>
<dd>The placeholder for the missing values. All occurrences of
<cite>missing_values</cite> will be imputed. For missing values encoded as np.nan,
use the string value “NaN”.</dd>
<dt>strategy <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=”mean”)</span></dt>
<dd><p class="first">The imputation strategy.</p>
<ul class="last simple">
<li>If “mean”, then replace missing values using the mean along
the axis.</li>
<li>If “median”, then replace missing values using the median along
the axis.</li>
<li>If “most_frequent”, then replace missing using the most frequent
value along the axis.</li>
</ul>
</dd>
<dt>axis <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=0)</span></dt>
<dd><p class="first">The axis along which to impute.</p>
<ul class="last simple">
<li>If <cite>axis=0</cite>, then impute along columns.</li>
<li>If <cite>axis=1</cite>, then impute along rows.</li>
</ul>
</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=0)</span></dt>
<dd>Controls the verbosity of the imputer.</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span></dt>
<dd><p class="first">If True, a copy of X will be created. If False, imputation will
be done in-place whenever possible. Note that, in the following cases,
a new copy will always be made, even if <cite>copy=False</cite>:</p>
<ul class="last simple">
<li>If X is not an array of floating values;</li>
<li>If X is sparse and <cite>missing_values=0</cite>;</li>
<li>If <cite>axis=0</cite> and X is encoded as a CSR matrix;</li>
<li>If <cite>axis=1</cite> and X is encoded as a CSC matrix.</li>
</ul>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">statistics_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_features,)</span></dt>
<dd>The imputation fill value for each feature if axis == 0.</dd>
</dl>
<p><strong>Notes</strong></p>
<ul class="simple">
<li>When <code class="docutils literal"><span class="pre">axis=0</span></code>, columns which only contained missing values at <cite>fit</cite>
are discarded upon <cite>transform</cite>.</li>
<li>When <code class="docutils literal"><span class="pre">axis=1</span></code>, an exception is raised if there are rows for which it is
not possible to fill in the missing values (e.g., because they only
contain missing values).</li>
</ul>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#ImputerScikitsLearnNode">ImputerScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.VarianceThresholdScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">VarianceThresholdScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.VarianceThresholdScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Feature selector that removes all low-variance features.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.feature_selection.variance_threshold.VarianceThreshold</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
This feature selection algorithm looks only at the features (X), not the
desired outputs (y), and can thus be used for unsupervised learning.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>threshold <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Features with a training-set variance lower than this threshold will
be removed. The default is to keep all features with non-zero variance,
i.e. remove the features that have the same value in all samples.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">variances_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,)</span></dt>
<dd>Variances of individual features.</dd>
</dl>
<p><strong>Examples</strong></p>
<p>The following dataset has integer features, two of which are the same
in every sample. These are removed with the default setting for threshold:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span> <span class="o">=</span> <span class="n">VarianceThreshold</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([[2, 0],</span>
<span class="go">       [1, 4],</span>
<span class="go">       [1, 1]])</span>
</pre></div>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#VarianceThresholdScikitsLearnNode">VarianceThresholdScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.GradientBoostingRegressorScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">GradientBoostingRegressorScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.GradientBoostingRegressorScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Gradient Boosting for regression.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.ensemble.gradient_boosting.GradientBoostingRegressor</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
GB builds an additive model in a forward stage-wise fashion;
it allows for the optimization of arbitrary differentiable loss functions.
In each stage a regression tree is fit on the negative gradient of the
given loss function.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>loss <span class="classifier-delimiter">:</span> <span class="classifier">{‘ls’, ‘lad’, ‘huber’, ‘quantile’}, optional (default=’ls’)</span></dt>
<dd>loss function to be optimized. ‘ls’ refers to least squares
regression. ‘lad’ (least absolute deviation) is a highly robust
loss function solely based on order information of the input
variables. ‘huber’ is a combination of the two. ‘quantile’
allows quantile regression (use <cite>alpha</cite> to specify the quantile).</dd>
<dt>learning_rate <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.1)</span></dt>
<dd>learning rate shrinks the contribution of each tree by <cite>learning_rate</cite>.
There is a trade-off between learning_rate and n_estimators.</dd>
<dt>n_estimators <span class="classifier-delimiter">:</span> <span class="classifier">int (default=100)</span></dt>
<dd>The number of boosting stages to perform. Gradient boosting
is fairly robust to over-fitting so a large number usually
results in better performance.</dd>
<dt>subsample <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span></dt>
<dd>The fraction of samples to be used for fitting the individual base
learners. If smaller than 1.0 this results in Stochastic Gradient
Boosting. <cite>subsample</cite> interacts with the parameter <cite>n_estimators</cite>.
Choosing <cite>subsample &lt; 1.0</cite> leads to a reduction of variance
and an increase in bias.</dd>
<dt>criterion <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=”friedman_mse”)</span></dt>
<dd><p class="first">The function to measure the quality of a split. Supported criteria
are “friedman_mse” for the mean squared error with improvement
score by Friedman, “mse” for mean squared error, and “mae” for
the mean absolute error. The default value of “friedman_mse” is
generally the best as it can provide a better approximation in
some cases.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.18.</span></p>
</div>
</dd>
<dt>min_samples_split <span class="classifier-delimiter">:</span> <span class="classifier">int, float, optional (default=2)</span></dt>
<dd><p class="first">The minimum number of samples required to split an internal node:</p>
<ul class="simple">
<li>If int, then consider <cite>min_samples_split</cite> as the minimum number.</li>
<li>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</li>
</ul>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</dd>
<dt>min_samples_leaf <span class="classifier-delimiter">:</span> <span class="classifier">int, float, optional (default=1)</span></dt>
<dd><p class="first">The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul class="simple">
<li>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</li>
<li>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</li>
</ul>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</dd>
<dt>min_weight_fraction_leaf <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span></dt>
<dd>The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</dd>
<dt>max_depth <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=3)</span></dt>
<dd>maximum depth of the individual regression estimators. The maximum
depth limits the number of nodes in the tree. Tune this parameter
for best performance; the best value depends on the interaction
of the input variables.</dd>
<dt>min_impurity_decrease <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span></dt>
<dd><p class="first">A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal"><span class="pre">N</span></code>, <code class="docutils literal"><span class="pre">N_t</span></code>, <code class="docutils literal"><span class="pre">N_t_R</span></code> and <code class="docutils literal"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19.</span></p>
</div>
</dd>
<dt>min_impurity_split <span class="classifier-delimiter">:</span> <span class="classifier">float, (default=1e-7)</span></dt>
<dd><p class="first">Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="last deprecated">
<p><span class="versionmodified">Deprecated since version 0.19: </span><code class="docutils literal"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal"><span class="pre">min_impurity_split</span></code> will change from 1e-7 to 0 in 0.23 and it
will be removed in 0.25. Use <code class="docutils literal"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</dd>
<dt>init <span class="classifier-delimiter">:</span> <span class="classifier">estimator, optional (default=None)</span></dt>
<dd>An estimator object that is used to compute the initial
predictions. <code class="docutils literal"><span class="pre">init</span></code> has to provide <code class="docutils literal"><span class="pre">fit</span></code> and <code class="docutils literal"><span class="pre">predict</span></code>.
If None it uses <code class="docutils literal"><span class="pre">loss.init_estimator</span></code>.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int, float, string or None, optional (default=None)</span></dt>
<dd><p class="first">The number of features to consider when looking for the best split:</p>
<ul class="simple">
<li>If int, then consider <cite>max_features</cite> features at each split.</li>
<li>If float, then <cite>max_features</cite> is a fraction and
<cite>int(max_features * n_features)</cite> features are considered at each
split.</li>
<li>If “auto”, then <cite>max_features=n_features</cite>.</li>
<li>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite>.</li>
<li>If “log2”, then <cite>max_features=log2(n_features)</cite>.</li>
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
<p>Choosing <cite>max_features &lt; n_features</cite> leads to a reduction of variance
and an increase in bias.</p>
<p class="last">Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal"><span class="pre">max_features</span></code> features.</p>
</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float (default=0.9)</span></dt>
<dd>The alpha-quantile of the huber loss function and the quantile
loss function. Only if <code class="docutils literal"><span class="pre">loss='huber'</span></code> or <code class="docutils literal"><span class="pre">loss='quantile'</span></code>.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, default: 0</span></dt>
<dd>Enable verbose output. If 1 then it prints progress and performance
once in a while (the more trees the lower the frequency). If greater
than 1 then it prints progress and performance for every tree.</dd>
<dt>max_leaf_nodes <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Grow trees with <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, default: False</span></dt>
<dd>When set to <code class="docutils literal"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just erase the
previous solution. See <span class="xref std std-term">the Glossary</span>.</dd>
<dt>presort <span class="classifier-delimiter">:</span> <span class="classifier">bool or ‘auto’, optional (default=’auto’)</span></dt>
<dd><p class="first">Whether to presort the data to speed up the finding of best splits in
fitting. Auto mode by default will use presorting on dense data and
default to normal sorting on sparse data. Setting presort to true on
sparse data will raise an error.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span>optional parameter <em>presort</em>.</p>
</div>
</dd>
<dt>validation_fraction <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default 0.1</span></dt>
<dd><p class="first">The proportion of training data to set aside as validation set for
early stopping. Must be between 0 and 1.
Only used if <code class="docutils literal"><span class="pre">n_iter_no_change</span></code> is set to an integer.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
<dt>n_iter_no_change <span class="classifier-delimiter">:</span> <span class="classifier">int, default None</span></dt>
<dd><p class="first"><code class="docutils literal"><span class="pre">n_iter_no_change</span></code> is used to decide if early stopping will be used
to terminate training when validation score is not improving. By
default it is set to None to disable early stopping. If set to a
number, it will set aside <code class="docutils literal"><span class="pre">validation_fraction</span></code> size of the training
data as validation and terminate training when validation score is not
improving in all of the previous <code class="docutils literal"><span class="pre">n_iter_no_change</span></code> numbers of
iterations.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default 1e-4</span></dt>
<dd><p class="first">Tolerance for the early stopping. When the loss is not improving
by at least tol for <code class="docutils literal"><span class="pre">n_iter_no_change</span></code> iterations (if set to a
number), the training stops.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">feature_importances_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,)</span></dt>
<dd>The feature importances (the higher, the more important the feature).</dd>
<dt><code class="docutils literal"><span class="pre">oob_improvement_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_estimators,)</span></dt>
<dd>The improvement in loss (= deviance) on the out-of-bag samples
relative to the previous iteration.
<code class="docutils literal"><span class="pre">oob_improvement_[0]</span></code> is the improvement in
loss of the first stage over the <code class="docutils literal"><span class="pre">init</span></code> estimator.</dd>
<dt><code class="docutils literal"><span class="pre">train_score_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_estimators,)</span></dt>
<dd>The i-th score <code class="docutils literal"><span class="pre">train_score_[i]</span></code> is the deviance (= loss) of the
model at iteration <code class="docutils literal"><span class="pre">i</span></code> on the in-bag sample.
If <code class="docutils literal"><span class="pre">subsample</span> <span class="pre">==</span> <span class="pre">1</span></code> this is the deviance on the training data.</dd>
<dt><code class="docutils literal"><span class="pre">loss_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">LossFunction</span></dt>
<dd>The concrete <code class="docutils literal"><span class="pre">LossFunction</span></code> object.</dd>
<dt><code class="docutils literal"><span class="pre">init_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">estimator</span></dt>
<dd>The estimator that provides the initial predictions.
Set via the <code class="docutils literal"><span class="pre">init</span></code> argument or <code class="docutils literal"><span class="pre">loss.init_estimator</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">estimators_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of DecisionTreeRegressor, shape (n_estimators, 1)</span></dt>
<dd>The collection of fitted sub-estimators.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data and
<code class="docutils literal"><span class="pre">max_features=n_features</span></code>, if the improvement of the criterion is
identical for several splits enumerated during the search of the best
split. To obtain a deterministic behaviour during fitting,
<code class="docutils literal"><span class="pre">random_state</span></code> has to be fixed.</p>
<p>See also</p>
<p>DecisionTreeRegressor, RandomForestRegressor</p>
<p><strong>References</strong></p>
<p>J. Friedman, Greedy Function Approximation: A Gradient Boosting
Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.</p>
<ol class="upperalpha simple" start="10">
<li>Friedman, Stochastic Gradient Boosting, 1999</li>
</ol>
<p>T. Hastie, R. Tibshirani and J. Friedman.
Elements of Statistical Learning Ed. 2, Springer, 2009.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#GradientBoostingRegressorScikitsLearnNode">GradientBoostingRegressorScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.OrthogonalMatchingPursuitScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">OrthogonalMatchingPursuitScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.OrthogonalMatchingPursuitScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Orthogonal Matching Pursuit model (OMP)
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.omp.OrthogonalMatchingPursuit</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_nonzero_coefs <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Desired number of non-zero entries in the solution. If None (by
default) this value is set to 10% of n_features.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Maximum norm of the residual. If not None, overrides n_nonzero_coefs.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>This parameter is ignored when <code class="docutils literal"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<code class="xref py py-class docutils literal"><span class="pre">sklearn.preprocessing.StandardScaler</span></code> before calling <code class="docutils literal"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal"><span class="pre">normalize=False</span></code>.</dd>
<dt>precompute <span class="classifier-delimiter">:</span> <span class="classifier">{True, False, ‘auto’}, default ‘auto’</span></dt>
<dd>Whether to use a precomputed Gram and Xy matrix to speed up
calculations. Improves performance when <cite>n_targets</cite> or <cite>n_samples</cite> is
very large. Note that if you already have such matrices, you can pass
them directly to the fit method.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,) or (n_targets, n_features)</span></dt>
<dd>parameter vector (w in the formula)</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float or array, shape (n_targets,)</span></dt>
<dd>independent term in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int or array-like</span></dt>
<dd>Number of active features across every target.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">OrthogonalMatchingPursuit</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">noise</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">OrthogonalMatchingPursuit</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">0.9991...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">1</span><span class="p">,])</span>
<span class="go">array([-78.3854...])</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,
Matching pursuits with time-frequency dictionaries, IEEE Transactions on
Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.
(<a class="reference external" href="http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf">http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf</a>)</p>
<p>This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,
M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal
Matching Pursuit Technical Report - CS Technion, April 2008.
<a class="reference external" href="http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf">http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf</a></p>
<p>See also</p>
<p>orthogonal_mp
orthogonal_mp_gram
lars_path
Lars
LassoLars
decomposition.sparse_encode
OrthogonalMatchingPursuitCV</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#OrthogonalMatchingPursuitScikitsLearnNode">OrthogonalMatchingPursuitScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.PLSCanonicalScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">PLSCanonicalScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.PLSCanonicalScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>PLSCanonical implements the 2 blocks canonical PLS of the original Wold
algorithm [Tenenhaus 1998] p.204, referred as PLS-C2A in [Wegelin 2000].
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.cross_decomposition.pls_.PLSCanonical</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
This class inherits from PLS with mode=”A” and deflation_mode=”canonical”,
norm_y_weights=True and algorithm=”nipals”, but svd should provide similar
results up to numerical errors.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int, (default 2).</span></dt>
<dd>Number of components to keep</dd>
<dt>scale <span class="classifier-delimiter">:</span> <span class="classifier">boolean, (default True)</span></dt>
<dd>Option to scale data</dd>
<dt>algorithm <span class="classifier-delimiter">:</span> <span class="classifier">string, “nipals” or “svd”</span></dt>
<dd>The algorithm used to estimate the weights. It will be called
n_components times, i.e. once for each iteration of the outer loop.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">an integer, (default 500)</span></dt>
<dd>the maximum number of iterations of the NIPALS inner loop (used
only if algorithm=”nipals”)</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">non-negative real, default 1e-06</span></dt>
<dd>the tolerance used in the iterative algorithm</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default True</span></dt>
<dd>Whether the deflation should be done on a copy. Let the default
value to True unless you don’t care about side effect</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">x_weights_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [p, n_components]</span></dt>
<dd>X block weights vectors.</dd>
<dt><code class="docutils literal"><span class="pre">y_weights_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [q, n_components]</span></dt>
<dd>Y block weights vectors.</dd>
<dt><code class="docutils literal"><span class="pre">x_loadings_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [p, n_components]</span></dt>
<dd>X block loadings vectors.</dd>
<dt><code class="docutils literal"><span class="pre">y_loadings_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [q, n_components]</span></dt>
<dd>Y block loadings vectors.</dd>
<dt><code class="docutils literal"><span class="pre">x_scores_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_samples, n_components]</span></dt>
<dd>X scores.</dd>
<dt><code class="docutils literal"><span class="pre">y_scores_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_samples, n_components]</span></dt>
<dd>Y scores.</dd>
<dt><code class="docutils literal"><span class="pre">x_rotations_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [p, n_components]</span></dt>
<dd>X block to latents rotations.</dd>
<dt><code class="docutils literal"><span class="pre">y_rotations_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [q, n_components]</span></dt>
<dd>Y block to latents rotations.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like</span></dt>
<dd>Number of iterations of the NIPALS inner loop for each
component. Not useful if the algorithm provided is “svd”.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>Matrices:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>T: ``x_scores_``
U: ``y_scores_``
W: ``x_weights_``
C: ``y_weights_``
P: ``x_loadings_``
Q: ``y_loadings__``
</pre></div>
</div>
<p>Are computed such that:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>X = T P.T + Err and Y = U Q.T + Err
T[:, k] = Xk W[:, k] for k in range(n_components)
U[:, k] = Yk C[:, k] for k in range(n_components)
``x_rotations_`` = W (P.T W)^(-1)
``y_rotations_`` = C (Q.T C)^(-1)
</pre></div>
</div>
<p>where Xk and Yk are residual matrices at iteration k.</p>
<p><a class="reference external" href="http://www.eigenvector.com/Docs/Wise_pls_properties.pdf">Slides explaining PLS</a></p>
<p>For each component k, find weights u, v that optimize:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>max corr(Xk u, Yk v) * std(Xk u) std(Yk u), such that ``|u| = |v| = 1``
</pre></div>
</div>
<p>Note that it maximizes both the correlations between the scores and the
intra-block variances.</p>
<p>The residual matrix of X (Xk+1) block is obtained by the deflation on the
current X score: x_score.</p>
<p>The residual matrix of Y (Yk+1) block is obtained by deflation on the
current Y score. This performs a canonical symmetric version of the PLS
regression. But slightly different than the CCA. This is mostly used
for modeling.</p>
<p>This implementation provides the same results that the “plspm” package
provided in the R language (R-project), using the function plsca(X, Y).
Results are equal or collinear with the function
<code class="docutils literal"><span class="pre">pls(...,</span> <span class="pre">mode</span> <span class="pre">=</span> <span class="pre">&quot;canonical&quot;)</span></code> of the “mixOmics” package. The difference
relies in the fact that mixOmics implementation does not exactly implement
the Wold algorithm since it does not normalize y_weights to one.</p>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cross_decomposition</span> <span class="kn">import</span> <span class="n">PLSCanonical</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.</span><span class="p">,</span><span class="mf">5.</span><span class="p">,</span><span class="mf">4.</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.2</span><span class="p">,</span> <span class="mf">5.9</span><span class="p">],</span> <span class="p">[</span><span class="mf">11.9</span><span class="p">,</span> <span class="mf">12.3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plsca</span> <span class="o">=</span> <span class="n">PLSCanonical</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plsca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">PLSCanonical(algorithm=&#39;nipals&#39;, copy=True, max_iter=500, n_components=2,</span>
<span class="go">             scale=True, tol=1e-06)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_c</span><span class="p">,</span> <span class="n">Y_c</span> <span class="o">=</span> <span class="n">plsca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with
emphasis on the two-block case. Technical Report 371, Department of
Statistics, University of Washington, Seattle, 2000.</p>
<p>Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris:</p>
<p>Editions Technic.</p>
<p>See also</p>
<p>CCA
PLSSVD</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.PLSCanonicalScikitsLearnNode-class.html">PLSCanonicalScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.FeatureAgglomerationScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">FeatureAgglomerationScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.FeatureAgglomerationScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Agglomerate features.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.cluster.hierarchical.FeatureAgglomeration</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Similar to AgglomerativeClustering, but recursively merges features
instead of samples.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_clusters <span class="classifier-delimiter">:</span> <span class="classifier">int, default 2</span></dt>
<dd>The number of clusters to find.</dd>
<dt>affinity <span class="classifier-delimiter">:</span> <span class="classifier">string or callable, default “euclidean”</span></dt>
<dd>Metric used to compute the linkage. Can be “euclidean”, “l1”, “l2”,
“manhattan”, “cosine”, or ‘precomputed’.
If linkage is “ward”, only “euclidean” is accepted.</dd>
<dt>memory <span class="classifier-delimiter">:</span> <span class="classifier">None, str or object with the joblib.Memory interface, optional</span></dt>
<dd>Used to cache the output of the computation of the tree.
By default, no caching is done. If a string is given, it is the
path to the caching directory.</dd>
<dt>connectivity <span class="classifier-delimiter">:</span> <span class="classifier">array-like or callable, optional</span></dt>
<dd>Connectivity matrix. Defines for each feature the neighboring
features following a given structure of the data.
This can be a connectivity matrix itself or a callable that transforms
the data into a connectivity matrix, such as derived from
kneighbors_graph. Default is None, i.e, the
hierarchical clustering algorithm is unstructured.</dd>
<dt>compute_full_tree <span class="classifier-delimiter">:</span> <span class="classifier">bool or ‘auto’, optional, default “auto”</span></dt>
<dd>Stop early the construction of the tree at n_clusters. This is
useful to decrease computation time if the number of clusters is
not small compared to the number of features. This option is
useful only when specifying a connectivity matrix. Note also that
when varying the number of clusters and using caching, it may
be advantageous to compute the full tree.</dd>
<dt>linkage <span class="classifier-delimiter">:</span> <span class="classifier">{“ward”, “complete”, “average”, “single”}, optional            (default=”ward”)</span></dt>
<dd><p class="first">Which linkage criterion to use. The linkage criterion determines which
distance to use between sets of features. The algorithm will merge
the pairs of cluster that minimize this criterion.</p>
<ul class="last simple">
<li>ward minimizes the variance of the clusters being merged.</li>
<li>average uses the average of the distances of each feature of
the two sets.</li>
<li>complete or maximum linkage uses the maximum distances between
all features of the two sets.</li>
<li>single uses the minimum of the distances between all observations
of the two sets.</li>
</ul>
</dd>
<dt>pooling_func <span class="classifier-delimiter">:</span> <span class="classifier">callable, default np.mean</span></dt>
<dd>This combines the values of agglomerated features into a single
value, and should accept an array of shape [M, N] and the keyword
argument <cite>axis=1</cite>, and reduce it to an array of size [M].</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">labels_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, (n_features,)</span></dt>
<dd>cluster labels for each feature.</dd>
<dt><code class="docutils literal"><span class="pre">n_leaves_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of leaves in the hierarchical tree.</dd>
<dt><code class="docutils literal"><span class="pre">n_components_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The estimated number of connected components in the graph.</dd>
<dt><code class="docutils literal"><span class="pre">children_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_nodes-1, 2)</span></dt>
<dd>The children of each non-leaf node. Values less than <cite>n_features</cite>
correspond to leaves of the tree which are the original samples.
A node <cite>i</cite> greater than or equal to <cite>n_features</cite> is a non-leaf
node and has children <cite>children_[i - n_features]</cite>. Alternatively
at the i-th iteration, children[i][0] and children[i][1]
are merged to form node <cite>n_features + i</cite></dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">cluster</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">digits</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_digits</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">images</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">images</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">images</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">agglo</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">FeatureAgglomeration</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">agglo</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> 
<span class="go">FeatureAgglomeration(affinity=&#39;euclidean&#39;, compute_full_tree=&#39;auto&#39;,</span>
<span class="go">           connectivity=None, linkage=&#39;ward&#39;, memory=None, n_clusters=32,</span>
<span class="go">           pooling_func=...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_reduced</span> <span class="o">=</span> <span class="n">agglo</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_reduced</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(1797, 32)</span>
</pre></div>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#FeatureAgglomerationScikitsLearnNode">FeatureAgglomerationScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.SelectPercentileScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">SelectPercentileScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.SelectPercentileScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Select features according to a percentile of the highest scores.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.feature_selection.univariate_selection.SelectPercentile</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>score_func <span class="classifier-delimiter">:</span> <span class="classifier">callable</span></dt>
<dd>Function taking two arrays X and y, and returning a pair of arrays
(scores, pvalues) or a single array with scores.
Default is f_classif (see below “See also”). The default function only
works with classification tasks.</dd>
<dt>percentile <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default=10</span></dt>
<dd>Percent of features to keep.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">scores_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>Scores of features.</dd>
<dt><code class="docutils literal"><span class="pre">pvalues_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>p-values of feature scores, None if <cite>score_func</cite> returned only scores.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectPercentile</span><span class="p">,</span> <span class="n">chi2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(1797, 64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_new</span> <span class="o">=</span> <span class="n">SelectPercentile</span><span class="p">(</span><span class="n">chi2</span><span class="p">,</span> <span class="n">percentile</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_new</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(1797, 7)</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>Ties between features with equal scores will be broken in an unspecified
way.</p>
<p>See also</p>
<p>f_classif: ANOVA F-value between label/feature for classification tasks.
mutual_info_classif: Mutual information for a discrete target.
chi2: Chi-squared stats of non-negative features for classification tasks.
f_regression: F-value between label/feature for regression tasks.
mutual_info_regression: Mutual information for a continuous target.
SelectKBest: Select features based on the k highest scores.
SelectFpr: Select features based on a false positive rate test.
SelectFdr: Select features based on an estimated false discovery rate.
SelectFwe: Select features based on family-wise error rate.
GenericUnivariateSelect: Univariate feature selector with configurable mode.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.SelectPercentileScikitsLearnNode-class.html">SelectPercentileScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.KernelRidgeScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">KernelRidgeScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.KernelRidgeScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Kernel ridge regression.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.kernel_ridge.KernelRidge</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Kernel ridge regression (KRR) combines ridge regression (linear least
squares with l2-norm regularization) with the kernel trick. It thus
learns a linear function in the space induced by the respective kernel and
the data. For non-linear kernels, this corresponds to a non-linear
function in the original space.</p>
<p>The form of the model learned by KRR is identical to support vector
regression (SVR). However, different loss functions are used: KRR uses
squared error loss while support vector regression uses epsilon-insensitive
loss, both combined with l2 regularization. In contrast to SVR, fitting a
KRR model can be done in closed-form and is typically faster for
medium-sized datasets. On the other  hand, the learned model is non-sparse
and thus slower than SVR, which learns a sparse model for epsilon &gt; 0, at
prediction-time.</p>
<p>This estimator has built-in support for multi-variate regression
(i.e., when y is a 2d-array of shape [n_samples, n_targets]).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">{float, array-like}, shape = [n_targets]</span></dt>
<dd>Small positive values of alpha improve the conditioning of the problem
and reduce the variance of the estimates.  Alpha corresponds to
<code class="docutils literal"><span class="pre">(2*C)^-1</span></code> in other linear models such as LogisticRegression or
LinearSVC. If an array is passed, penalties are assumed to be specific
to the targets. Hence they must correspond in number.</dd>
<dt>kernel <span class="classifier-delimiter">:</span> <span class="classifier">string or callable, default=”linear”</span></dt>
<dd>Kernel mapping used internally. A callable should accept two arguments
and the keyword arguments passed to this object as kernel_params, and
should return a floating point number. Set to “precomputed” in
order to pass a precomputed kernel matrix to the estimator
methods instead of samples.</dd>
<dt>gamma <span class="classifier-delimiter">:</span> <span class="classifier">float, default=None</span></dt>
<dd>Gamma parameter for the RBF, laplacian, polynomial, exponential chi2
and sigmoid kernels. Interpretation of the default value is left to
the kernel; see the documentation for sklearn.metrics.pairwise.
Ignored by other kernels.</dd>
<dt>degree <span class="classifier-delimiter">:</span> <span class="classifier">float, default=3</span></dt>
<dd>Degree of the polynomial kernel. Ignored by other kernels.</dd>
<dt>coef0 <span class="classifier-delimiter">:</span> <span class="classifier">float, default=1</span></dt>
<dd>Zero coefficient for polynomial and sigmoid kernels.
Ignored by other kernels.</dd>
<dt>kernel_params <span class="classifier-delimiter">:</span> <span class="classifier">mapping of string to any, optional</span></dt>
<dd>Additional parameters (keyword arguments) for kernel function passed
as callable object.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">dual_coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_samples] or [n_samples, n_targets]</span></dt>
<dd>Representation of weight vector(s) in kernel space</dd>
<dt><code class="docutils literal"><span class="pre">X_fit_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">{array-like, sparse matrix}, shape = [n_samples, n_features]</span></dt>
<dd>Training data, which is also required for prediction. If
kernel == “precomputed” this is instead the precomputed
training matrix, shape = [n_samples, n_samples].</dd>
</dl>
<p><strong>References</strong></p>
<ul class="simple">
<li>Kevin P. Murphy
“Machine Learning: A Probabilistic Perspective”, The MIT Press
chapter 14.4.3, pp. 492-493</li>
</ul>
<p>See also</p>
<p>sklearn.linear_model.Ridge:</p>
<blockquote>
<div><ul class="simple">
<li>Linear ridge regression.</li>
</ul>
</div></blockquote>
<p>sklearn.svm.SVR:</p>
<blockquote>
<div><ul class="simple">
<li>Support Vector Regression implemented using libsvm.</li>
</ul>
</div></blockquote>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.kernel_ridge</span> <span class="kn">import</span> <span class="n">KernelRidge</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">KernelRidge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">KernelRidge(alpha=1.0, coef0=1, degree=3, gamma=None, kernel=&#39;linear&#39;,</span>
<span class="go">            kernel_params=None)</span>
</pre></div>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#KernelRidgeScikitsLearnNode">KernelRidgeScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.MultiTaskLassoCVScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">MultiTaskLassoCVScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.MultiTaskLassoCVScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.coordinate_descent.MultiTaskLassoCV</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
See glossary entry for <span class="xref std std-term">cross-validation estimator</span>.</p>
<p>The optimization objective for MultiTaskLasso is:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">Y</span> <span class="o">-</span> <span class="n">XW</span><span class="o">||^</span><span class="n">Fro_2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span>
</pre></div>
</div>
<p>Where:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span> <span class="o">=</span> \<span class="n">sum_i</span> \<span class="n">sqrt</span><span class="p">{</span>\<span class="n">sum_j</span> <span class="n">w_</span><span class="p">{</span><span class="n">ij</span><span class="p">}</span><span class="o">^</span><span class="mi">2</span><span class="p">}</span>
</pre></div>
</div>
<p>i.e. the sum of norm of each row.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>eps <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Length of the path. <code class="docutils literal"><span class="pre">eps=1e-3</span></code> means that
<code class="docutils literal"><span class="pre">alpha_min</span> <span class="pre">/</span> <span class="pre">alpha_max</span> <span class="pre">=</span> <span class="pre">1e-3</span></code>.</dd>
<dt>n_alphas <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Number of alphas along the regularization path</dd>
<dt>alphas <span class="classifier-delimiter">:</span> <span class="classifier">array-like, optional</span></dt>
<dd>List of alphas where to compute the models.
If not provided, set automatically.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span></dt>
<dd>This parameter is ignored when <code class="docutils literal"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<code class="xref py py-class docutils literal"><span class="pre">sklearn.preprocessing.StandardScaler</span></code> before calling <code class="docutils literal"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal"><span class="pre">normalize=False</span></code>.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>The maximum number of iterations.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The tolerance for the optimization: if the updates are
smaller than <code class="docutils literal"><span class="pre">tol</span></code>, the optimization code checks the
dual gap for optimality and continues until it is smaller
than <code class="docutils literal"><span class="pre">tol</span></code>.</dd>
<dt>copy_X <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>If <code class="docutils literal"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</dd>
<dt>cv <span class="classifier-delimiter">:</span> <span class="classifier">int, cross-validation generator or an iterable, optional</span></dt>
<dd><p class="first">Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul class="simple">
<li>None, to use the default 3-fold cross-validation,</li>
<li>integer, to specify the number of folds.</li>
<li><span class="xref std std-term">CV splitter</span>,</li>
<li>An iterable yielding (train, test) splits as arrays of indices.</li>
</ul>
<p>For integer/None inputs, <code class="xref py py-class docutils literal"><span class="pre">KFold</span></code> is used.</p>
<p>Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.20: </span><code class="docutils literal"><span class="pre">cv</span></code> default value if None will change from 3-fold to 5-fold
in v0.22.</p>
</div>
</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">bool or integer</span></dt>
<dd>Amount of verbosity.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Number of CPUs to use during the cross validation. Note that this is
used only if multiple values for l1_ratio are given.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional, default None</span></dt>
<dd>The seed of the pseudo random number generator that selects a random
feature to update.  If int, random_state is the seed used by the random
number generator; If RandomState instance, random_state is the random
number generator; If None, the random number generator is the
RandomState instance used by <cite>np.random</cite>. Used when <code class="docutils literal"><span class="pre">selection</span></code> ==
‘random’</dd>
<dt>selection <span class="classifier-delimiter">:</span> <span class="classifier">str, default ‘cyclic’</span></dt>
<dd>If set to ‘random’, a random coefficient is updated every iteration
rather than looping over features sequentially by default. This
(setting to ‘random’) often leads to significantly faster convergence
especially when tol is higher than 1e-4.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_tasks,)</span></dt>
<dd>Independent term in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_tasks, n_features)</span></dt>
<dd>Parameter vector (W in the cost function formula).
Note that <code class="docutils literal"><span class="pre">coef_</span></code> stores the transpose of <code class="docutils literal"><span class="pre">W</span></code>, <code class="docutils literal"><span class="pre">W.T</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">alpha_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The amount of penalization chosen by cross validation</dd>
<dt><code class="docutils literal"><span class="pre">mse_path_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_alphas, n_folds)</span></dt>
<dd>mean square error for the test set on each fold, varying alpha</dd>
<dt><code class="docutils literal"><span class="pre">alphas_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">numpy array, shape (n_alphas,)</span></dt>
<dd>The grid of alphas used for fitting.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>number of iterations run by the coordinate descent solver to reach
the specified tolerance for the optimal alpha.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">MultiTaskLassoCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_targets</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">MultiTaskLassoCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">0.9994...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">alpha_</span>
<span class="go">0.5713...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">1</span><span class="p">,])</span>
<span class="go">array([[153.7971...,  94.9015...]])</span>
</pre></div>
</div>
<p>See also</p>
<p>MultiTaskElasticNet
ElasticNetCV
MultiTaskElasticNetCV</p>
<p><strong>Notes</strong></p>
<p>The algorithm used to fit the model is coordinate descent.</p>
<p>To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#MultiTaskLassoCVScikitsLearnNode">MultiTaskLassoCVScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.GaussianNBScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">GaussianNBScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.GaussianNBScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Gaussian Naive Bayes (GaussianNB)
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.naive_bayes.GaussianNB</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Can perform online updates to model parameters via <cite>partial_fit</cite> method.
For details on algorithm used to update feature means and variance online,
see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:</p>
<blockquote>
<div><a class="reference external" href="http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf">http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf</a></div></blockquote>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>priors <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_classes,)</span></dt>
<dd>Prior probabilities of the classes. If specified the priors are not
adjusted according to the data.</dd>
<dt>var_smoothing <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1e-9)</span></dt>
<dd>Portion of the largest variance of all features that is added to
variances for calculation stability.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">class_prior_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes,)</span></dt>
<dd>probability of each class.</dd>
<dt><code class="docutils literal"><span class="pre">class_count_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes,)</span></dt>
<dd>number of training samples observed in each class.</dd>
<dt><code class="docutils literal"><span class="pre">theta_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes, n_features)</span></dt>
<dd>mean of each feature per class</dd>
<dt><code class="docutils literal"><span class="pre">sigma_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes, n_features)</span></dt>
<dd>variance of each feature per class</dd>
<dt><code class="docutils literal"><span class="pre">epsilon_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>absolute additive value to variances</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="go">GaussianNB(priors=None, var_smoothing=1e-09)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf_pf</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf_pf</span><span class="o">.</span><span class="n">partial_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span>
<span class="go">GaussianNB(priors=None, var_smoothing=1e-09)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf_pf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#GaussianNBScikitsLearnNode">GaussianNBScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.LabelSpreadingScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">LabelSpreadingScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.LabelSpreadingScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>LabelSpreading model for semi-supervised learning
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.semi_supervised.label_propagation.LabelSpreading</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
This model is similar to the basic Label Propagation algorithm,
but uses affinity matrix based on the normalized graph Laplacian
and soft clamping across the labels.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>kernel <span class="classifier-delimiter">:</span> <span class="classifier">{‘knn’, ‘rbf’, callable}</span></dt>
<dd>String identifier for kernel function to use or the kernel function
itself. Only ‘rbf’ and ‘knn’ strings are valid inputs. The function
passed should take two inputs, each of shape [n_samples, n_features],
and return a [n_samples, n_samples] shaped weight matrix</dd>
<dt>gamma <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>parameter for rbf kernel</dd>
<dt>n_neighbors <span class="classifier-delimiter">:</span> <span class="classifier">integer &gt; 0</span></dt>
<dd>parameter for knn kernel</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Clamping factor. A value in (0, 1) that specifies the relative amount
that an instance should adopt the information from its neighbors as
opposed to its initial label.
alpha=0 means keeping the initial label information; alpha=1 means
replacing all initial information.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd>maximum number of iterations allowed</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Convergence tolerance: threshold to consider the system at steady
state</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The number of parallel jobs to run.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">X_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_samples, n_features]</span></dt>
<dd>Input array.</dd>
<dt><code class="docutils literal"><span class="pre">classes_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes]</span></dt>
<dd>The distinct labels used in classifying instances.</dd>
<dt><code class="docutils literal"><span class="pre">label_distributions_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_samples, n_classes]</span></dt>
<dd>Categorical distribution for each item.</dd>
<dt><code class="docutils literal"><span class="pre">transduction_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_samples]</span></dt>
<dd>Label assigned to each item via the transduction.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of iterations run.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.semi_supervised</span> <span class="kn">import</span> <span class="n">LabelSpreading</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label_prop_model</span> <span class="o">=</span> <span class="n">LabelSpreading</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">random_unlabeled_points</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">))</span> <span class="o">&lt;</span> <span class="mf">0.3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span><span class="p">[</span><span class="n">random_unlabeled_points</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label_prop_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">LabelSpreading(...)</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston,
Bernhard Schoelkopf. Learning with local and global consistency (2004)
<a class="reference external" href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.115.3219">http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.115.3219</a></p>
<p>See Also</p>
<p>LabelPropagation : Unregularized graph based semi-supervised learning</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#LabelSpreadingScikitsLearnNode">LabelSpreadingScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.LatentDirichletAllocationScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">LatentDirichletAllocationScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.LatentDirichletAllocationScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Latent Dirichlet Allocation with online variational Bayes algorithm
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.decomposition.online_lda.LatentDirichletAllocation</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
.. versionadded:: 0.17</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=10)</span></dt>
<dd>Number of topics.</dd>
<dt>doc_topic_prior <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=None)</span></dt>
<dd>Prior of document topic distribution <cite>theta</cite>. If the value is None,
defaults to <cite>1 / n_components</cite>.
In <a href="#id85"><span class="problematic" id="id12">[1]_</span></a>, this is called <cite>alpha</cite>.</dd>
<dt>topic_word_prior <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=None)</span></dt>
<dd>Prior of topic word distribution <cite>beta</cite>. If the value is None, defaults
to <cite>1 / n_components</cite>.
In <a href="#id86"><span class="problematic" id="id13">[1]_</span></a>, this is called <cite>eta</cite>.</dd>
<dt>learning_method <span class="classifier-delimiter">:</span> <span class="classifier">‘batch’ | ‘online’, default=’batch’</span></dt>
<dd><p class="first">Method used to update <cite>_component</cite>. Only used in <cite>fit</cite> method.
In general, if the data size is large, the online update will be much
faster than the batch update.</p>
<p>Valid options:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>&#39;batch&#39;: Batch variational Bayes method. Use all training data in
    each EM update.
    Old `components_` will be overwritten in each iteration.
&#39;online&#39;: Online variational Bayes method. In each EM update, use
    mini-batch of training data to update the ``components_``
    variable incrementally. The learning rate is controlled by the
    ``learning_decay`` and the ``learning_offset`` parameters.
</pre></div>
</div>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.20: </span>The default learning method is now <code class="docutils literal"><span class="pre">&quot;batch&quot;</span></code>.</p>
</div>
</dd>
<dt>learning_decay <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.7)</span></dt>
<dd>It is a parameter that control learning rate in the online learning
method. The value should be set between (0.5, 1.0] to guarantee
asymptotic convergence. When the value is 0.0 and batch_size is
<code class="docutils literal"><span class="pre">n_samples</span></code>, the update method is same as batch learning. In the
literature, this is called kappa.</dd>
<dt>learning_offset <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=10.)</span></dt>
<dd>A (positive) parameter that downweights early iterations in online
learning.  It should be greater than 1.0. In the literature, this is
called tau_0.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=10)</span></dt>
<dd>The maximum number of iterations.</dd>
<dt>batch_size <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=128)</span></dt>
<dd>Number of documents to use in each EM iteration. Only used in online
learning.</dd>
<dt>evaluate_every <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=0)</span></dt>
<dd>How often to evaluate perplexity. Only used in <cite>fit</cite> method.
set it to 0 or negative number to not evalute perplexity in
training at all. Evaluating perplexity can help you check convergence
in training process, but it will also increase total training time.
Evaluating perplexity in every iteration might increase training time
up to two-fold.</dd>
<dt>total_samples <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=1e6)</span></dt>
<dd>Total number of documents. Only used in the <cite>partial_fit</cite> method.</dd>
<dt>perp_tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1e-1)</span></dt>
<dd>Perplexity tolerance in batch learning. Only used when
<code class="docutils literal"><span class="pre">evaluate_every</span></code> is greater than 0.</dd>
<dt>mean_change_tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1e-3)</span></dt>
<dd>Stopping tolerance for updating document topic distribution in E-step.</dd>
<dt>max_doc_update_iter <span class="classifier-delimiter">:</span> <span class="classifier">int (default=100)</span></dt>
<dd>Max number of iterations for updating document topic distribution in
the E-step.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The number of jobs to use in the E-step.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=0)</span></dt>
<dd>Verbosity level.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>n_topics <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=None)</span></dt>
<dd>This parameter has been renamed to n_components and will
be removed in version 0.21.
.. deprecated:: 0.19</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">components_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd><p class="first">Variational parameters for topic word distribution. Since the complete
conditional for topic word distribution is a Dirichlet,
<code class="docutils literal"><span class="pre">components_[i,</span> <span class="pre">j]</span></code> can be viewed as pseudocount that represents the
number of times word <cite>j</cite> was assigned to topic <cite>i</cite>.
It can also be viewed as distribution over the words for each topic
after normalization:</p>
<ul class="last simple">
<li><code class="docutils literal"><span class="pre">model.components_</span> <span class="pre">/</span> <span class="pre">model.components_.sum(axis=1)[:,</span> <span class="pre">np.newaxis]</span></code>.</li>
</ul>
</dd>
<dt><code class="docutils literal"><span class="pre">n_batch_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of iterations of the EM step.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of passes over the dataset.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">LatentDirichletAllocation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_multilabel_classification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># This produces a feature matrix of token counts, similar to what</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># CountVectorizer would produce on text.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">make_multilabel_classification</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lda</span> <span class="o">=</span> <span class="n">LatentDirichletAllocation</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lda</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> 
<span class="go">LatentDirichletAllocation(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># get topics for some given samples:</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lda</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:])</span>
<span class="go">array([[0.00360392, 0.25499205, 0.0036211 , 0.64236448, 0.09541846],</span>
<span class="go">       [0.15297572, 0.00362644, 0.44412786, 0.39568399, 0.003586  ]])</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<dl class="docutils">
<dt>[1] “Online Learning for Latent Dirichlet Allocation”, Matthew D. Hoffman,</dt>
<dd>David M. Blei, Francis Bach, 2010</dd>
<dt>[2] “Stochastic Variational Inference”, Matthew D. Hoffman, David M. Blei,</dt>
<dd>Chong Wang, John Paisley, 2013</dd>
</dl>
<p>[3] Matthew D. Hoffman’s onlineldavb code. Link:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference external" href="https://github.com/blei-lab/onlineldavb">https://github.com/blei-lab/onlineldavb</a></li>
</ul>
</div></blockquote>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#LatentDirichletAllocationScikitsLearnNode">LatentDirichletAllocationScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.NMFScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">NMFScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.NMFScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Non-Negative Matrix Factorization (NMF)
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.decomposition.nmf.NMF</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Find two non-negative matrices (W, H) whose product approximates the non-
negative matrix X. This factorization can be used for example for
dimensionality reduction, source separation or topic extraction.</p>
<p>The objective function is:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="mf">0.5</span> <span class="o">*</span> <span class="o">||</span><span class="n">X</span> <span class="o">-</span> <span class="n">WH</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span>
<span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="o">||</span><span class="n">vec</span><span class="p">(</span><span class="n">W</span><span class="p">)</span><span class="o">||</span><span class="n">_1</span>
<span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="o">||</span><span class="n">vec</span><span class="p">(</span><span class="n">H</span><span class="p">)</span><span class="o">||</span><span class="n">_1</span>
<span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span>
<span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">H</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span>
</pre></div>
</div>
<p>Where:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">||</span><span class="n">A</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span> <span class="o">=</span> \<span class="n">sum_</span><span class="p">{</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">}</span> <span class="n">A_</span><span class="p">{</span><span class="n">ij</span><span class="p">}</span><span class="o">^</span><span class="mi">2</span> <span class="p">(</span><span class="n">Frobenius</span> <span class="n">norm</span><span class="p">)</span>
<span class="o">||</span><span class="n">vec</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="o">||</span><span class="n">_1</span> <span class="o">=</span> \<span class="n">sum_</span><span class="p">{</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">}</span> <span class="nb">abs</span><span class="p">(</span><span class="n">A_</span><span class="p">{</span><span class="n">ij</span><span class="p">})</span> <span class="p">(</span><span class="n">Elementwise</span> <span class="n">L1</span> <span class="n">norm</span><span class="p">)</span>
</pre></div>
</div>
<p>For multiplicative-update (‘mu’) solver, the Frobenius norm
(0.5 * ||X - WH||_Fro^2) can be changed into another beta-divergence loss,
by changing the beta_loss parameter.</p>
<p>The objective function is minimized with an alternating minimization of W
and H.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int or None</span></dt>
<dd>Number of components, if n_components is not set all features
are kept.</dd>
<dt>init <span class="classifier-delimiter">:</span> <span class="classifier">‘random’ | ‘nndsvd’ |  ‘nndsvda’ | ‘nndsvdar’ | ‘custom’</span></dt>
<dd><p class="first">Method used to initialize the procedure.
Default: ‘nndsvd’ if n_components &lt; n_features, otherwise random.
Valid options:</p>
<ul class="last">
<li><p class="first">‘random’: non-negative random matrices, scaled with:</p>
<blockquote>
<div><ul class="simple">
<li>sqrt(X.mean() / n_components)</li>
</ul>
</div></blockquote>
</li>
<li><dl class="first docutils">
<dt>‘nndsvd’: Nonnegative Double Singular Value Decomposition (NNDSVD)</dt>
<dd><p class="first last">initialization (better for sparseness)</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>‘nndsvda’: NNDSVD with zeros filled with the average of X</dt>
<dd><p class="first last">(better when sparsity is not desired)</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>‘nndsvdar’: NNDSVD with zeros filled with small random values</dt>
<dd><p class="first last">(generally faster, less accurate alternative to NNDSVDa
for when sparsity is not desired)</p>
</dd>
</dl>
</li>
<li><p class="first">‘custom’: use custom matrices W and H</p>
</li>
</ul>
</dd>
<dt>solver <span class="classifier-delimiter">:</span> <span class="classifier">‘cd’ | ‘mu’</span></dt>
<dd><p class="first">Numerical solver to use:</p>
<ul class="simple">
<li>‘cd’ is a Coordinate Descent solver.</li>
<li>‘mu’ is a Multiplicative Update solver.</li>
</ul>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17: </span>Coordinate Descent solver.</p>
</div>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19: </span>Multiplicative Update solver.</p>
</div>
</dd>
<dt>beta_loss <span class="classifier-delimiter">:</span> <span class="classifier">float or string, default ‘frobenius’</span></dt>
<dd><p class="first">String must be in {‘frobenius’, ‘kullback-leibler’, ‘itakura-saito’}.
Beta divergence to be minimized, measuring the distance between X
and the dot product WH. Note that values different from ‘frobenius’
(or 2) and ‘kullback-leibler’ (or 1) lead to significantly slower
fits. Note that for beta_loss &lt;= 0 (or ‘itakura-saito’), the input
matrix X cannot contain zeros. Used only in ‘mu’ solver.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19.</span></p>
</div>
</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, default: 1e-4</span></dt>
<dd>Tolerance of the stopping condition.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">integer, default: 200</span></dt>
<dd>Maximum number of iterations before timing out.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional, default: None</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">double, default: 0.</span></dt>
<dd><p class="first">Constant that multiplies the regularization terms. Set it to zero to
have no regularization.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>alpha</em> used in the Coordinate Descent solver.</p>
</div>
</dd>
<dt>l1_ratio <span class="classifier-delimiter">:</span> <span class="classifier">double, default: 0.</span></dt>
<dd><p class="first">The regularization mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.
For l1_ratio = 0 the penalty is an elementwise L2 penalty
(aka Frobenius Norm).
For l1_ratio = 1 it is an elementwise L1 penalty.
For 0 &lt; l1_ratio &lt; 1, the penalty is a combination of L1 and L2.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span>Regularization parameter <em>l1_ratio</em> used in the Coordinate Descent
solver.</p>
</div>
</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">bool, default=False</span></dt>
<dd>Whether to be verbose.</dd>
<dt>shuffle <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default: False</span></dt>
<dd><p class="first">If true, randomize the order of coordinates in the CD solver.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>shuffle</em> parameter used in the Coordinate Descent solver.</p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">components_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>Factorization matrix, sometimes called ‘dictionary’.</dd>
<dt><code class="docutils literal"><span class="pre">reconstruction_err_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">number</span></dt>
<dd>Frobenius norm of the matrix difference, or beta-divergence, between
the training data <code class="docutils literal"><span class="pre">X</span></code> and the reconstructed data <code class="docutils literal"><span class="pre">WH</span></code> from
the fitted model.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Actual number of iterations.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">NMF</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">NMF</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;random&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">W</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">H</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">components_</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>Cichocki, Andrzej, and P. H. A. N. Anh-Huy. “Fast local algorithms for
large scale nonnegative matrix and tensor factorizations.”
IEICE transactions on fundamentals of electronics, communications and
computer sciences 92.3: 708-721, 2009.</p>
<p>Fevotte, C., &amp; Idier, J. (2011). Algorithms for nonnegative matrix
factorization with the beta-divergence. Neural Computation, 23(9).</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.NMFScikitsLearnNode-class.html">NMFScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.ScaledLogOddsEstimatorScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">ScaledLogOddsEstimatorScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.ScaledLogOddsEstimatorScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.ensemble.gradient_boosting.ScaledLogOddsEstimator</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#ScaledLogOddsEstimatorScikitsLearnNode">ScaledLogOddsEstimatorScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.MaxAbsScalerScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">MaxAbsScalerScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.MaxAbsScalerScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Scale each feature by its maximum absolute value.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.preprocessing.data.MaxAbsScaler</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
This estimator scales and translates each feature individually such
that the maximal absolute value of each feature in the
training set will be 1.0. It does not shift/center the data, and
thus does not destroy any sparsity.</p>
<p>This scaler can also be applied to sparse CSR or CSC matrices.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17.</span></p>
</div>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default is True</span></dt>
<dd>Set to False to perform inplace scaling and avoid a copy (if the input
is already a numpy array).</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">scale_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">ndarray, shape (n_features,)</span></dt>
<dd><p class="first">Per feature relative scaling of the data.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>scale_</em> attribute.</p>
</div>
</dd>
<dt><code class="docutils literal"><span class="pre">max_abs_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">ndarray, shape (n_features,)</span></dt>
<dd>Per feature maximum absolute value.</dd>
<dt><code class="docutils literal"><span class="pre">n_samples_seen_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The number of samples processed by the estimator. Will be reset on
new calls to fit, but increments across <code class="docutils literal"><span class="pre">partial_fit</span></code> calls.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MaxAbsScaler</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span> <span class="mf">2.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span> <span class="o">=</span> <span class="n">MaxAbsScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span>
<span class="go">MaxAbsScaler(copy=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([[ 0.5, -1. ,  1. ],</span>
<span class="go">       [ 1. ,  0. ,  0. ],</span>
<span class="go">       [ 0. ,  1. , -0.5]])</span>
</pre></div>
</div>
<p>See also</p>
<p>maxabs_scale: Equivalent function without the estimator API.</p>
<p><strong>Notes</strong></p>
<p>NaNs are treated as missing values: disregarded in fit, and maintained in
transform.</p>
<p>For a comparison of the different scalers, transformers, and normalizers,
see <span class="xref std std-ref">examples/preprocessing/plot_all_scaling.py</span>.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#MaxAbsScalerScikitsLearnNode">MaxAbsScalerScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.HashingVectorizerScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">HashingVectorizerScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.HashingVectorizerScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert a collection of text documents to a matrix of token occurrences
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.feature_extraction.text.HashingVectorizer</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
It turns a collection of text documents into a scipy.sparse matrix holding
token occurrence counts (or binary occurrence information), possibly
normalized as token frequencies if norm=’l1’ or projected on the euclidean
unit sphere if norm=’l2’.</p>
<p>This text vectorizer implementation uses the hashing trick to find the
token string name to feature integer index mapping.</p>
<p>This strategy has several advantages:</p>
<ul class="simple">
<li>it is very low memory scalable to large datasets as there is no need to
store a vocabulary dictionary in memory</li>
<li>it is fast to pickle and un-pickle as it holds no state besides the
constructor parameters</li>
<li>it can be used in a streaming (partial fit) or parallel pipeline as there
is no state computed during fit.</li>
</ul>
<p>There are also a couple of cons (vs using a CountVectorizer with an
in-memory vocabulary):</p>
<ul class="simple">
<li>there is no way to compute the inverse transform (from feature indices to
string feature names) which can be a problem when trying to introspect
which features are most important to a model.</li>
<li>there can be collisions: distinct tokens can be mapped to the same
feature index. However in practice this is rarely an issue if n_features
is large enough (e.g. 2 ** 18 for text classification problems).</li>
<li>no IDF weighting as this would render the transformer stateful.</li>
</ul>
<p>The hash function employed is the signed 32-bit version of Murmurhash3.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>input <span class="classifier-delimiter">:</span> <span class="classifier">string {‘filename’, ‘file’, ‘content’}</span></dt>
<dd><p class="first">If ‘filename’, the sequence passed as an argument to fit is
expected to be a list of filenames that need reading to fetch
the raw content to analyze.</p>
<p>If ‘file’, the sequence items must have a ‘read’ method (file-like
object) that is called to fetch the bytes in memory.</p>
<p class="last">Otherwise the input is expected to be the sequence strings or
bytes items are expected to be analyzed directly.</p>
</dd>
<dt>encoding <span class="classifier-delimiter">:</span> <span class="classifier">string, default=’utf-8’</span></dt>
<dd>If bytes or files are given to analyze, this encoding is used to
decode.</dd>
<dt>decode_error <span class="classifier-delimiter">:</span> <span class="classifier">{‘strict’, ‘ignore’, ‘replace’}</span></dt>
<dd>Instruction on what to do if a byte sequence is given to analyze that
contains characters not of the given <cite>encoding</cite>. By default, it is
‘strict’, meaning that a UnicodeDecodeError will be raised. Other
values are ‘ignore’ and ‘replace’.</dd>
<dt>strip_accents <span class="classifier-delimiter">:</span> <span class="classifier">{‘ascii’, ‘unicode’, None}</span></dt>
<dd><p class="first">Remove accents and perform other character normalization
during the preprocessing step.
‘ascii’ is a fast method that only works on characters that have
an direct ASCII mapping.
‘unicode’ is a slightly slower method that works on any characters.
None (default) does nothing.</p>
<p class="last">Both ‘ascii’ and ‘unicode’ use NFKD normalization from
<code class="xref py py-func docutils literal"><span class="pre">unicodedata.normalize()</span></code>.</p>
</dd>
<dt>lowercase <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=True</span></dt>
<dd>Convert all characters to lowercase before tokenizing.</dd>
<dt>preprocessor <span class="classifier-delimiter">:</span> <span class="classifier">callable or None (default)</span></dt>
<dd>Override the preprocessing (string transformation) stage while
preserving the tokenizing and n-grams generation steps.</dd>
<dt>tokenizer <span class="classifier-delimiter">:</span> <span class="classifier">callable or None (default)</span></dt>
<dd>Override the string tokenization step while preserving the
preprocessing and n-grams generation steps.
Only applies if <code class="docutils literal"><span class="pre">analyzer</span> <span class="pre">==</span> <span class="pre">'word'</span></code>.</dd>
<dt>stop_words <span class="classifier-delimiter">:</span> <span class="classifier">string {‘english’}, list, or None (default)</span></dt>
<dd><p class="first">If ‘english’, a built-in stop word list for English is used.
There are several known issues with ‘english’ and you should
consider an alternative (see <span class="xref std std-ref">stop_words</span>).</p>
<p class="last">If a list, that list is assumed to contain stop words, all of which
will be removed from the resulting tokens.
Only applies if <code class="docutils literal"><span class="pre">analyzer</span> <span class="pre">==</span> <span class="pre">'word'</span></code>.</p>
</dd>
<dt>token_pattern <span class="classifier-delimiter">:</span> <span class="classifier">string</span></dt>
<dd>Regular expression denoting what constitutes a “token”, only used
if <code class="docutils literal"><span class="pre">analyzer</span> <span class="pre">==</span> <span class="pre">'word'</span></code>. The default regexp selects tokens of 2
or more alphanumeric characters (punctuation is completely ignored
and always treated as a token separator).</dd>
<dt>ngram_range <span class="classifier-delimiter">:</span> <span class="classifier">tuple (min_n, max_n), default=(1, 1)</span></dt>
<dd>The lower and upper boundary of the range of n-values for different
n-grams to be extracted. All values of n such that min_n &lt;= n &lt;= max_n
will be used.</dd>
<dt>analyzer <span class="classifier-delimiter">:</span> <span class="classifier">string, {‘word’, ‘char’, ‘char_wb’} or callable</span></dt>
<dd><p class="first">Whether the feature should be made of word or character n-grams.
Option ‘char_wb’ creates character n-grams only from text inside
word boundaries; n-grams at the edges of words are padded with space.</p>
<p class="last">If a callable is passed it is used to extract the sequence of features
out of the raw, unprocessed input.</p>
</dd>
<dt>n_features <span class="classifier-delimiter">:</span> <span class="classifier">integer, default=(2 ** 20)</span></dt>
<dd>The number of features (columns) in the output matrices. Small numbers
of features are likely to cause hash collisions, but large numbers
will cause larger coefficient dimensions in linear learners.</dd>
<dt>binary <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=False.</span></dt>
<dd>If True, all non zero counts are set to 1. This is useful for discrete
probabilistic models that model binary events rather than integer
counts.</dd>
<dt>norm <span class="classifier-delimiter">:</span> <span class="classifier">‘l1’, ‘l2’ or None, optional</span></dt>
<dd>Norm used to normalize term vectors. None for no normalization.</dd>
<dt>alternate_sign <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd><p class="first">When True, an alternating sign is added to the features as to
approximately conserve the inner product in the hashed space even for
small n_features. This approach is similar to sparse random projection.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19.</span></p>
</div>
</dd>
<dt>non_negative <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span></dt>
<dd><p class="first">When True, an absolute value is applied to the features matrix prior to
returning it. When used in conjunction with alternate_sign=True, this
significantly reduces the inner product preservation property.</p>
<div class="last deprecated">
<p><span class="versionmodified">Deprecated since version 0.19: </span>This option will be removed in 0.21.</p>
</div>
</dd>
<dt>dtype <span class="classifier-delimiter">:</span> <span class="classifier">type, optional</span></dt>
<dd>Type of the matrix returned by fit_transform() or transform().</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">HashingVectorizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
<span class="gp">... </span>    <span class="s1">&#39;This is the first document.&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;This document is the second document.&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;And this is the third one.&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;Is this the first document?&#39;</span><span class="p">,</span>
<span class="gp">... </span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">HashingVectorizer</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(4, 16)</span>
</pre></div>
</div>
<p>See also</p>
<p>CountVectorizer, TfidfVectorizer</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#HashingVectorizerScikitsLearnNode">HashingVectorizerScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.LogisticRegressionCVScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">LogisticRegressionCVScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.LogisticRegressionCVScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Logistic Regression CV (aka logit, MaxEnt) classifier.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.logistic.LogisticRegressionCV</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
See glossary entry for <span class="xref std std-term">cross-validation estimator</span>.</p>
<p>This class implements logistic regression using liblinear, newton-cg, sag
of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2
regularization with primal formulation. The liblinear solver supports both
L1 and L2 regularization, with a dual formulation only for the L2 penalty.</p>
<p>For the grid of Cs values (that are set by default to be ten values in
a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is
selected by the cross-validator StratifiedKFold, but it can be changed
using the cv parameter. In the case of newton-cg and lbfgs solvers,
we warm start along the path i.e guess the initial coefficients of the
present fit to be the coefficients got after convergence in the previous
fit, so it is supposed to be faster for high-dimensional dense data.</p>
<p>For a multiclass problem, the hyperparameters for each class are computed
using the best scores got by doing a one-vs-rest in parallel across all
folds and classes. Hence this is not the true multinomial loss.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>Cs <span class="classifier-delimiter">:</span> <span class="classifier">list of floats | int</span></dt>
<dd>Each of the values in Cs describes the inverse of regularization
strength. If Cs is as an int, then a grid of Cs values are chosen
in a logarithmic scale between 1e-4 and 1e4.
Like in support vector machines, smaller values specify stronger
regularization.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">bool, default: True</span></dt>
<dd>Specifies if a constant (a.k.a. bias or intercept) should be
added to the decision function.</dd>
<dt>cv <span class="classifier-delimiter">:</span> <span class="classifier">integer or cross-validation generator, default: None</span></dt>
<dd><p class="first">The default cross-validation generator used is Stratified K-Folds.
If an integer is provided, then it is the number of folds used.
See the module <code class="xref py py-mod docutils literal"><span class="pre">sklearn.model_selection</span></code> module for the
list of possible cross-validation objects.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.20: </span><code class="docutils literal"><span class="pre">cv</span></code> default value if None will change from 3-fold to 5-fold
in v0.22.</p>
</div>
</dd>
<dt>dual <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Dual or primal formulation. Dual formulation is only implemented for
l2 penalty with liblinear solver. Prefer dual=False when
n_samples &gt; n_features.</dd>
<dt>penalty <span class="classifier-delimiter">:</span> <span class="classifier">str, ‘l1’ or ‘l2’</span></dt>
<dd>Used to specify the norm used in the penalization. The ‘newton-cg’,
‘sag’ and ‘lbfgs’ solvers support only l2 penalties.</dd>
<dt>scoring <span class="classifier-delimiter">:</span> <span class="classifier">string, callable, or None</span></dt>
<dd>A string (see model evaluation documentation) or
a scorer callable object / function with signature
<code class="docutils literal"><span class="pre">scorer(estimator,</span> <span class="pre">X,</span> <span class="pre">y)</span></code>. For a list of scoring functions
that can be used, look at <code class="xref py py-mod docutils literal"><span class="pre">sklearn.metrics</span></code>. The
default scoring option used is ‘accuracy’.</dd>
</dl>
<p>solver : str, {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’},              default: ‘lbfgs’.</p>
<blockquote>
<div><p>Algorithm to use in the optimization problem.</p>
<ul class="simple">
<li>For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and
‘saga’ are faster for large ones.</li>
<li>For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’
handle multinomial loss; ‘liblinear’ is limited to one-versus-rest
schemes.</li>
<li>‘newton-cg’, ‘lbfgs’ and ‘sag’ only handle L2 penalty, whereas
‘liblinear’ and ‘saga’ handle L1 penalty.</li>
<li>‘liblinear’ might be slower in LogisticRegressionCV because it does
not handle warm-starting.</li>
</ul>
<p>Note that ‘sag’ and ‘saga’ fast convergence is only guaranteed on
features with approximately the same scale. You can preprocess the data
with a scaler from sklearn.preprocessing.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17: </span>Stochastic Average Gradient descent solver.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.19: </span>SAGA solver.</p>
</div>
</div></blockquote>
<dl class="docutils">
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Tolerance for stopping criteria.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Maximum number of iterations of the optimization algorithm.</dd>
<dt>class_weight <span class="classifier-delimiter">:</span> <span class="classifier">dict or ‘balanced’, optional</span></dt>
<dd><p class="first">Weights associated with classes in the form <code class="docutils literal"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
If not given, all classes are supposed to have weight one.</p>
<p>The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code>.</p>
<p>Note that these weights will be multiplied with sample_weight (passed
through the fit method) if sample_weight is specified.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span>class_weight == ‘balanced’</p>
</div>
</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Number of CPU cores used during the cross-validation loop.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>For the ‘liblinear’, ‘sag’ and ‘lbfgs’ solvers set verbose to any
positive number for verbosity.</dd>
<dt>refit <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>If set to True, the scores are averaged across all folds, and the
coefs and the C that corresponds to the best score is taken, and a
final refit is done using these parameters.
Otherwise the coefs, intercepts and C that correspond to the
best scores across folds are averaged.</dd>
<dt>intercept_scaling <span class="classifier-delimiter">:</span> <span class="classifier">float, default 1.</span></dt>
<dd><p class="first">Useful only when the solver ‘liblinear’ is used
and self.fit_intercept is set to True. In this case, x becomes
[x, self.intercept_scaling],
i.e. a “synthetic” feature with constant value equal to
intercept_scaling is appended to the instance vector.
The intercept becomes <code class="docutils literal"><span class="pre">intercept_scaling</span> <span class="pre">*</span> <span class="pre">synthetic_feature_weight</span></code>.</p>
<p class="last">Note! the synthetic feature weight is subject to l1/l2 regularization
as all other features.
To lessen the effect of regularization on synthetic feature weight
(and therefore on the intercept) intercept_scaling has to be increased.</p>
</dd>
<dt>multi_class <span class="classifier-delimiter">:</span> <span class="classifier">str, {‘ovr’, ‘multinomial’, ‘auto’}, default: ‘ovr’</span></dt>
<dd><p class="first">If the option chosen is ‘ovr’, then a binary problem is fit for each
label. For ‘multinomial’ the loss minimised is the multinomial loss fit
across the entire probability distribution, <em>even when the data is
binary</em>. ‘multinomial’ is unavailable when solver=’liblinear’.
‘auto’ selects ‘ovr’ if the data is binary, or if solver=’liblinear’,
and otherwise selects ‘multinomial’.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.18: </span>Stochastic Average Gradient descent solver for ‘multinomial’ case.</p>
</div>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.20: </span>Default will change from ‘ovr’ to ‘auto’ in 0.22.</p>
</div>
</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional, default None</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">classes_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes, )</span></dt>
<dd>A list of class labels known to the classifier.</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (1, n_features) or (n_classes, n_features)</span></dt>
<dd><p class="first">Coefficient of the features in the decision function.</p>
<p class="last"><cite>coef_</cite> is of shape (1, n_features) when the given problem
is binary.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (1,) or (n_classes,)</span></dt>
<dd><p class="first">Intercept (a.k.a. bias) added to the decision function.</p>
<p class="last">If <cite>fit_intercept</cite> is set to False, the intercept is set to zero.
<cite>intercept_</cite> is of shape(1,) when the problem is binary.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">Cs_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array</span></dt>
<dd>Array of C i.e. inverse of regularization parameter values used
for cross-validation.</dd>
<dt><code class="docutils literal"><span class="pre">coefs_paths_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape <code class="docutils literal"><span class="pre">(n_folds,</span> <span class="pre">len(Cs_),</span> <span class="pre">n_features)</span></code> or                    <code class="docutils literal"><span class="pre">(n_folds,</span> <span class="pre">len(Cs_),</span> <span class="pre">n_features</span> <span class="pre">+</span> <span class="pre">1)</span></code></span></dt>
<dd>dict with classes as the keys, and the path of coefficients obtained
during cross-validating across each fold and then across each Cs
after doing an OvR for the corresponding class as values.
If the ‘multi_class’ option is set to ‘multinomial’, then
the coefs_paths are the coefficients corresponding to each class.
Each dict value has shape <code class="docutils literal"><span class="pre">(n_folds,</span> <span class="pre">len(Cs_),</span> <span class="pre">n_features)</span></code> or
<code class="docutils literal"><span class="pre">(n_folds,</span> <span class="pre">len(Cs_),</span> <span class="pre">n_features</span> <span class="pre">+</span> <span class="pre">1)</span></code> depending on whether the
intercept is fit or not.</dd>
<dt><code class="docutils literal"><span class="pre">scores_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd>dict with classes as the keys, and the values as the
grid of scores obtained during cross-validating each fold, after doing
an OvR for the corresponding class. If the ‘multi_class’ option
given is ‘multinomial’ then the same scores are repeated across
all classes, since this is the multinomial class.
Each dict value has shape (n_folds, len(Cs))</dd>
<dt><code class="docutils literal"><span class="pre">C_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes,) or (n_classes - 1,)</span></dt>
<dd>Array of C that maps to the best scores across every class. If refit is
set to False, then for each class, the best C is the average of the
C’s that correspond to the best scores for each fold.
<cite>C_</cite> is of shape(n_classes,) when the problem is binary.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)</span></dt>
<dd>Actual number of iterations for all classes, folds and Cs.
In the binary or multinomial cases, the first dimension is equal to 1.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegressionCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">LogisticRegressionCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;multinomial&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">2</span><span class="p">,</span> <span class="p">:])</span>
<span class="go">array([0, 0])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">2</span><span class="p">,</span> <span class="p">:])</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(2, 3)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">0.98...</span>
</pre></div>
</div>
<p>See also</p>
<p>LogisticRegression</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#LogisticRegressionCVScikitsLearnNode">LogisticRegressionCVScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.ZeroEstimatorScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">ZeroEstimatorScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.ZeroEstimatorScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.ensemble.gradient_boosting.ZeroEstimator</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#ZeroEstimatorScikitsLearnNode">ZeroEstimatorScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.SVCScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">SVCScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.SVCScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>C-Support Vector Classification.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.svm.classes.SVC</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
The implementation is based on libsvm. The fit time complexity
is more than quadratic with the number of samples which makes it hard
to scale to dataset with more than a couple of 10000 samples.</p>
<p>The multiclass support is handled according to a one-vs-one scheme.</p>
<p>For details on the precise mathematical formulation of the provided
kernel functions and how <cite>gamma</cite>, <cite>coef0</cite> and <cite>degree</cite> affect each
other, see the corresponding section in the narrative documentation:</p>
<p><span class="xref std std-ref">svm_kernels</span>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>C <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span></dt>
<dd>Penalty parameter C of the error term.</dd>
<dt>kernel <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=’rbf’)</span></dt>
<dd>Specifies the kernel type to be used in the algorithm.
It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or
a callable.
If none is given, ‘rbf’ will be used. If a callable is given it is
used to pre-compute the kernel matrix from data matrices; that matrix
should be an array of shape <code class="docutils literal"><span class="pre">(n_samples,</span> <span class="pre">n_samples)</span></code>.</dd>
<dt>degree <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=3)</span></dt>
<dd>Degree of the polynomial kernel function (‘poly’).
Ignored by all other kernels.</dd>
<dt>gamma <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=’auto’)</span></dt>
<dd><p class="first">Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.</p>
<p class="last">Current default is ‘auto’ which uses 1 / n_features,
if <code class="docutils literal"><span class="pre">gamma='scale'</span></code> is passed then it uses 1 / (n_features * X.var())
as value of gamma. The current default of gamma, ‘auto’, will change
to ‘scale’ in version 0.22. ‘auto_deprecated’, a deprecated version of
‘auto’ is used as a default indicating that no explicit value of gamma
was passed.</p>
</dd>
<dt>coef0 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.0)</span></dt>
<dd>Independent term in kernel function.
It is only significant in ‘poly’ and ‘sigmoid’.</dd>
<dt>shrinking <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span></dt>
<dd>Whether to use the shrinking heuristic.</dd>
<dt>probability <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=False)</span></dt>
<dd>Whether to enable probability estimates. This must be enabled prior
to calling <cite>fit</cite>, and will slow down that method.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1e-3)</span></dt>
<dd>Tolerance for stopping criterion.</dd>
<dt>cache_size <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Specify the size of the kernel cache (in MB).</dd>
<dt>class_weight <span class="classifier-delimiter">:</span> <span class="classifier">{dict, ‘balanced’}, optional</span></dt>
<dd>Set the parameter C of class i to class_weight[i]*C for
SVC. If not given, all classes are supposed to have
weight one.
The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">bool, default: False</span></dt>
<dd>Enable verbose output. Note that this setting takes advantage of a
per-process runtime setting in libsvm that, if enabled, may not work
properly in a multithreaded context.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=-1)</span></dt>
<dd>Hard limit on iterations within solver, or -1 for no limit.</dd>
<dt>decision_function_shape <span class="classifier-delimiter">:</span> <span class="classifier">‘ovo’, ‘ovr’, default=’ovr’</span></dt>
<dd><p class="first">Whether to return a one-vs-rest (‘ovr’) decision function of shape
(n_samples, n_classes) as all other classifiers, or the original
one-vs-one (‘ovo’) decision function of libsvm which has shape
(n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one
(‘ovo’) is always used as multi-class strategy.</p>
<div class="versionchanged">
<p><span class="versionmodified">Changed in version 0.19: </span>decision_function_shape is ‘ovr’ by default.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>decision_function_shape=’ovr’</em> is recommended.</p>
</div>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.17: </span>Deprecated <em>decision_function_shape=’ovo’ and None</em>.</p>
</div>
</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>The seed of the pseudo random number generator used when shuffling
the data for probability estimates. If int, random_state is the
seed used by the random number generator; If RandomState instance,
random_state is the random number generator; If None, the random
number generator is the RandomState instance used by <cite>np.random</cite>.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">support_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_SV]</span></dt>
<dd>Indices of support vectors.</dd>
<dt><code class="docutils literal"><span class="pre">support_vectors_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_SV, n_features]</span></dt>
<dd>Support vectors.</dd>
<dt><code class="docutils literal"><span class="pre">n_support_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, dtype=int32, shape = [n_class]</span></dt>
<dd>Number of support vectors for each class.</dd>
<dt><code class="docutils literal"><span class="pre">dual_coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_class-1, n_SV]</span></dt>
<dd>Coefficients of the support vector in the decision function.
For multiclass, coefficient for all 1-vs-1 classifiers.
The layout of the coefficients in the multiclass case is somewhat
non-trivial. See the section about multi-class classification in the
SVM section of the User Guide for details.</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_class * (n_class-1) / 2, n_features]</span></dt>
<dd><p class="first">Weights assigned to the features (coefficients in the primal
problem). This is only available in the case of a linear kernel.</p>
<p class="last"><cite>coef_</cite> is a readonly property derived from <cite>dual_coef_</cite> and
<cite>support_vectors_</cite>.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_class * (n_class-1) / 2]</span></dt>
<dd>Constants in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">fit_status_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>0 if correctly fitted, 1 otherwise (will raise warning)</dd>
</dl>
<p><code class="docutils literal"><span class="pre">probA_</span></code> : array, shape = [n_class * (n_class-1) / 2]
<code class="docutils literal"><span class="pre">probB_</span></code> : array, shape = [n_class * (n_class-1) / 2]</p>
<blockquote>
<div>If probability=True, the parameters learned in Platt scaling to
produce probability estimates from decision values. If
probability=False, an empty array. Platt scaling uses the logistic
function
<code class="docutils literal"><span class="pre">1</span> <span class="pre">/</span> <span class="pre">(1</span> <span class="pre">+</span> <span class="pre">exp(decision_value</span> <span class="pre">*</span> <span class="pre">``probA_</span></code> + <a href="#id87"><span class="problematic" id="id88">probB_</span></a>))``
where <code class="docutils literal"><span class="pre">probA_</span></code> and <code class="docutils literal"><span class="pre">probB_</span></code> are learned from the dataset <a href="#id89"><span class="problematic" id="id14">[2]_</span></a>. For
more information on the multiclass case and training procedure see
section 8 of <a href="#id90"><span class="problematic" id="id15">[1]_</span></a>.</div></blockquote>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,</span>
<span class="go">    decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto&#39;, kernel=&#39;rbf&#39;,</span>
<span class="go">    max_iter=-1, probability=False, random_state=None, shrinking=True,</span>
<span class="go">    tol=0.001, verbose=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>SVR</dt>
<dd>Support Vector Machine for Regression implemented using libsvm.</dd>
<dt>LinearSVC</dt>
<dd>Scalable Linear Support Vector Machine for classification
implemented using liblinear. Check the See also section of
LinearSVC for more comparison element.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id16" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf">LIBSVM: A Library for Support Vector Machines</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id17" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><a class="reference external" href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639">Platt, John (1999). “Probabilistic outputs for support vector
machines and comparison to regularizedlikelihood methods.”</a></td></tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.SVCScikitsLearnNode-class.html">SVCScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.IsotonicRegressionScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">IsotonicRegressionScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.IsotonicRegressionScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Isotonic regression model.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.isotonic.IsotonicRegression</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
The isotonic regression optimization problem is defined by:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="nb">min</span> <span class="nb">sum</span> <span class="n">w_i</span> <span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">y_</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span>

<span class="n">subject</span> <span class="n">to</span> <span class="n">y_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">y_</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="n">whenever</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">X</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
<span class="ow">and</span> <span class="nb">min</span><span class="p">(</span><span class="n">y_</span><span class="p">)</span> <span class="o">=</span> <span class="n">y_min</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">y_</span><span class="p">)</span> <span class="o">=</span> <span class="n">y_max</span>
</pre></div>
</div>
<p>where:</p>
<blockquote>
<div><ul class="simple">
<li><ul class="first">
<li><code class="docutils literal"><span class="pre">y[i]</span></code> are inputs (real numbers)</li>
</ul>
</li>
<li><ul class="first">
<li><code class="docutils literal"><span class="pre">y_[i]</span></code> are fitted</li>
</ul>
</li>
<li><ul class="first">
<li><code class="docutils literal"><span class="pre">X</span></code> specifies the order.</li>
</ul>
</li>
<li>If <code class="docutils literal"><span class="pre">X</span></code> is non-decreasing then <code class="docutils literal"><span class="pre">y_</span></code> is non-decreasing.</li>
<li><ul class="first">
<li><code class="docutils literal"><span class="pre">w[i]</span></code> are optional strictly positive weights (default to 1.0)</li>
</ul>
</li>
</ul>
</div></blockquote>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>y_min <span class="classifier-delimiter">:</span> <span class="classifier">optional, default: None</span></dt>
<dd>If not None, set the lowest value of the fit to y_min.</dd>
<dt>y_max <span class="classifier-delimiter">:</span> <span class="classifier">optional, default: None</span></dt>
<dd>If not None, set the highest value of the fit to y_max.</dd>
<dt>increasing <span class="classifier-delimiter">:</span> <span class="classifier">boolean or string, optional, default: True</span></dt>
<dd><p class="first">If boolean, whether or not to fit the isotonic regression with y
increasing or decreasing.</p>
<p class="last">The string value “auto” determines whether y should
increase or decrease based on the Spearman correlation estimate’s
sign.</p>
</dd>
<dt>out_of_bounds <span class="classifier-delimiter">:</span> <span class="classifier">string, optional, default: “nan”</span></dt>
<dd>The <code class="docutils literal"><span class="pre">out_of_bounds</span></code> parameter handles how x-values outside of the
training domain are handled.  When set to “nan”, predicted y-values
will be NaN.  When set to “clip”, predicted y-values will be
set to the value corresponding to the nearest train interval endpoint.
When set to “raise”, allow <code class="docutils literal"><span class="pre">interp1d</span></code> to throw ValueError.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">X_min_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Minimum value of input array <cite>X_</cite> for left bound.</dd>
<dt><code class="docutils literal"><span class="pre">X_max_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Maximum value of input array <cite>X_</cite> for right bound.</dd>
<dt><code class="docutils literal"><span class="pre">f_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">function</span></dt>
<dd>The stepwise interpolating function that covers the input domain <code class="docutils literal"><span class="pre">X</span></code>.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>Ties are broken using the secondary method from Leeuw, 1977.</p>
<p><strong>References</strong></p>
<p>Isotonic Median Regression: A Linear Programming Approach
Nilotpal Chakravarti
Mathematics of Operations Research
Vol. 14, No. 2 (May, 1989), pp. 303-308</p>
<p>Isotone Optimization in R : Pool-Adjacent-Violators
Algorithm (PAVA) and Active Set Methods
Leeuw, Hornik, Mair
Journal of Statistical Software 2009</p>
<p>Correctness of Kruskal’s algorithms for monotone regression with ties
Leeuw, Psychometrica, 1977</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#IsotonicRegressionScikitsLearnNode">IsotonicRegressionScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.DictVectorizerScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">DictVectorizerScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.DictVectorizerScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Transforms lists of feature-value mappings to vectors.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.feature_extraction.dict_vectorizer.DictVectorizer</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
This transformer turns lists of mappings (dict-like objects) of feature
names to feature values into Numpy arrays or scipy.sparse matrices for use
with scikit-learn estimators.</p>
<p>When feature values are strings, this transformer will do a binary one-hot
(aka one-of-K) coding: one boolean-valued feature is constructed for each
of the possible string values that the feature can take on. For instance,
a feature “f” that can take on the values “ham” and “spam” will become two
features in the output, one signifying “f=ham”, the other “f=spam”.</p>
<p>However, note that this transformer will only do a binary one-hot encoding
when feature values are of type string. If categorical features are
represented as numeric values such as int, the DictVectorizer can be
followed by <code class="xref py py-class docutils literal"><span class="pre">sklearn.preprocessing.OneHotEncoder</span></code> to complete
binary one-hot encoding.</p>
<p>Features that do not occur in a sample (mapping) will have a zero value
in the resulting array/matrix.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>dtype <span class="classifier-delimiter">:</span> <span class="classifier">callable, optional</span></dt>
<dd>The type of feature values. Passed to Numpy array/scipy.sparse matrix
constructors as the dtype argument.</dd>
<dt>separator <span class="classifier-delimiter">:</span> <span class="classifier">string, optional</span></dt>
<dd>Separator string used when constructing new features for one-hot
coding.</dd>
<dt>sparse <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional.</span></dt>
<dd>Whether transform should produce scipy.sparse matrices.
True by default.</dd>
<dt>sort <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional.</span></dt>
<dd>Whether <code class="docutils literal"><span class="pre">feature_names_</span></code> and <code class="docutils literal"><span class="pre">vocabulary_</span></code> should be sorted when fitting.
True by default.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">vocabulary_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd>A dictionary mapping feature names to feature indices.</dd>
<dt><code class="docutils literal"><span class="pre">feature_names_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list</span></dt>
<dd>A list of length n_features containing the feature names (e.g., “f=ham”
and “f=spam”).</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction</span> <span class="kn">import</span> <span class="n">DictVectorizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">DictVectorizer</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">D</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;foo&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;bar&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span> <span class="p">{</span><span class="s1">&#39;foo&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">&#39;baz&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span>
<span class="go">array([[2., 0., 1.],</span>
<span class="go">       [0., 1., 3.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">==</span>         <span class="p">[{</span><span class="s1">&#39;bar&#39;</span><span class="p">:</span> <span class="mf">2.0</span><span class="p">,</span> <span class="s1">&#39;foo&#39;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">},</span> <span class="p">{</span><span class="s1">&#39;baz&#39;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;foo&#39;</span><span class="p">:</span> <span class="mf">3.0</span><span class="p">}]</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span><span class="o">.</span><span class="n">transform</span><span class="p">({</span><span class="s1">&#39;foo&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;unseen_feature&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">})</span>
<span class="go">array([[0., 0., 4.]])</span>
</pre></div>
</div>
<p>See also</p>
<p>FeatureHasher : performs vectorization using only a hash function.
sklearn.preprocessing.OrdinalEncoder : handles nominal/categorical</p>
<blockquote>
<div>features encoded as columns of arbitrary data types.</div></blockquote>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#DictVectorizerScikitsLearnNode">DictVectorizerScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.LinearSVCScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">LinearSVCScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.LinearSVCScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Linear Support Vector Classification.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.svm.classes.LinearSVC</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Similar to SVC with parameter kernel=’linear’, but implemented in terms of
liblinear rather than libsvm, so it has more flexibility in the choice of
penalties and loss functions and should scale better to large numbers of
samples.</p>
<p>This class supports both dense and sparse input and the multiclass support
is handled according to a one-vs-the-rest scheme.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>penalty <span class="classifier-delimiter">:</span> <span class="classifier">string, ‘l1’ or ‘l2’ (default=’l2’)</span></dt>
<dd>Specifies the norm used in the penalization. The ‘l2’
penalty is the standard used in SVC. The ‘l1’ leads to <code class="docutils literal"><span class="pre">coef_</span></code>
vectors that are sparse.</dd>
<dt>loss <span class="classifier-delimiter">:</span> <span class="classifier">string, ‘hinge’ or ‘squared_hinge’ (default=’squared_hinge’)</span></dt>
<dd>Specifies the loss function. ‘hinge’ is the standard SVM loss
(used e.g. by the SVC class) while ‘squared_hinge’ is the
square of the hinge loss.</dd>
<dt>dual <span class="classifier-delimiter">:</span> <span class="classifier">bool, (default=True)</span></dt>
<dd>Select the algorithm to either solve the dual or primal
optimization problem. Prefer dual=False when n_samples &gt; n_features.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1e-4)</span></dt>
<dd>Tolerance for stopping criteria.</dd>
<dt>C <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span></dt>
<dd>Penalty parameter C of the error term.</dd>
<dt>multi_class <span class="classifier-delimiter">:</span> <span class="classifier">string, ‘ovr’ or ‘crammer_singer’ (default=’ovr’)</span></dt>
<dd>Determines the multi-class strategy if <cite>y</cite> contains more than
two classes.
<code class="docutils literal"><span class="pre">&quot;ovr&quot;</span></code> trains n_classes one-vs-rest classifiers, while
<code class="docutils literal"><span class="pre">&quot;crammer_singer&quot;</span></code> optimizes a joint objective over all classes.
While <cite>crammer_singer</cite> is interesting from a theoretical perspective
as it is consistent, it is seldom used in practice as it rarely leads
to better accuracy and is more expensive to compute.
If <code class="docutils literal"><span class="pre">&quot;crammer_singer&quot;</span></code> is chosen, the options loss, penalty and dual
will be ignored.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span></dt>
<dd>Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(i.e. data is expected to be already centered).</dd>
<dt>intercept_scaling <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1)</span></dt>
<dd>When self.fit_intercept is True, instance vector x becomes
<code class="docutils literal"><span class="pre">[x,</span> <span class="pre">self.intercept_scaling]</span></code>,
i.e. a “synthetic” feature with constant value equals to
intercept_scaling is appended to the instance vector.
The intercept becomes intercept_scaling * synthetic feature weight
Note! the synthetic feature weight is subject to l1/l2 regularization
as all other features.
To lessen the effect of regularization on synthetic feature weight
(and therefore on the intercept) intercept_scaling has to be increased.</dd>
<dt>class_weight <span class="classifier-delimiter">:</span> <span class="classifier">{dict, ‘balanced’}, optional</span></dt>
<dd>Set the parameter C of class i to <code class="docutils literal"><span class="pre">class_weight[i]*C</span></code> for
SVC. If not given, all classes are supposed to have
weight one.
The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, (default=0)</span></dt>
<dd>Enable verbose output. Note that this setting takes advantage of a
per-process runtime setting in liblinear that, if enabled, may not work
properly in a multithreaded context.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>The seed of the pseudo random number generator to use when shuffling
the data for the dual coordinate descent (if <code class="docutils literal"><span class="pre">dual=True</span></code>). When
<code class="docutils literal"><span class="pre">dual=False</span></code> the underlying implementation of <code class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></code>
is not random and <code class="docutils literal"><span class="pre">random_state</span></code> has no effect on the results. If
int, random_state is the seed used by the random number generator; If
RandomState instance, random_state is the random number generator; If
None, the random number generator is the RandomState instance used by
<cite>np.random</cite>.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, (default=1000)</span></dt>
<dd>The maximum number of iterations to be run.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features] if n_classes == 2 else [n_classes, n_features]</span></dt>
<dd><p class="first">Weights assigned to the features (coefficients in the primal
problem). This is only available in the case of a linear kernel.</p>
<p class="last"><code class="docutils literal"><span class="pre">coef_</span></code> is a readonly property derived from <code class="docutils literal"><span class="pre">raw_coef_</span></code> that
follows the internal memory layout of liblinear.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1] if n_classes == 2 else [n_classes]</span></dt>
<dd>Constants in decision function.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">LinearSVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,</span>
<span class="go">     intercept_scaling=1, loss=&#39;squared_hinge&#39;, max_iter=1000,</span>
<span class="go">     multi_class=&#39;ovr&#39;, penalty=&#39;l2&#39;, random_state=0, tol=1e-05, verbose=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">[[0.085... 0.394... 0.498... 0.375...]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="go">[0.284...]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>The underlying C implementation uses a random number generator to
select features when fitting the model. It is thus not uncommon
to have slightly different results for the same input data. If
that happens, try with a smaller <code class="docutils literal"><span class="pre">tol</span></code> parameter.</p>
<p>The underlying implementation, liblinear, uses a sparse internal
representation for the data that will incur a memory copy.</p>
<p>Predict output may not match that of standalone liblinear in certain
cases. See <span class="xref std std-ref">differences from liblinear</span>
in the narrative documentation.</p>
<p><strong>References</strong></p>
<p><a class="reference external" href="https://www.csie.ntu.edu.tw/~cjlin/liblinear/">LIBLINEAR: A Library for Large Linear Classification</a></p>
<p>See also</p>
<dl class="docutils">
<dt>SVC</dt>
<dd><p class="first">Implementation of Support Vector Machine classifier using libsvm:</p>
<ul class="simple">
<li>the kernel can be non-linear but its SMO algorithm does not</li>
<li>scale to large number of samples as LinearSVC does.</li>
</ul>
<p>Furthermore SVC multi-class mode is implemented using one
vs one scheme while LinearSVC uses one vs the rest. It is
possible to implement one vs the rest with SVC by using the
<code class="xref py py-class docutils literal"><span class="pre">sklearn.multiclass.OneVsRestClassifier</span></code> wrapper.</p>
<p class="last">Finally SVC can fit dense data without memory copy if the input
is C-contiguous. Sparse data will still incur memory copy though.</p>
</dd>
<dt>sklearn.linear_model.SGDClassifier</dt>
<dd>SGDClassifier can optimize the same cost function as LinearSVC
by adjusting the penalty and loss parameters. In addition it requires
less memory, allows incremental (online) learning, and implements
various loss functions and regularization regimes.</dd>
</dl>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.LinearSVCScikitsLearnNode-class.html">LinearSVCScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.RandomizedLassoScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">RandomizedLassoScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.RandomizedLassoScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomized Lasso.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.randomized_l1.RandomizedLasso</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Randomized Lasso works by subsampling the training data and
computing a Lasso estimate where the penalty of a random subset of
coefficients has been scaled. By performing this double
randomization several times, the method assigns high scores to
features that are repeatedly selected across randomizations. This
is known as stability selection. In short, features selected more
often are considered good features.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, ‘aic’, or ‘bic’, optional</span></dt>
<dd>The regularization parameter alpha parameter in the Lasso.
Warning: this is not the alpha parameter in the stability selection
article which is scaling.</dd>
<dt>scaling <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The s parameter used to randomly scale the penalty of different
features.
Should be between 0 and 1.</dd>
<dt>sample_fraction <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The fraction of samples to be used in each randomized design.
Should be between 0 and 1. If 1, all samples are used.</dd>
<dt>n_resampling <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Number of randomized models.</dd>
<dt>selection_threshold <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The score above which features should be selected.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">boolean or integer, optional</span></dt>
<dd>Sets the verbosity amount</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>If True, the regressors X will be normalized before regression.
This parameter is ignored when <cite>fit_intercept</cite> is set to False.
When the regressors are normalized, note that this makes the
hyperparameters learned more robust and almost independent of
the number of samples. The same property is not valid for
standardized data. However, if you wish to standardize, please
use <cite>preprocessing.StandardScaler</cite> before calling <cite>fit</cite> on an
estimator with <cite>normalize=False</cite>.</dd>
<dt>precompute <span class="classifier-delimiter">:</span> <span class="classifier">True | False | ‘auto’ | array-like</span></dt>
<dd>Whether to use a precomputed Gram matrix to speed up calculations.
If set to ‘auto’ let us decide.
The Gram matrix can also be passed as argument, but it will be used
only for the selection of parameter alpha, if alpha is ‘aic’ or ‘bic’.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span></dt>
<dd>Maximum number of iterations to perform in the Lars algorithm.</dd>
<dt>eps <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The machine-precision regularization in the computation of the
Cholesky diagonal factors. Increase this for very ill-conditioned
systems. Unlike the ‘tol’ parameter in some iterative
optimization-based algorithms, this parameter does not control
the tolerance of the optimization.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Number of CPUs to use during the resampling.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>pre_dispatch <span class="classifier-delimiter">:</span> <span class="classifier">int, or string, optional</span></dt>
<dd><p class="first">Controls the number of jobs that get dispatched during parallel
execution. Reducing this number can be useful to avoid an
explosion of memory consumption when more jobs get dispatched
than CPUs can process. This parameter can be:</p>
<blockquote class="last">
<div><ul class="simple">
<li>None, in which case all the jobs are immediately
created and spawned. Use this for lightweight and
fast-running jobs, to avoid delays due to on-demand
spawning of the jobs</li>
<li>An int, giving the exact number of total jobs that are
spawned</li>
<li>A string, giving an expression as a function of n_jobs,
as in ‘2*n_jobs’</li>
</ul>
</div></blockquote>
</dd>
<dt>memory <span class="classifier-delimiter">:</span> <span class="classifier">None, str or object with the joblib.Memory interface, optional             (default=None)</span></dt>
<dd>Used for internal caching. By default, no caching is done.
If a string is given, it is the path to the caching directory.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">scores_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features]</span></dt>
<dd>Feature scores between 0 and 1.</dd>
<dt><code class="docutils literal"><span class="pre">all_scores_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features, n_reg_parameter]</span></dt>
<dd>Feature scores between 0 and 1 for all values of the regularization         parameter. The reference article suggests <code class="docutils literal"><span class="pre">scores_</span></code> is the max of         <code class="docutils literal"><span class="pre">all_scores_</span></code>.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RandomizedLasso</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">randomized_lasso</span> <span class="o">=</span> <span class="n">RandomizedLasso</span><span class="p">()</span> 
</pre></div>
</div>
<p><strong>References</strong></p>
<p>Stability selection
Nicolai Meinshausen, Peter Buhlmann
Journal of the Royal Statistical Society: Series B
Volume 72, Issue 4, pages 417-473, September 2010
DOI: 10.1111/j.1467-9868.2010.00740.x</p>
<p>See also</p>
<p>RandomizedLogisticRegression, Lasso, ElasticNet</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#RandomizedLassoScikitsLearnNode">RandomizedLassoScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.MultiLabelBinarizerScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">MultiLabelBinarizerScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.MultiLabelBinarizerScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform between iterable of iterables and a multilabel format
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.preprocessing.label.MultiLabelBinarizer</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Although a list of sets or tuples is a very intuitive format for multilabel
data, it is unwieldy to process. This transformer converts between this
intuitive format and the supported multilabel format: a (samples x classes)
binary matrix indicating the presence of a class label.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>classes <span class="classifier-delimiter">:</span> <span class="classifier">array-like of shape [n_classes] (optional)</span></dt>
<dd>Indicates an ordering for the class labels.
All entries should be unique (cannot contain duplicate classes).</dd>
<dt>sparse_output <span class="classifier-delimiter">:</span> <span class="classifier">boolean (default: False),</span></dt>
<dd>Set to true if output binary array is desired in CSR sparse format</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">classes_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of labels</span></dt>
<dd>A copy of the <cite>classes</cite> parameter where provided,
or otherwise, the sorted set of classes found when fitting.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MultiLabelBinarizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mlb</span> <span class="o">=</span> <span class="n">MultiLabelBinarizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mlb</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">([(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)])</span>
<span class="go">array([[1, 1, 0],</span>
<span class="go">       [0, 0, 1]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mlb</span><span class="o">.</span><span class="n">classes_</span>
<span class="go">array([1, 2, 3])</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">mlb</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">([</span><span class="nb">set</span><span class="p">([</span><span class="s1">&#39;sci-fi&#39;</span><span class="p">,</span> <span class="s1">&#39;thriller&#39;</span><span class="p">]),</span> <span class="nb">set</span><span class="p">([</span><span class="s1">&#39;comedy&#39;</span><span class="p">])])</span>
<span class="go">array([[0, 1, 1],</span>
<span class="go">       [1, 0, 0]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">mlb</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="go">[&#39;comedy&#39;, &#39;sci-fi&#39;, &#39;thriller&#39;]</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>sklearn.preprocessing.OneHotEncoder <span class="classifier-delimiter">:</span> <span class="classifier">encode categorical features</span></dt>
<dd>using a one-hot aka one-of-K scheme.</dd>
</dl>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#MultiLabelBinarizerScikitsLearnNode">MultiLabelBinarizerScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.FastICAScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">FastICAScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.FastICAScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>FastICA: a fast algorithm for Independent Component Analysis.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.decomposition.fastica_.FastICA</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Number of components to use. If none is passed, all are used.</dd>
<dt>algorithm <span class="classifier-delimiter">:</span> <span class="classifier">{‘parallel’, ‘deflation’}</span></dt>
<dd>Apply parallel or deflational algorithm for FastICA.</dd>
<dt>whiten <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>If whiten is false, the data is already considered to be
whitened, and no whitening is performed.</dd>
<dt>fun <span class="classifier-delimiter">:</span> <span class="classifier">string or function, optional. Default: ‘logcosh’</span></dt>
<dd><p class="first">The functional form of the G function used in the
approximation to neg-entropy. Could be either ‘logcosh’, ‘exp’,
or ‘cube’.
You can also provide your own function. It should return a tuple
containing the value of the function, and of its derivative, in the
point. Example:</p>
<p>def my_g(x):</p>
<blockquote class="last">
<div><ul class="simple">
<li>return x ** 3, (3 * x ** 2).mean(axis=-1)</li>
</ul>
</div></blockquote>
</dd>
<dt>fun_args <span class="classifier-delimiter">:</span> <span class="classifier">dictionary, optional</span></dt>
<dd>Arguments to send to the functional form.
If empty and if fun=’logcosh’, fun_args will take value
{‘alpha’ : 1.0}.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Maximum number of iterations during fit.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Tolerance on update at each iteration.</dd>
<dt>w_init <span class="classifier-delimiter">:</span> <span class="classifier">None of an (n_components, n_components) ndarray</span></dt>
<dd>The mixing matrix to be used to initialize the algorithm.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">components_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">2D array, shape (n_components, n_features)</span></dt>
<dd>The unmixing matrix.</dd>
<dt><code class="docutils literal"><span class="pre">mixing_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features, n_components)</span></dt>
<dd>The mixing matrix.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>If the algorithm is “deflation”, n_iter is the
maximum number of iterations run across all components. Else
they are just the number of iterations taken to converge.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">FastICA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span> <span class="o">=</span> <span class="n">FastICA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_transformed</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_transformed</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(1797, 7)</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>Implementation based on
<a href="#id18"><span class="problematic" id="id19">`</span></a>A. Hyvarinen and E. Oja, Independent Component Analysis:</p>
<p>Algorithms and Applications, Neural Networks, 13(4-5), 2000,
pp. 411-430`</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.FastICAScikitsLearnNode-class.html">FastICAScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.RandomForestRegressorScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">RandomForestRegressorScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.RandomForestRegressorScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>A random forest regressor.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.ensemble.forest.RandomForestRegressor</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
A random forest is a meta estimator that fits a number of classifying
decision trees on various sub-samples of the dataset and uses averaging
to improve the predictive accuracy and control over-fitting.
The sub-sample size is always the same as the original
input sample size but the samples are drawn with replacement if
<cite>bootstrap=True</cite> (default).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_estimators <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=10)</span></dt>
<dd><p class="first">The number of trees in the forest.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.20: </span>The default value of <code class="docutils literal"><span class="pre">n_estimators</span></code> will change from 10 in
version 0.20 to 100 in version 0.22.</p>
</div>
</dd>
<dt>criterion <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=”mse”)</span></dt>
<dd><p class="first">The function to measure the quality of a split. Supported criteria
are “mse” for the mean squared error, which is equal to variance
reduction as feature selection criterion, and “mae” for the mean
absolute error.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.18: </span>Mean Absolute Error (MAE) criterion.</p>
</div>
</dd>
<dt>max_depth <span class="classifier-delimiter">:</span> <span class="classifier">integer or None, optional (default=None)</span></dt>
<dd>The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</dd>
<dt>min_samples_split <span class="classifier-delimiter">:</span> <span class="classifier">int, float, optional (default=2)</span></dt>
<dd><p class="first">The minimum number of samples required to split an internal node:</p>
<ul class="simple">
<li>If int, then consider <cite>min_samples_split</cite> as the minimum number.</li>
<li>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</li>
</ul>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</dd>
<dt>min_samples_leaf <span class="classifier-delimiter">:</span> <span class="classifier">int, float, optional (default=1)</span></dt>
<dd><p class="first">The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul class="simple">
<li>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</li>
<li>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</li>
</ul>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</dd>
<dt>min_weight_fraction_leaf <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span></dt>
<dd>The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int, float, string or None, optional (default=”auto”)</span></dt>
<dd><p class="first">The number of features to consider when looking for the best split:</p>
<ul class="simple">
<li>If int, then consider <cite>max_features</cite> features at each split.</li>
<li>If float, then <cite>max_features</cite> is a fraction and
<cite>int(max_features * n_features)</cite> features are considered at each
split.</li>
<li>If “auto”, then <cite>max_features=n_features</cite>.</li>
<li>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite>.</li>
<li>If “log2”, then <cite>max_features=log2(n_features)</cite>.</li>
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
<p class="last">Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal"><span class="pre">max_features</span></code> features.</p>
</dd>
<dt>max_leaf_nodes <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Grow trees with <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</dd>
<dt>min_impurity_decrease <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span></dt>
<dd><p class="first">A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal"><span class="pre">N</span></code>, <code class="docutils literal"><span class="pre">N_t</span></code>, <code class="docutils literal"><span class="pre">N_t_R</span></code> and <code class="docutils literal"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19.</span></p>
</div>
</dd>
<dt>min_impurity_split <span class="classifier-delimiter">:</span> <span class="classifier">float, (default=1e-7)</span></dt>
<dd><p class="first">Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="last deprecated">
<p><span class="versionmodified">Deprecated since version 0.19: </span><code class="docutils literal"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal"><span class="pre">min_impurity_split</span></code> will change from 1e-7 to 0 in 0.23 and it
will be removed in 0.25. Use <code class="docutils literal"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</dd>
<dt>bootstrap <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span></dt>
<dd>Whether bootstrap samples are used when building trees. If False, the
whole datset is used to build each tree.</dd>
<dt>oob_score <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default=False)</span></dt>
<dd>whether to use out-of-bag samples to estimate
the R^2 on unseen data.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The number of jobs to run in parallel for both <cite>fit</cite> and <cite>predict</cite>.
<cite>None`</cite> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=0)</span></dt>
<dd>Controls the verbosity when fitting and predicting.</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default=False)</span></dt>
<dd>When set to <code class="docutils literal"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit a whole
new forest. See <span class="xref std std-term">the Glossary</span>.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">estimators_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list of DecisionTreeRegressor</span></dt>
<dd>The collection of fitted sub-estimators.</dd>
<dt><code class="docutils literal"><span class="pre">feature_importances_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span></dt>
<dd>The feature importances (the higher, the more important the feature).</dd>
<dt><code class="docutils literal"><span class="pre">n_features_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The number of features when <code class="docutils literal"><span class="pre">fit</span></code> is performed.</dd>
<dt><code class="docutils literal"><span class="pre">n_outputs_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The number of outputs when <code class="docutils literal"><span class="pre">fit</span></code> is performed.</dd>
<dt><code class="docutils literal"><span class="pre">oob_score_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Score of the training dataset obtained using an out-of-bag estimate.</dd>
<dt><code class="docutils literal"><span class="pre">oob_prediction_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_samples]</span></dt>
<dd>Prediction computed with out-of-bag estimate on the training set.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>                             <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=2,</span>
<span class="go">           max_features=&#39;auto&#39;, max_leaf_nodes=None,</span>
<span class="go">           min_impurity_decrease=0.0, min_impurity_split=None,</span>
<span class="go">           min_samples_leaf=1, min_samples_split=2,</span>
<span class="go">           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,</span>
<span class="go">           oob_score=False, random_state=0, verbose=0, warm_start=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">)</span>
<span class="go">[0.18146984 0.81473937 0.00145312 0.00233767]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>
<span class="go">[-8.32987858]</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code class="docutils literal"><span class="pre">max_depth</span></code>, <code class="docutils literal"><span class="pre">min_samples_leaf</span></code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<p>The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data,
<code class="docutils literal"><span class="pre">max_features=n_features</span></code> and <code class="docutils literal"><span class="pre">bootstrap=False</span></code>, if the improvement
of the criterion is identical for several splits enumerated during the
search of the best split. To obtain a deterministic behaviour during
fitting, <code class="docutils literal"><span class="pre">random_state</span></code> has to be fixed.</p>
<p>The default value <code class="docutils literal"><span class="pre">max_features=&quot;auto&quot;</span></code> uses <code class="docutils literal"><span class="pre">n_features</span></code>
rather than <code class="docutils literal"><span class="pre">n_features</span> <span class="pre">/</span> <span class="pre">3</span></code>. The latter was originally suggested in
[1], whereas the former was more recently justified empirically in [2].</p>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id20" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><ol class="first last upperalpha simple" start="12">
<li>Breiman, “Random Forests”, Machine Learning, 45(1), 5-32, 2001.</li>
</ol>
</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id21" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized
trees”, Machine Learning, 63(1), 3-42, 2006.</td></tr>
</tbody>
</table>
<p>See also</p>
<p>DecisionTreeRegressor, ExtraTreesRegressor</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#RandomForestRegressorScikitsLearnNode">RandomForestRegressorScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.MultinomialNBScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">MultinomialNBScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.MultinomialNBScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Naive Bayes classifier for multinomial models
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.naive_bayes.MultinomialNB</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
The multinomial Naive Bayes classifier is suitable for classification with
discrete features (e.g., word counts for text classification). The
multinomial distribution normally requires integer feature counts. However,
in practice, fractional counts such as tf-idf may also work.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span></dt>
<dd>Additive (Laplace/Lidstone) smoothing parameter
(0 for no smoothing).</dd>
<dt>fit_prior <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span></dt>
<dd>Whether to learn class prior probabilities or not.
If false, a uniform prior will be used.</dd>
<dt>class_prior <span class="classifier-delimiter">:</span> <span class="classifier">array-like, size (n_classes,), optional (default=None)</span></dt>
<dd>Prior probabilities of the classes. If specified the priors are not
adjusted according to the data.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">class_log_prior_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes, )</span></dt>
<dd>Smoothed empirical log probability for each class.</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes, )</span></dt>
<dd>Mirrors <code class="docutils literal"><span class="pre">class_log_prior_</span></code> for interpreting MultinomialNB
as a linear model.</dd>
<dt><code class="docutils literal"><span class="pre">feature_log_prob_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes, n_features)</span></dt>
<dd>Empirical log probability of features
given a class, <code class="docutils literal"><span class="pre">P(x_i|y)</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes, n_features)</span></dt>
<dd>Mirrors <code class="docutils literal"><span class="pre">feature_log_prob_</span></code> for interpreting MultinomialNB
as a linear model.</dd>
<dt><code class="docutils literal"><span class="pre">class_count_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes,)</span></dt>
<dd>Number of samples encountered for each class during fitting. This
value is weighted by the sample weight when provided.</dd>
<dt><code class="docutils literal"><span class="pre">feature_count_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes, n_features)</span></dt>
<dd>Number of samples encountered for each (class, feature)
during fitting. This value is weighted by the sample weight when
provided.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">3</span><span class="p">]))</span>
<span class="go">[3]</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>For the rationale behind the names <cite>coef_</cite> and <cite>intercept_</cite>, i.e.
naive Bayes as a linear classifier, see J. Rennie et al. (2003),
Tackling the poor assumptions of naive Bayes text classifiers, ICML.</p>
<p><strong>References</strong></p>
<p>C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to
Information Retrieval. Cambridge University Press, pp. 234-265.
<a class="reference external" href="http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html">http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html</a></p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#MultinomialNBScikitsLearnNode">MultinomialNBScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.LabelEncoderScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">LabelEncoderScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.LabelEncoderScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Encode labels with value between 0 and n_classes-1.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.preprocessing.label.LabelEncoder</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">classes_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_class,)</span></dt>
<dd>Holds the label for each class.</dd>
</dl>
<p><strong>Examples</strong></p>
<p><cite>LabelEncoder</cite> can be used to normalize labels.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">LabelEncoder</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="go">LabelEncoder()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">classes_</span>
<span class="go">array([1, 2, 6])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span> 
<span class="go">array([0, 0, 1, 2]...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="go">array([1, 1, 2, 6])</span>
</pre></div>
</div>
<p>It can also be used to transform non-numerical labels (as long as they are
hashable and comparable) to numerical labels.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">le</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">LabelEncoder</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="s2">&quot;paris&quot;</span><span class="p">,</span> <span class="s2">&quot;paris&quot;</span><span class="p">,</span> <span class="s2">&quot;tokyo&quot;</span><span class="p">,</span> <span class="s2">&quot;amsterdam&quot;</span><span class="p">])</span>
<span class="go">LabelEncoder()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">le</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="go">[&#39;amsterdam&#39;, &#39;paris&#39;, &#39;tokyo&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="s2">&quot;tokyo&quot;</span><span class="p">,</span> <span class="s2">&quot;tokyo&quot;</span><span class="p">,</span> <span class="s2">&quot;paris&quot;</span><span class="p">])</span> 
<span class="go">array([2, 2, 1]...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">le</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="go">[&#39;tokyo&#39;, &#39;tokyo&#39;, &#39;paris&#39;]</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>sklearn.preprocessing.OrdinalEncoder <span class="classifier-delimiter">:</span> <span class="classifier">encode categorical features</span></dt>
<dd>using a one-hot or ordinal encoding scheme.</dd>
</dl>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#LabelEncoderScikitsLearnNode">LabelEncoderScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.LocallyLinearEmbeddingScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">LocallyLinearEmbeddingScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.LocallyLinearEmbeddingScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Locally Linear Embedding
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.manifold.locally_linear.LocallyLinearEmbedding</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_neighbors <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd>number of neighbors to consider for each point.</dd>
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd>number of coordinates for the manifold</dd>
<dt>reg <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>regularization constant, multiplies the trace of the local covariance
matrix of the distances.</dd>
<dt>eigen_solver <span class="classifier-delimiter">:</span> <span class="classifier">string, {‘auto’, ‘arpack’, ‘dense’}</span></dt>
<dd><p class="first">auto : algorithm will attempt to choose the best method for input data</p>
<dl class="last docutils">
<dt>arpack <span class="classifier-delimiter">:</span> <span class="classifier">use arnoldi iteration in shift-invert mode.</span></dt>
<dd>For this method, M may be a dense matrix, sparse matrix,
or general linear operator.
Warning: ARPACK can be unstable for some problems.  It is
best to try several random seeds in order to check results.</dd>
<dt>dense <span class="classifier-delimiter">:</span> <span class="classifier">use standard dense matrix operations for the eigenvalue</span></dt>
<dd>decomposition.  For this method, M must be an array
or matrix type.  This method should be avoided for
large problems.</dd>
</dl>
</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Tolerance for ‘arpack’ method
Not used if eigen_solver==’dense’.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd>maximum number of iterations for the arpack solver.
Not used if eigen_solver==’dense’.</dd>
<dt>method <span class="classifier-delimiter">:</span> <span class="classifier">string (‘standard’, ‘hessian’, ‘modified’ or ‘ltsa’)</span></dt>
<dd><dl class="first last docutils">
<dt>standard <span class="classifier-delimiter">:</span> <span class="classifier">use the standard locally linear embedding algorithm.  see</span></dt>
<dd>reference [1]</dd>
<dt>hessian <span class="classifier-delimiter">:</span> <span class="classifier">use the Hessian eigenmap method. This method requires</span></dt>
<dd><code class="docutils literal"><span class="pre">n_neighbors</span> <span class="pre">&gt;</span> <span class="pre">n_components</span> <span class="pre">*</span> <span class="pre">(1</span> <span class="pre">+</span> <span class="pre">(n_components</span> <span class="pre">+</span> <span class="pre">1)</span> <span class="pre">/</span> <span class="pre">2</span></code>
see reference [2]</dd>
<dt>modified <span class="classifier-delimiter">:</span> <span class="classifier">use the modified locally linear embedding algorithm.</span></dt>
<dd>see reference [3]</dd>
<dt>ltsa <span class="classifier-delimiter">:</span> <span class="classifier">use local tangent space alignment algorithm</span></dt>
<dd>see reference [4]</dd>
</dl>
</dd>
<dt>hessian_tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Tolerance for Hessian eigenmapping method.
Only used if <code class="docutils literal"><span class="pre">method</span> <span class="pre">==</span> <span class="pre">'hessian'</span></code></dd>
<dt>modified_tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Tolerance for modified LLE method.
Only used if <code class="docutils literal"><span class="pre">method</span> <span class="pre">==</span> <span class="pre">'modified'</span></code></dd>
<dt>neighbors_algorithm <span class="classifier-delimiter">:</span> <span class="classifier">string [‘auto’|’brute’|’kd_tree’|’ball_tree’]</span></dt>
<dd>algorithm to use for nearest neighbors search,
passed to neighbors.NearestNeighbors instance</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>. Used when <code class="docutils literal"><span class="pre">eigen_solver</span></code> == ‘arpack’.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The number of parallel jobs to run.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">embedding_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape [n_samples, n_components]</span></dt>
<dd>Stores the embedding vectors</dd>
<dt><code class="docutils literal"><span class="pre">reconstruction_error_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Reconstruction error associated with <cite>embedding_</cite></dd>
<dt><code class="docutils literal"><span class="pre">nbrs_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">NearestNeighbors object</span></dt>
<dd>Stores nearest neighbors instance, including BallTree or KDtree
if applicable.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">LocallyLinearEmbedding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(1797, 64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span> <span class="o">=</span> <span class="n">LocallyLinearEmbedding</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_transformed</span> <span class="o">=</span> <span class="n">embedding</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">100</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_transformed</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(100, 2)</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id22" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><cite>Roweis, S. &amp; Saul, L. Nonlinear dimensionality reduction
by locally linear embedding.  Science 290:2323 (2000).</cite></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id23" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><cite>Donoho, D. &amp; Grimes, C. Hessian eigenmaps: Locally
linear embedding techniques for high-dimensional data.
Proc Natl Acad Sci U S A.  100:5591 (2003).</cite></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id24" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td><cite>Zhang, Z. &amp; Wang, J. MLLE: Modified Locally Linear
Embedding Using Multiple Weights.</cite>
<a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id25" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td><cite>Zhang, Z. &amp; Zha, H. Principal manifolds and nonlinear
dimensionality reduction via tangent space alignment.
Journal of Shanghai Univ.  8:406 (2004)</cite></td></tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#LocallyLinearEmbeddingScikitsLearnNode">LocallyLinearEmbeddingScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.AdaBoostClassifierScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">AdaBoostClassifierScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.AdaBoostClassifierScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>An AdaBoost classifier.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.ensemble.weight_boosting.AdaBoostClassifier</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
An AdaBoost [1] classifier is a meta-estimator that begins by fitting a
classifier on the original dataset and then fits additional copies of the
classifier on the same dataset but where the weights of incorrectly
classified instances are adjusted such that subsequent classifiers focus
more on difficult cases.</p>
<p>This class implements the algorithm known as AdaBoost-SAMME [2].</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>base_estimator <span class="classifier-delimiter">:</span> <span class="classifier">object, optional (default=None)</span></dt>
<dd>The base estimator from which the boosted ensemble is built.
Support for sample weighting is required, as well as proper
<code class="docutils literal"><span class="pre">classes_</span></code> and <code class="docutils literal"><span class="pre">n_classes_</span></code> attributes. If <code class="docutils literal"><span class="pre">None</span></code>, then
the base estimator is <code class="docutils literal"><span class="pre">DecisionTreeClassifier(max_depth=1)</span></code></dd>
<dt>n_estimators <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=50)</span></dt>
<dd>The maximum number of estimators at which boosting is terminated.
In case of perfect fit, the learning procedure is stopped early.</dd>
<dt>learning_rate <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.)</span></dt>
<dd>Learning rate shrinks the contribution of each classifier by
<code class="docutils literal"><span class="pre">learning_rate</span></code>. There is a trade-off between <code class="docutils literal"><span class="pre">learning_rate</span></code> and
<code class="docutils literal"><span class="pre">n_estimators</span></code>.</dd>
<dt>algorithm <span class="classifier-delimiter">:</span> <span class="classifier">{‘SAMME’, ‘SAMME.R’}, optional (default=’SAMME.R’)</span></dt>
<dd>If ‘SAMME.R’ then use the SAMME.R real boosting algorithm.
<code class="docutils literal"><span class="pre">base_estimator</span></code> must support calculation of class probabilities.
If ‘SAMME’ then use the SAMME discrete boosting algorithm.
The SAMME.R algorithm typically converges faster than SAMME,
achieving a lower test error with fewer boosting iterations.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">estimators_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list of classifiers</span></dt>
<dd>The collection of fitted sub-estimators.</dd>
<dt><code class="docutils literal"><span class="pre">classes_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_classes]</span></dt>
<dd>The classes labels.</dd>
<dt><code class="docutils literal"><span class="pre">n_classes_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The number of classes.</dd>
<dt><code class="docutils literal"><span class="pre">estimator_weights_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of floats</span></dt>
<dd>Weights for each estimator in the boosted ensemble.</dd>
<dt><code class="docutils literal"><span class="pre">estimator_errors_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of floats</span></dt>
<dd>Classification error for each estimator in the boosted
ensemble.</dd>
<dt><code class="docutils literal"><span class="pre">feature_importances_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span></dt>
<dd>The feature importances if supported by the <code class="docutils literal"><span class="pre">base_estimator</span></code>.</dd>
</dl>
<p>See also</p>
<p>AdaBoostRegressor, GradientBoostingClassifier,
sklearn.tree.DecisionTreeClassifier</p>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id26" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Y. Freund, R. Schapire, “A Decision-Theoretic Generalization of
on-Line Learning and an Application to Boosting”, 1995.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id27" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><ol class="first last upperalpha simple" start="10">
<li>Zhu, H. Zou, S. Rosset, T. Hastie, “Multi-class AdaBoost”, 2009.</li>
</ol>
</td></tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#AdaBoostClassifierScikitsLearnNode">AdaBoostClassifierScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.GaussianProcessClassifierScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">GaussianProcessClassifierScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.GaussianProcessClassifierScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Gaussian process classification (GPC) based on Laplace approximation.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.gaussian_process.gpc.GaussianProcessClassifier</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
The implementation is based on Algorithm 3.1, 3.2, and 5.1 of
Gaussian Processes for Machine Learning (GPML) by Rasmussen and
Williams.</p>
<p>Internally, the Laplace approximation is used for approximating the
non-Gaussian posterior by a Gaussian.</p>
<p>Currently, the implementation is restricted to using the logistic link
function. For multi-class classification, several binary one-versus rest
classifiers are fitted. Note that this class thus does not implement
a true multi-class Laplace approximation.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>kernel <span class="classifier-delimiter">:</span> <span class="classifier">kernel object</span></dt>
<dd>The kernel specifying the covariance function of the GP. If None is
passed, the kernel “1.0 * RBF(1.0)” is used as default. Note that
the kernel’s hyperparameters are optimized during fitting.</dd>
<dt>optimizer <span class="classifier-delimiter">:</span> <span class="classifier">string or callable, optional (default: “fmin_l_bfgs_b”)</span></dt>
<dd><p class="first">Can either be one of the internally supported optimizers for optimizing
the kernel’s parameters, specified by a string, or an externally
defined optimizer passed as a callable. If a callable is passed, it
must have the  signature:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">optimizer</span><span class="p">(</span><span class="n">obj_func</span><span class="p">,</span> <span class="n">initial_theta</span><span class="p">,</span> <span class="n">bounds</span><span class="p">):</span>

    <span class="o">-</span> <span class="c1"># * &#39;obj_func&#39; is the objective function to be maximized, which</span>
    <span class="o">-</span> <span class="c1">#   takes the hyperparameters theta as parameter and an</span>
    <span class="o">-</span> <span class="c1">#   optional flag eval_gradient, which determines if the</span>
    <span class="o">-</span> <span class="c1">#   gradient is returned additionally to the function value</span>
    <span class="o">-</span> <span class="c1"># * &#39;initial_theta&#39;: the initial value for theta, which can be</span>
    <span class="o">-</span> <span class="c1">#   used by local optimizers</span>
    <span class="o">-</span> <span class="c1"># * &#39;bounds&#39;: the bounds on the values of theta</span>
    <span class="o">-</span> <span class="o">....</span>
    <span class="o">-</span> <span class="c1"># Returned are the best found hyperparameters theta and</span>
    <span class="o">-</span> <span class="c1"># the corresponding value of the target function.</span>
    <span class="o">-</span> <span class="k">return</span> <span class="n">theta_opt</span><span class="p">,</span> <span class="n">func_min</span>
</pre></div>
</div>
<p>Per default, the ‘fmin_l_bfgs_b’ algorithm from scipy.optimize
is used. If None is passed, the kernel’s parameters are kept fixed.
Available internal optimizers are:</p>
<div class="last highlight-default"><div class="highlight"><pre><span></span><span class="s1">&#39;fmin_l_bfgs_b&#39;</span>
</pre></div>
</div>
</dd>
<dt>n_restarts_optimizer <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default: 0)</span></dt>
<dd>The number of restarts of the optimizer for finding the kernel’s
parameters which maximize the log-marginal likelihood. The first run
of the optimizer is performed from the kernel’s initial parameters,
the remaining ones (if any) from thetas sampled log-uniform randomly
from the space of allowed theta-values. If greater than 0, all bounds
must be finite. Note that n_restarts_optimizer=0 implies that one
run is performed.</dd>
<dt>max_iter_predict <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default: 100)</span></dt>
<dd>The maximum number of iterations in Newton’s method for approximating
the posterior during predict. Smaller values will reduce computation
time at the cost of worse results.</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default: False)</span></dt>
<dd>If warm-starts are enabled, the solution of the last Newton iteration
on the Laplace approximation of the posterior mode is used as
initialization for the next call of _posterior_mode(). This can speed
up convergence when _posterior_mode is called several times on similar
problems as in hyperparameter optimization. See <span class="xref std std-term">the Glossary</span>.</dd>
<dt>copy_X_train <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default: True)</span></dt>
<dd>If True, a persistent copy of the training data is stored in the
object. Otherwise, just a reference to the training data is stored,
which might cause predictions to change if the data is modified
externally.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default: None)</span></dt>
<dd>The generator used to initialize the centers.
If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>multi_class <span class="classifier-delimiter">:</span> <span class="classifier">string, default</span> <span class="classifier-delimiter">:</span> <span class="classifier">“one_vs_rest”</span></dt>
<dd>Specifies how multi-class classification problems are handled.
Supported are “one_vs_rest” and “one_vs_one”. In “one_vs_rest”,
one binary Gaussian process classifier is fitted for each class, which
is trained to separate this class from the rest. In “one_vs_one”, one
binary Gaussian process classifier is fitted for each pair of classes,
which is trained to separate these two classes. The predictions of
these binary predictors are combined into multi-class predictions.
Note that “one_vs_one” does not support predicting probability
estimates.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The number of jobs to use for the computation.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">kernel_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">kernel object</span></dt>
<dd>The kernel used for prediction. In case of binary classification,
the structure of the kernel is the same as the one passed as parameter
but with optimized hyperparameters. In case of multi-class
classification, a CompoundKernel is returned which consists of the
different kernels used in the one-versus-rest classifiers.</dd>
<dt><code class="docutils literal"><span class="pre">log_marginal_likelihood_value_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The log-marginal-likelihood of <code class="docutils literal"><span class="pre">self.kernel_.theta</span></code></dd>
<dt><code class="docutils literal"><span class="pre">classes_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = (n_classes,)</span></dt>
<dd>Unique class labels.</dd>
<dt><code class="docutils literal"><span class="pre">n_classes_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The number of classes in the training data</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.gaussian_process</span> <span class="kn">import</span> <span class="n">GaussianProcessClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.gaussian_process.kernels</span> <span class="kn">import</span> <span class="n">RBF</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kernel</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="n">RBF</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gpc</span> <span class="o">=</span> <span class="n">GaussianProcessClassifier</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gpc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">0.9866...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gpc</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">2</span><span class="p">,:])</span>
<span class="go">array([[0.83548752, 0.03228706, 0.13222543],</span>
<span class="go">       [0.79064206, 0.06525643, 0.14410151]])</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.18.</span></p>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#GaussianProcessClassifierScikitsLearnNode">GaussianProcessClassifierScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.LarsCVScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">LarsCVScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.LarsCVScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Cross-validated Least Angle Regression model.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.least_angle.LarsCV</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
See glossary entry for <span class="xref std std-term">cross-validation estimator</span>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">boolean or integer, optional</span></dt>
<dd>Sets the verbosity amount</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span></dt>
<dd>Maximum number of iterations to perform.</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>This parameter is ignored when <code class="docutils literal"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<code class="xref py py-class docutils literal"><span class="pre">sklearn.preprocessing.StandardScaler</span></code> before calling <code class="docutils literal"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal"><span class="pre">normalize=False</span></code>.</dd>
<dt>precompute <span class="classifier-delimiter">:</span> <span class="classifier">True | False | ‘auto’ | array-like</span></dt>
<dd>Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal"><span class="pre">'auto'</span></code> let us decide. The Gram matrix
cannot be passed as argument since we will use only subsets of X.</dd>
<dt>cv <span class="classifier-delimiter">:</span> <span class="classifier">int, cross-validation generator or an iterable, optional</span></dt>
<dd><p class="first">Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul class="simple">
<li>None, to use the default 3-fold cross-validation,</li>
<li>integer, to specify the number of folds.</li>
<li><span class="xref std std-term">CV splitter</span>,</li>
<li>An iterable yielding (train, test) splits as arrays of indices.</li>
</ul>
<p>For integer/None inputs, <code class="xref py py-class docutils literal"><span class="pre">KFold</span></code> is used.</p>
<p>Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.20: </span><code class="docutils literal"><span class="pre">cv</span></code> default value if None will change from 3-fold to 5-fold
in v0.22.</p>
</div>
</dd>
<dt>max_n_alphas <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span></dt>
<dd>The maximum number of points on the path used to compute the
residuals in the cross-validation</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Number of CPUs to use during the cross validation.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>eps <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The machine-precision regularization in the computation of the
Cholesky diagonal factors. Increase this for very ill-conditioned
systems.</dd>
<dt>copy_X <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>If <code class="docutils literal"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</dd>
<dt>positive <span class="classifier-delimiter">:</span> <span class="classifier">boolean (default=False)</span></dt>
<dd><p class="first">Restrict coefficients to be &gt;= 0. Be aware that you might want to
remove fit_intercept which is set True by default.</p>
<div class="last deprecated">
<p><span class="versionmodified">Deprecated since version 0.20: </span>The option is broken and deprecated. It will be removed in v0.22.</p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,)</span></dt>
<dd>parameter vector (w in the formulation formula)</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>independent term in decision function</dd>
<dt><code class="docutils literal"><span class="pre">coef_path_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features, n_alphas)</span></dt>
<dd>the varying values of the coefficients along the path</dd>
<dt><code class="docutils literal"><span class="pre">alpha_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>the estimated regularization parameter alpha</dd>
<dt><code class="docutils literal"><span class="pre">alphas_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_alphas,)</span></dt>
<dd>the different values of alpha along the path</dd>
<dt><code class="docutils literal"><span class="pre">cv_alphas_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_cv_alphas,)</span></dt>
<dd>all the values of alpha along the path for the different folds</dd>
<dt><code class="docutils literal"><span class="pre">mse_path_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_folds, n_cv_alphas)</span></dt>
<dd>the mean square error on left-out for each fold along the path
(alpha values given by <code class="docutils literal"><span class="pre">cv_alphas</span></code>)</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like or int</span></dt>
<dd>the number of iterations run by Lars with the optimal alpha.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LarsCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">LarsCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">0.9996...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">alpha_</span>
<span class="go">0.0254...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">1</span><span class="p">,])</span>
<span class="go">array([154.0842...])</span>
</pre></div>
</div>
<p>See also</p>
<p>lars_path, LassoLars, LassoLarsCV</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#LarsCVScikitsLearnNode">LarsCVScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.AdditiveChi2SamplerScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">AdditiveChi2SamplerScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.AdditiveChi2SamplerScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Approximate feature map for additive chi2 kernel.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.kernel_approximation.AdditiveChi2Sampler</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Uses sampling the fourier transform of the kernel characteristic
at regular intervals.</p>
<p>Since the kernel that is to be approximated is additive, the components of
the input vectors can be treated separately.  Each entry in the original
space is transformed into 2*sample_steps+1 features, where sample_steps is
a parameter of the method. Typical values of sample_steps include 1, 2 and
3.</p>
<p>Optimal choices for the sampling interval for certain data ranges can be
computed (see the reference). The default values should be reasonable.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>sample_steps <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Gives the number of (complex) sampling points.</dd>
<dt>sample_interval <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Sampling interval. Must be specified when sample_steps not in {1,2,3}.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.kernel_approximation</span> <span class="kn">import</span> <span class="n">AdditiveChi2Sampler</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">chi2sampler</span> <span class="o">=</span> <span class="n">AdditiveChi2Sampler</span><span class="p">(</span><span class="n">sample_steps</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_transformed</span> <span class="o">=</span> <span class="n">chi2sampler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_transformed</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">SGDClassifier(alpha=0.0001, average=False, class_weight=None,</span>
<span class="go">       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,</span>
<span class="go">       l1_ratio=0.15, learning_rate=&#39;optimal&#39;, loss=&#39;hinge&#39;, max_iter=5,</span>
<span class="go">       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty=&#39;l2&#39;,</span>
<span class="go">       power_t=0.5, random_state=0, shuffle=True, tol=0.001,</span>
<span class="go">       validation_fraction=0.1, verbose=0, warm_start=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_transformed</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">0.9543...</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>This estimator approximates a slightly different version of the additive
chi squared kernel then <code class="docutils literal"><span class="pre">metric.additive_chi2</span></code> computes.</p>
<p>See also</p>
<dl class="docutils">
<dt>SkewedChi2Sampler <span class="classifier-delimiter">:</span> <span class="classifier">A Fourier-approximation to a non-additive variant of</span></dt>
<dd>the chi squared kernel.</dd>
</dl>
<p>sklearn.metrics.pairwise.chi2_kernel : The exact chi squared kernel.</p>
<dl class="docutils">
<dt>sklearn.metrics.pairwise.additive_chi2_kernel <span class="classifier-delimiter">:</span> <span class="classifier">The exact additive chi</span></dt>
<dd>squared kernel.</dd>
</dl>
<p><strong>References</strong></p>
<p>See <a class="reference external" href="http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/vedaldi11efficient.pdf">“Efficient additive kernels via explicit feature maps”</a>
A. Vedaldi and A. Zisserman, Pattern Analysis and Machine Intelligence,
2011</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#AdditiveChi2SamplerScikitsLearnNode">AdditiveChi2SamplerScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.QuantileEstimatorScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">QuantileEstimatorScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.QuantileEstimatorScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>An estimator predicting the alpha-quantile of the training targets.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.ensemble.gradient_boosting.QuantileEstimator</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
<strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The quantile</dd>
</dl>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#QuantileEstimatorScikitsLearnNode">QuantileEstimatorScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.BirchScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">BirchScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.BirchScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the Birch clustering algorithm.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.cluster.birch.Birch</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
It is a memory-efficient, online-learning algorithm provided as an
alternative to <code class="xref py py-class docutils literal"><span class="pre">MiniBatchKMeans</span></code>. It constructs a tree
data structure with the cluster centroids being read off the leaf.
These can be either the final cluster centroids or can be provided as input
to another clustering algorithm such as <code class="xref py py-class docutils literal"><span class="pre">AgglomerativeClustering</span></code>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>threshold <span class="classifier-delimiter">:</span> <span class="classifier">float, default 0.5</span></dt>
<dd>The radius of the subcluster obtained by merging a new sample and the
closest subcluster should be lesser than the threshold. Otherwise a new
subcluster is started. Setting this value to be very low promotes
splitting and vice-versa.</dd>
<dt>branching_factor <span class="classifier-delimiter">:</span> <span class="classifier">int, default 50</span></dt>
<dd>Maximum number of CF subclusters in each node. If a new samples enters
such that the number of subclusters exceed the branching_factor then
that node is split into two nodes with the subclusters redistributed
in each. The parent subcluster of that node is removed and two new
subclusters are added as parents of the 2 split nodes.</dd>
<dt>n_clusters <span class="classifier-delimiter">:</span> <span class="classifier">int, instance of sklearn.cluster model, default 3</span></dt>
<dd><p class="first">Number of clusters after the final clustering step, which treats the
subclusters from the leaves as new samples.</p>
<ul class="last simple">
<li><cite>None</cite> : the final clustering step is not performed and the
subclusters are returned as they are.</li>
<li><cite>sklearn.cluster</cite> Estimator : If a model is provided, the model is
fit treating the subclusters as new samples and the initial data is
mapped to the label of the closest subcluster.</li>
<li><cite>int</cite> : the model fit is <code class="xref py py-class docutils literal"><span class="pre">AgglomerativeClustering</span></code> with
<cite>n_clusters</cite> set to be equal to the int.</li>
</ul>
</dd>
<dt>compute_labels <span class="classifier-delimiter">:</span> <span class="classifier">bool, default True</span></dt>
<dd>Whether or not to compute labels for each fit.</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">bool, default True</span></dt>
<dd>Whether or not to make a copy of the given data. If set to False,
the initial data will be overwritten.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">root_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">_CFNode</span></dt>
<dd>Root of the CFTree.</dd>
<dt><code class="docutils literal"><span class="pre">dummy_leaf_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">_CFNode</span></dt>
<dd>Start pointer to all the leaves.</dd>
<dt><code class="docutils literal"><span class="pre">subcluster_centers_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">ndarray,</span></dt>
<dd>Centroids of all subclusters read directly from the leaves.</dd>
<dt><code class="docutils literal"><span class="pre">subcluster_labels_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">ndarray,</span></dt>
<dd>Labels assigned to the centroids of the subclusters after
they are clustered globally.</dd>
<dt><code class="docutils literal"><span class="pre">labels_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">ndarray, shape (n_samples,)</span></dt>
<dd>Array of labels assigned to the input data.
if partial_fit is used instead of fit, they are assigned to the
last batch of data.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">Birch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">brc</span> <span class="o">=</span> <span class="n">Birch</span><span class="p">(</span><span class="n">branching_factor</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">n_clusters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
<span class="gp">... </span><span class="n">compute_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">brc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> 
<span class="go">Birch(branching_factor=50, compute_labels=True, copy=True, n_clusters=None,</span>
<span class="go">   threshold=0.5)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">brc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([0, 0, 0, 1, 1, 1])</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<ul class="simple">
<li>Tian Zhang, Raghu Ramakrishnan, Maron Livny
BIRCH: An efficient data clustering method for large databases.
<a class="reference external" href="http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf">http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf</a></li>
<li>Roberto Perdisci
JBirch - Java implementation of BIRCH clustering algorithm
<a class="reference external" href="https://code.google.com/archive/p/jbirch">https://code.google.com/archive/p/jbirch</a></li>
</ul>
<p><strong>Notes</strong></p>
<p>The tree data structure consists of nodes with each node consisting of
a number of subclusters. The maximum number of subclusters in a node
is determined by the branching factor. Each subcluster maintains a
linear sum, squared sum and the number of samples in that subcluster.
In addition, each subcluster can also have a node as its child, if the
subcluster is not a member of a leaf node.</p>
<p>For a new point entering the root, it is merged with the subcluster closest
to it and the linear sum, squared sum and the number of samples of that
subcluster are updated. This is done recursively till the properties of
the leaf node are updated.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#BirchScikitsLearnNode">BirchScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.QuantileTransformerScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">QuantileTransformerScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.QuantileTransformerScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform features using quantiles information.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.preprocessing.data.QuantileTransformer</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
This method transforms the features to follow a uniform or a normal
distribution. Therefore, for a given feature, this transformation tends
to spread out the most frequent values. It also reduces the impact of
(marginal) outliers: this is therefore a robust preprocessing scheme.</p>
<p>The transformation is applied on each feature independently.
The cumulative distribution function of a feature is used to project the
original values. Features values of new/unseen data that fall below
or above the fitted range will be mapped to the bounds of the output
distribution. Note that this transform is non-linear. It may distort linear
correlations between variables measured at the same scale but renders
variables measured at different scales more directly comparable.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_quantiles <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=1000)</span></dt>
<dd>Number of quantiles to be computed. It corresponds to the number
of landmarks used to discretize the cumulative distribution function.</dd>
<dt>output_distribution <span class="classifier-delimiter">:</span> <span class="classifier">str, optional (default=’uniform’)</span></dt>
<dd>Marginal distribution for the transformed data. The choices are
‘uniform’ (default) or ‘normal’.</dd>
<dt>ignore_implicit_zeros <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default=False)</span></dt>
<dd>Only applies to sparse matrices. If True, the sparse entries of the
matrix are discarded to compute the quantile statistics. If False,
these entries are treated as zeros.</dd>
<dt>subsample <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=1e5)</span></dt>
<dd>Maximum number of samples used to estimate the quantiles for
computational efficiency. Note that the subsampling procedure may
differ for value-identical sparse and dense matrices.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by np.random. Note that this is used by subsampling and smoothing
noise.</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, (default=True)</span></dt>
<dd>Set to False to perform inplace transformation and avoid a copy (if the
input is already a numpy array).</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">quantiles_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">ndarray, shape (n_quantiles, n_features)</span></dt>
<dd>The values corresponding the quantiles of reference.</dd>
<dt><code class="docutils literal"><span class="pre">references_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">ndarray, shape(n_quantiles, )</span></dt>
<dd>Quantiles of references.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">QuantileTransformer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qt</span> <span class="o">=</span> <span class="n">QuantileTransformer</span><span class="p">(</span><span class="n">n_quantiles</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qt</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> 
<span class="go">array([...])</span>
</pre></div>
</div>
<p>See also</p>
<p>quantile_transform : Equivalent function without the estimator API.
PowerTransformer : Perform mapping to a normal distribution using a power</p>
<blockquote>
<div>transform.</div></blockquote>
<dl class="docutils">
<dt>StandardScaler <span class="classifier-delimiter">:</span> <span class="classifier">Perform standardization that is faster, but less robust</span></dt>
<dd>to outliers.</dd>
<dt>RobustScaler <span class="classifier-delimiter">:</span> <span class="classifier">Perform robust standardization that removes the influence</span></dt>
<dd>of outliers but does not put outliers and inliers on the same scale.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>NaNs are treated as missing values: disregarded in fit, and maintained in
transform.</p>
<p>For a comparison of the different scalers, transformers, and normalizers,
see <span class="xref std std-ref">examples/preprocessing/plot_all_scaling.py</span>.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#QuantileTransformerScikitsLearnNode">QuantileTransformerScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.CountVectorizerScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">CountVectorizerScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.CountVectorizerScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert a collection of text documents to a matrix of token counts
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.feature_extraction.text.CountVectorizer</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
This implementation produces a sparse representation of the counts using
scipy.sparse.csr_matrix.</p>
<p>If you do not provide an a-priori dictionary and you do not use an analyzer
that does some kind of feature selection then the number of features will
be equal to the vocabulary size found by analyzing the data.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>input <span class="classifier-delimiter">:</span> <span class="classifier">string {‘filename’, ‘file’, ‘content’}</span></dt>
<dd><p class="first">If ‘filename’, the sequence passed as an argument to fit is
expected to be a list of filenames that need reading to fetch
the raw content to analyze.</p>
<p>If ‘file’, the sequence items must have a ‘read’ method (file-like
object) that is called to fetch the bytes in memory.</p>
<p class="last">Otherwise the input is expected to be the sequence strings or
bytes items are expected to be analyzed directly.</p>
</dd>
<dt>encoding <span class="classifier-delimiter">:</span> <span class="classifier">string, ‘utf-8’ by default.</span></dt>
<dd>If bytes or files are given to analyze, this encoding is used to
decode.</dd>
<dt>decode_error <span class="classifier-delimiter">:</span> <span class="classifier">{‘strict’, ‘ignore’, ‘replace’}</span></dt>
<dd>Instruction on what to do if a byte sequence is given to analyze that
contains characters not of the given <cite>encoding</cite>. By default, it is
‘strict’, meaning that a UnicodeDecodeError will be raised. Other
values are ‘ignore’ and ‘replace’.</dd>
<dt>strip_accents <span class="classifier-delimiter">:</span> <span class="classifier">{‘ascii’, ‘unicode’, None}</span></dt>
<dd><p class="first">Remove accents and perform other character normalization
during the preprocessing step.
‘ascii’ is a fast method that only works on characters that have
an direct ASCII mapping.
‘unicode’ is a slightly slower method that works on any characters.
None (default) does nothing.</p>
<p class="last">Both ‘ascii’ and ‘unicode’ use NFKD normalization from
<code class="xref py py-func docutils literal"><span class="pre">unicodedata.normalize()</span></code>.</p>
</dd>
<dt>lowercase <span class="classifier-delimiter">:</span> <span class="classifier">boolean, True by default</span></dt>
<dd>Convert all characters to lowercase before tokenizing.</dd>
<dt>preprocessor <span class="classifier-delimiter">:</span> <span class="classifier">callable or None (default)</span></dt>
<dd>Override the preprocessing (string transformation) stage while
preserving the tokenizing and n-grams generation steps.</dd>
<dt>tokenizer <span class="classifier-delimiter">:</span> <span class="classifier">callable or None (default)</span></dt>
<dd>Override the string tokenization step while preserving the
preprocessing and n-grams generation steps.
Only applies if <code class="docutils literal"><span class="pre">analyzer</span> <span class="pre">==</span> <span class="pre">'word'</span></code>.</dd>
<dt>stop_words <span class="classifier-delimiter">:</span> <span class="classifier">string {‘english’}, list, or None (default)</span></dt>
<dd><p class="first">If ‘english’, a built-in stop word list for English is used.
There are several known issues with ‘english’ and you should
consider an alternative (see <span class="xref std std-ref">stop_words</span>).</p>
<p>If a list, that list is assumed to contain stop words, all of which
will be removed from the resulting tokens.
Only applies if <code class="docutils literal"><span class="pre">analyzer</span> <span class="pre">==</span> <span class="pre">'word'</span></code>.</p>
<p class="last">If None, no stop words will be used. max_df can be set to a value
in the range [0.7, 1.0) to automatically detect and filter stop
words based on intra corpus document frequency of terms.</p>
</dd>
<dt>token_pattern <span class="classifier-delimiter">:</span> <span class="classifier">string</span></dt>
<dd>Regular expression denoting what constitutes a “token”, only used
if <code class="docutils literal"><span class="pre">analyzer</span> <span class="pre">==</span> <span class="pre">'word'</span></code>. The default regexp select tokens of 2
or more alphanumeric characters (punctuation is completely ignored
and always treated as a token separator).</dd>
<dt>ngram_range <span class="classifier-delimiter">:</span> <span class="classifier">tuple (min_n, max_n)</span></dt>
<dd>The lower and upper boundary of the range of n-values for different
n-grams to be extracted. All values of n such that min_n &lt;= n &lt;= max_n
will be used.</dd>
<dt>analyzer <span class="classifier-delimiter">:</span> <span class="classifier">string, {‘word’, ‘char’, ‘char_wb’} or callable</span></dt>
<dd><p class="first">Whether the feature should be made of word or character n-grams.
Option ‘char_wb’ creates character n-grams only from text inside
word boundaries; n-grams at the edges of words are padded with space.</p>
<p class="last">If a callable is passed it is used to extract the sequence of features
out of the raw, unprocessed input.</p>
</dd>
<dt>max_df <span class="classifier-delimiter">:</span> <span class="classifier">float in range [0.0, 1.0] or int, default=1.0</span></dt>
<dd>When building the vocabulary ignore terms that have a document
frequency strictly higher than the given threshold (corpus-specific
stop words).
If float, the parameter represents a proportion of documents, integer
absolute counts.
This parameter is ignored if vocabulary is not None.</dd>
<dt>min_df <span class="classifier-delimiter">:</span> <span class="classifier">float in range [0.0, 1.0] or int, default=1</span></dt>
<dd>When building the vocabulary ignore terms that have a document
frequency strictly lower than the given threshold. This value is also
called cut-off in the literature.
If float, the parameter represents a proportion of documents, integer
absolute counts.
This parameter is ignored if vocabulary is not None.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int or None, default=None</span></dt>
<dd><p class="first">If not None, build a vocabulary that only consider the top
max_features ordered by term frequency across the corpus.</p>
<p class="last">This parameter is ignored if vocabulary is not None.</p>
</dd>
<dt>vocabulary <span class="classifier-delimiter">:</span> <span class="classifier">Mapping or iterable, optional</span></dt>
<dd>Either a Mapping (e.g., a dict) where keys are terms and values are
indices in the feature matrix, or an iterable over terms. If not
given, a vocabulary is determined from the input documents. Indices
in the mapping should not be repeated and should not have any gap
between 0 and the largest index.</dd>
<dt>binary <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=False</span></dt>
<dd>If True, all non zero counts are set to 1. This is useful for discrete
probabilistic models that model binary events rather than integer
counts.</dd>
<dt>dtype <span class="classifier-delimiter">:</span> <span class="classifier">type, optional</span></dt>
<dd>Type of the matrix returned by fit_transform() or transform().</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">vocabulary_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd>A mapping of terms to feature indices.</dd>
<dt><code class="docutils literal"><span class="pre">stop_words_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">set</span></dt>
<dd><p class="first">Terms that were ignored because they either:</p>
<blockquote>
<div><ul class="simple">
<li>occurred in too many documents (<cite>max_df</cite>)</li>
<li>occurred in too few documents (<cite>min_df</cite>)</li>
<li>were cut off by feature selection (<cite>max_features</cite>).</li>
</ul>
</div></blockquote>
<p class="last">This is only available if no vocabulary was given.</p>
</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
<span class="gp">... </span>    <span class="s1">&#39;This is the first document.&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;This document is the second document.&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;And this is the third one.&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;Is this the first document?&#39;</span><span class="p">,</span>
<span class="gp">... </span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
<span class="go">[&#39;and&#39;, &#39;document&#39;, &#39;first&#39;, &#39;is&#39;, &#39;one&#39;, &#39;second&#39;, &#39;the&#39;, &#39;third&#39;, &#39;this&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>  
<span class="go">[[0 1 1 1 0 0 1 0 1]</span>
<span class="go"> [0 2 0 1 0 1 1 0 1]</span>
<span class="go"> [1 0 0 1 1 0 1 1 1]</span>
<span class="go"> [0 1 1 1 0 0 1 0 1]]</span>
</pre></div>
</div>
<p>See also</p>
<p>HashingVectorizer, TfidfVectorizer</p>
<p><strong>Notes</strong></p>
<p>The <code class="docutils literal"><span class="pre">stop_words_</span></code> attribute can get large and increase the model size
when pickling. This attribute is provided only for introspection and can
be safely removed using delattr or set to None before pickling.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.CountVectorizerScikitsLearnNode-class.html">CountVectorizerScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.ExtraTreesRegressorScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">ExtraTreesRegressorScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.ExtraTreesRegressorScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>An extra-trees regressor.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.ensemble.forest.ExtraTreesRegressor</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
This class implements a meta estimator that fits a number of
randomized decision trees (a.k.a. extra-trees) on various sub-samples
of the dataset and uses averaging to improve the predictive accuracy
and control over-fitting.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_estimators <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=10)</span></dt>
<dd><p class="first">The number of trees in the forest.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.20: </span>The default value of <code class="docutils literal"><span class="pre">n_estimators</span></code> will change from 10 in
version 0.20 to 100 in version 0.22.</p>
</div>
</dd>
<dt>criterion <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=”mse”)</span></dt>
<dd><p class="first">The function to measure the quality of a split. Supported criteria
are “mse” for the mean squared error, which is equal to variance
reduction as feature selection criterion, and “mae” for the mean
absolute error.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.18: </span>Mean Absolute Error (MAE) criterion.</p>
</div>
</dd>
<dt>max_depth <span class="classifier-delimiter">:</span> <span class="classifier">integer or None, optional (default=None)</span></dt>
<dd>The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</dd>
<dt>min_samples_split <span class="classifier-delimiter">:</span> <span class="classifier">int, float, optional (default=2)</span></dt>
<dd><p class="first">The minimum number of samples required to split an internal node:</p>
<ul class="simple">
<li>If int, then consider <cite>min_samples_split</cite> as the minimum number.</li>
<li>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</li>
</ul>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</dd>
<dt>min_samples_leaf <span class="classifier-delimiter">:</span> <span class="classifier">int, float, optional (default=1)</span></dt>
<dd><p class="first">The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul class="simple">
<li>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</li>
<li>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</li>
</ul>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</dd>
<dt>min_weight_fraction_leaf <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span></dt>
<dd>The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int, float, string or None, optional (default=”auto”)</span></dt>
<dd><p class="first">The number of features to consider when looking for the best split:</p>
<ul class="simple">
<li>If int, then consider <cite>max_features</cite> features at each split.</li>
<li>If float, then <cite>max_features</cite> is a fraction and
<cite>int(max_features * n_features)</cite> features are considered at each
split.</li>
<li>If “auto”, then <cite>max_features=n_features</cite>.</li>
<li>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite>.</li>
<li>If “log2”, then <cite>max_features=log2(n_features)</cite>.</li>
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
<p class="last">Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal"><span class="pre">max_features</span></code> features.</p>
</dd>
<dt>max_leaf_nodes <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Grow trees with <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</dd>
<dt>min_impurity_decrease <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span></dt>
<dd><p class="first">A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal"><span class="pre">N</span></code>, <code class="docutils literal"><span class="pre">N_t</span></code>, <code class="docutils literal"><span class="pre">N_t_R</span></code> and <code class="docutils literal"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19.</span></p>
</div>
</dd>
<dt>min_impurity_split <span class="classifier-delimiter">:</span> <span class="classifier">float, (default=1e-7)</span></dt>
<dd><p class="first">Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="last deprecated">
<p><span class="versionmodified">Deprecated since version 0.19: </span><code class="docutils literal"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal"><span class="pre">min_impurity_split</span></code> will change from 1e-7 to 0 in 0.23 and it
will be removed in 0.25. Use <code class="docutils literal"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</dd>
<dt>bootstrap <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=False)</span></dt>
<dd>Whether bootstrap samples are used when building trees. If False, the
whole datset is used to build each tree.</dd>
<dt>oob_score <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default=False)</span></dt>
<dd>Whether to use out-of-bag samples to estimate the R^2 on unseen data.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The number of jobs to run in parallel for both <cite>fit</cite> and <cite>predict</cite>.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=0)</span></dt>
<dd>Controls the verbosity when fitting and predicting.</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default=False)</span></dt>
<dd>When set to <code class="docutils literal"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit a whole
new forest. See <span class="xref std std-term">the Glossary</span>.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">estimators_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list of DecisionTreeRegressor</span></dt>
<dd>The collection of fitted sub-estimators.</dd>
<dt><code class="docutils literal"><span class="pre">feature_importances_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span></dt>
<dd>The feature importances (the higher, the more important the feature).</dd>
<dt><code class="docutils literal"><span class="pre">n_features_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The number of features.</dd>
<dt><code class="docutils literal"><span class="pre">n_outputs_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The number of outputs.</dd>
<dt><code class="docutils literal"><span class="pre">oob_score_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Score of the training dataset obtained using an out-of-bag estimate.</dd>
<dt><code class="docutils literal"><span class="pre">oob_prediction_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_samples]</span></dt>
<dd>Prediction computed with out-of-bag estimate on the training set.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code class="docutils literal"><span class="pre">max_depth</span></code>, <code class="docutils literal"><span class="pre">min_samples_leaf</span></code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id28" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized trees”,
Machine Learning, 63(1), 3-42, 2006.</td></tr>
</tbody>
</table>
<p>See also</p>
<p>sklearn.tree.ExtraTreeRegressor: Base estimator for this ensemble.
RandomForestRegressor: Ensemble regressor using trees with optimal splits.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#ExtraTreesRegressorScikitsLearnNode">ExtraTreesRegressorScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.LabelPropagationScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">LabelPropagationScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.LabelPropagationScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Label Propagation classifier
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.semi_supervised.label_propagation.LabelPropagation</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>kernel <span class="classifier-delimiter">:</span> <span class="classifier">{‘knn’, ‘rbf’, callable}</span></dt>
<dd>String identifier for kernel function to use or the kernel function
itself. Only ‘rbf’ and ‘knn’ strings are valid inputs. The function
passed should take two inputs, each of shape [n_samples, n_features],
and return a [n_samples, n_samples] shaped weight matrix.</dd>
<dt>gamma <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Parameter for rbf kernel</dd>
<dt>n_neighbors <span class="classifier-delimiter">:</span> <span class="classifier">integer &gt; 0</span></dt>
<dd>Parameter for knn kernel</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd><p class="first">Clamping factor.</p>
<div class="last deprecated">
<p><span class="versionmodified">Deprecated since version 0.19: </span>This parameter will be removed in 0.21.
‘alpha’ is fixed to zero in ‘LabelPropagation’.</p>
</div>
</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd>Change maximum number of iterations allowed</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Convergence tolerance: threshold to consider the system at steady
state</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The number of parallel jobs to run.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">X_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_samples, n_features]</span></dt>
<dd>Input array.</dd>
<dt><code class="docutils literal"><span class="pre">classes_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes]</span></dt>
<dd>The distinct labels used in classifying instances.</dd>
<dt><code class="docutils literal"><span class="pre">label_distributions_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_samples, n_classes]</span></dt>
<dd>Categorical distribution for each item.</dd>
<dt><code class="docutils literal"><span class="pre">transduction_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_samples]</span></dt>
<dd>Label assigned to each item via the transduction.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of iterations run.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.semi_supervised</span> <span class="kn">import</span> <span class="n">LabelPropagation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label_prop_model</span> <span class="o">=</span> <span class="n">LabelPropagation</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">random_unlabeled_points</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">))</span> <span class="o">&lt;</span> <span class="mf">0.3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span><span class="p">[</span><span class="n">random_unlabeled_points</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label_prop_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">LabelPropagation(...)</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data
with label propagation. Technical Report CMU-CALD-02-107, Carnegie Mellon
University, 2002 <a class="reference external" href="http://pages.cs.wisc.edu/~jerryzhu/pub/CMU-CALD-02-107.pdf">http://pages.cs.wisc.edu/~jerryzhu/pub/CMU-CALD-02-107.pdf</a></p>
<p>See Also</p>
<p>LabelSpreading : Alternate label propagation strategy more robust to noise</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#LabelPropagationScikitsLearnNode">LabelPropagationScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.GaussianMixtureScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">GaussianMixtureScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.GaussianMixtureScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Gaussian Mixture.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.mixture.gaussian_mixture.GaussianMixture</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Representation of a Gaussian mixture model probability distribution.
This class allows to estimate the parameters of a Gaussian mixture
distribution.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.18.</span></p>
</div>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int, defaults to 1.</span></dt>
<dd>The number of mixture components.</dd>
<dt>covariance_type <span class="classifier-delimiter">:</span> <span class="classifier">{‘full’ (default), ‘tied’, ‘diag’, ‘spherical’}</span></dt>
<dd><p class="first">String describing the type of covariance parameters to use.
Must be one of:</p>
<dl class="last docutils">
<dt>‘full’</dt>
<dd>each component has its own general covariance matrix</dd>
<dt>‘tied’</dt>
<dd>all components share the same general covariance matrix</dd>
<dt>‘diag’</dt>
<dd>each component has its own diagonal covariance matrix</dd>
<dt>‘spherical’</dt>
<dd>each component has its own single variance</dd>
</dl>
</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, defaults to 1e-3.</span></dt>
<dd>The convergence threshold. EM iterations will stop when the
lower bound average gain is below this threshold.</dd>
<dt>reg_covar <span class="classifier-delimiter">:</span> <span class="classifier">float, defaults to 1e-6.</span></dt>
<dd>Non-negative regularization added to the diagonal of covariance.
Allows to assure that the covariance matrices are all positive.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, defaults to 100.</span></dt>
<dd>The number of EM iterations to perform.</dd>
<dt>n_init <span class="classifier-delimiter">:</span> <span class="classifier">int, defaults to 1.</span></dt>
<dd>The number of initializations to perform. The best results are kept.</dd>
<dt>init_params <span class="classifier-delimiter">:</span> <span class="classifier">{‘kmeans’, ‘random’}, defaults to ‘kmeans’.</span></dt>
<dd><p class="first">The method used to initialize the weights, the means and the
precisions.
Must be one of:</p>
<div class="last highlight-default"><div class="highlight"><pre><span></span><span class="s1">&#39;kmeans&#39;</span> <span class="p">:</span> <span class="n">responsibilities</span> <span class="n">are</span> <span class="n">initialized</span> <span class="n">using</span> <span class="n">kmeans</span><span class="o">.</span>
<span class="s1">&#39;random&#39;</span> <span class="p">:</span> <span class="n">responsibilities</span> <span class="n">are</span> <span class="n">initialized</span> <span class="n">randomly</span><span class="o">.</span>
</pre></div>
</div>
</dd>
<dt>weights_init <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_components, ), optional</span></dt>
<dd>The user-provided initial weights, defaults to None.
If it None, weights are initialized using the <cite>init_params</cite> method.</dd>
<dt>means_init <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_components, n_features), optional</span></dt>
<dd>The user-provided initial means, defaults to None,
If it None, means are initialized using the <cite>init_params</cite> method.</dd>
<dt>precisions_init <span class="classifier-delimiter">:</span> <span class="classifier">array-like, optional.</span></dt>
<dd><p class="first">The user-provided initial precisions (inverse of the covariance
matrices), defaults to None.
If it None, precisions are initialized using the ‘init_params’ method.
The shape depends on ‘covariance_type’:</p>
<div class="last highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">n_components</span><span class="p">,)</span>                        <span class="k">if</span> <span class="s1">&#39;spherical&#39;</span><span class="p">,</span>
<span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>               <span class="k">if</span> <span class="s1">&#39;tied&#39;</span><span class="p">,</span>
<span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>             <span class="k">if</span> <span class="s1">&#39;diag&#39;</span><span class="p">,</span>
<span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;full&#39;</span>
</pre></div>
</div>
</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, default to False.</span></dt>
<dd>If ‘warm_start’ is True, the solution of the last fitting is used as
initialization for the next call of fit(). This can speed up
convergence when fit is called several times on similar problems.
In that case, ‘n_init’ is ignored and only a single initialization
occurs upon the first call.
See <span class="xref std std-term">the Glossary</span>.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, default to 0.</span></dt>
<dd>Enable verbose output. If 1 then it prints the current
initialization and each iteration step. If greater than 1 then
it prints also the log probability and the time needed
for each step.</dd>
<dt>verbose_interval <span class="classifier-delimiter">:</span> <span class="classifier">int, default to 10.</span></dt>
<dd>Number of iteration done before the next print.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">weights_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_components,)</span></dt>
<dd>The weights of each mixture components.</dd>
<dt><code class="docutils literal"><span class="pre">means_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_components, n_features)</span></dt>
<dd>The mean of each mixture component.</dd>
<dt><code class="docutils literal"><span class="pre">covariances_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like</span></dt>
<dd><p class="first">The covariance of each mixture component.
The shape depends on <cite>covariance_type</cite>:</p>
<div class="last highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">n_components</span><span class="p">,)</span>                        <span class="k">if</span> <span class="s1">&#39;spherical&#39;</span><span class="p">,</span>
<span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>               <span class="k">if</span> <span class="s1">&#39;tied&#39;</span><span class="p">,</span>
<span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>             <span class="k">if</span> <span class="s1">&#39;diag&#39;</span><span class="p">,</span>
<span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;full&#39;</span>
</pre></div>
</div>
</dd>
<dt><code class="docutils literal"><span class="pre">precisions_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like</span></dt>
<dd><p class="first">The precision matrices for each component in the mixture. A precision
matrix is the inverse of a covariance matrix. A covariance matrix is
symmetric positive definite so the mixture of Gaussian can be
equivalently parameterized by the precision matrices. Storing the
precision matrices instead of the covariance matrices makes it more
efficient to compute the log-likelihood of new samples at test time.
The shape depends on <cite>covariance_type</cite>:</p>
<div class="last highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">n_components</span><span class="p">,)</span>                        <span class="k">if</span> <span class="s1">&#39;spherical&#39;</span><span class="p">,</span>
<span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>               <span class="k">if</span> <span class="s1">&#39;tied&#39;</span><span class="p">,</span>
<span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>             <span class="k">if</span> <span class="s1">&#39;diag&#39;</span><span class="p">,</span>
<span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;full&#39;</span>
</pre></div>
</div>
</dd>
<dt><code class="docutils literal"><span class="pre">precisions_cholesky_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like</span></dt>
<dd><p class="first">The cholesky decomposition of the precision matrices of each mixture
component. A precision matrix is the inverse of a covariance matrix.
A covariance matrix is symmetric positive definite so the mixture of
Gaussian can be equivalently parameterized by the precision matrices.
Storing the precision matrices instead of the covariance matrices makes
it more efficient to compute the log-likelihood of new samples at test
time. The shape depends on <cite>covariance_type</cite>:</p>
<div class="last highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">n_components</span><span class="p">,)</span>                        <span class="k">if</span> <span class="s1">&#39;spherical&#39;</span><span class="p">,</span>
<span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>               <span class="k">if</span> <span class="s1">&#39;tied&#39;</span><span class="p">,</span>
<span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>             <span class="k">if</span> <span class="s1">&#39;diag&#39;</span><span class="p">,</span>
<span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;full&#39;</span>
</pre></div>
</div>
</dd>
<dt><code class="docutils literal"><span class="pre">converged_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>True when convergence was reached in fit(), False otherwise.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of step used by the best fit of EM to reach the convergence.</dd>
<dt><code class="docutils literal"><span class="pre">lower_bound_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Lower bound value on the log-likelihood (of the training data with
respect to the model) of the best fit of EM.</dd>
</dl>
<p>See Also</p>
<dl class="docutils">
<dt>BayesianGaussianMixture <span class="classifier-delimiter">:</span> <span class="classifier">Gaussian mixture model fit with a variational</span></dt>
<dd>inference.</dd>
</dl>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#GaussianMixtureScikitsLearnNode">GaussianMixtureScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.MeanEstimatorScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">MeanEstimatorScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.MeanEstimatorScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.ensemble.gradient_boosting.MeanEstimator</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#MeanEstimatorScikitsLearnNode">MeanEstimatorScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.SelectFromModelScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">SelectFromModelScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.SelectFromModelScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Meta-transformer for selecting features based on importance weights.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.feature_selection.from_model.SelectFromModel</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
.. versionadded:: 0.17</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>estimator <span class="classifier-delimiter">:</span> <span class="classifier">object</span></dt>
<dd>The base estimator from which the transformer is built.
This can be both a fitted (if <code class="docutils literal"><span class="pre">prefit</span></code> is set to True)
or a non-fitted estimator. The estimator must have either a
<code class="docutils literal"><span class="pre">feature_importances_</span></code> or <code class="docutils literal"><span class="pre">coef_</span></code> attribute after fitting.</dd>
<dt>threshold <span class="classifier-delimiter">:</span> <span class="classifier">string, float, optional default None</span></dt>
<dd>The threshold value to use for feature selection. Features whose
importance is greater or equal are kept while the others are
discarded. If “median” (resp. “mean”), then the <code class="docutils literal"><span class="pre">threshold</span></code> value is
the median (resp. the mean) of the feature importances. A scaling
factor (e.g., “1.25*mean”) may also be used. If None and if the
estimator has a parameter penalty set to l1, either explicitly
or implicitly (e.g, Lasso), the threshold used is 1e-5.
Otherwise, “mean” is used by default.</dd>
<dt>prefit <span class="classifier-delimiter">:</span> <span class="classifier">bool, default False</span></dt>
<dd>Whether a prefit model is expected to be passed into the constructor
directly or not. If True, <code class="docutils literal"><span class="pre">transform</span></code> must be called directly
and SelectFromModel cannot be used with <code class="docutils literal"><span class="pre">cross_val_score</span></code>,
<code class="docutils literal"><span class="pre">GridSearchCV</span></code> and similar utilities that clone the estimator.
Otherwise train the model using <code class="docutils literal"><span class="pre">fit</span></code> and then <code class="docutils literal"><span class="pre">transform</span></code> to do
feature selection.</dd>
<dt>norm_order <span class="classifier-delimiter">:</span> <span class="classifier">non-zero int, inf, -inf, default 1</span></dt>
<dd>Order of the norm used to filter the vectors of coefficients below
<code class="docutils literal"><span class="pre">threshold</span></code> in the case where the <code class="docutils literal"><span class="pre">coef_</span></code> attribute of the
estimator is of dimension 2.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional</span></dt>
<dd><p class="first">The maximum number of features selected scoring above <code class="docutils literal"><span class="pre">threshold</span></code>.
To disable <code class="docutils literal"><span class="pre">threshold</span></code> and only select based on <code class="docutils literal"><span class="pre">max_features</span></code>,
set <code class="docutils literal"><span class="pre">threshold=-np.inf</span></code>.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">estimator_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">an estimator</span></dt>
<dd>The base estimator from which the transformer is built.
This is stored only when a non-fitted estimator is passed to the
<code class="docutils literal"><span class="pre">SelectFromModel</span></code>, i.e when prefit is False.</dd>
<dt><code class="docutils literal"><span class="pre">threshold_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The threshold value used for feature selection.</dd>
</dl>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#SelectFromModelScikitsLearnNode">SelectFromModelScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.RadiusNeighborsRegressorScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">RadiusNeighborsRegressorScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.RadiusNeighborsRegressorScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Regression based on neighbors within a fixed radius.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.neighbors.regression.RadiusNeighborsRegressor</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
The target is predicted by local interpolation of the targets
associated of the nearest neighbors in the training set.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>radius <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default = 1.0)</span></dt>
<dd>Range of parameter space to use by default for <code class="xref py py-meth docutils literal"><span class="pre">radius_neighbors()</span></code>
queries.</dd>
<dt>weights <span class="classifier-delimiter">:</span> <span class="classifier">str or callable</span></dt>
<dd><p class="first">weight function used in prediction.  Possible values:</p>
<ul class="simple">
<li>‘uniform’ : uniform weights.  All points in each neighborhood
are weighted equally.</li>
<li>‘distance’ : weight points by the inverse of their distance.
in this case, closer neighbors of a query point will have a
greater influence than neighbors which are further away.</li>
<li>[callable] : a user-defined function which accepts an
array of distances, and returns an array of the same shape
containing the weights.</li>
</ul>
<p class="last">Uniform weights are used by default.</p>
</dd>
<dt>algorithm <span class="classifier-delimiter">:</span> <span class="classifier">{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, optional</span></dt>
<dd><p class="first">Algorithm used to compute the nearest neighbors:</p>
<ul class="simple">
<li>‘ball_tree’ will use <code class="xref py py-class docutils literal"><span class="pre">BallTree</span></code></li>
<li>‘kd_tree’ will use <code class="xref py py-class docutils literal"><span class="pre">KDTree</span></code></li>
<li>‘brute’ will use a brute-force search.</li>
<li>‘auto’ will attempt to decide the most appropriate algorithm
based on the values passed to <code class="xref py py-meth docutils literal"><span class="pre">fit()</span></code> method.</li>
</ul>
<p class="last">Note: fitting on sparse input will override the setting of
this parameter, using brute force.</p>
</dd>
<dt>leaf_size <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default = 30)</span></dt>
<dd>Leaf size passed to BallTree or KDTree.  This can affect the
speed of the construction and query, as well as the memory
required to store the tree.  The optimal value depends on the
nature of the problem.</dd>
<dt>p <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default = 2)</span></dt>
<dd>Power parameter for the Minkowski metric. When p = 1, this is
equivalent to using manhattan_distance (l1), and euclidean_distance
(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.</dd>
<dt>metric <span class="classifier-delimiter">:</span> <span class="classifier">string or callable, default ‘minkowski’</span></dt>
<dd>the distance metric to use for the tree.  The default metric is
minkowski, and with p=2 is equivalent to the standard Euclidean
metric. See the documentation of the DistanceMetric class for a
list of available metrics.</dd>
<dt>metric_params <span class="classifier-delimiter">:</span> <span class="classifier">dict, optional (default = None)</span></dt>
<dd>Additional keyword arguments for the metric function.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd><dl class="first docutils">
<dt>The number of parallel jobs to run for neighbors search.</dt>
<dd><code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.</dd>
</dl>
<p class="last"><code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p>
</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">RadiusNeighborsRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span> <span class="o">=</span> <span class="n">RadiusNeighborsRegressor</span><span class="p">(</span><span class="n">radius</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">RadiusNeighborsRegressor(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">neigh</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">1.5</span><span class="p">]]))</span>
<span class="go">[0.5]</span>
</pre></div>
</div>
<p>See also</p>
<p>NearestNeighbors
KNeighborsRegressor
KNeighborsClassifier
RadiusNeighborsClassifier</p>
<p><strong>Notes</strong></p>
<p>See <span class="xref std std-ref">Nearest Neighbors</span> in the online documentation
for a discussion of the choice of <code class="docutils literal"><span class="pre">algorithm</span></code> and <code class="docutils literal"><span class="pre">leaf_size</span></code>.</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm">https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm</a></p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#RadiusNeighborsRegressorScikitsLearnNode">RadiusNeighborsRegressorScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.PLSSVDScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">PLSSVDScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.PLSSVDScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Partial Least Square SVD
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.cross_decomposition.pls_.PLSSVD</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Simply perform a svd on the crosscovariance matrix: X’Y
There are no iterative deflation here.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int, default 2</span></dt>
<dd>Number of components to keep.</dd>
<dt>scale <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default True</span></dt>
<dd>Whether to scale X and Y.</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default True</span></dt>
<dd>Whether to copy X and Y, or perform in-place computations.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">x_weights_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [p, n_components]</span></dt>
<dd>X block weights vectors.</dd>
<dt><code class="docutils literal"><span class="pre">y_weights_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [q, n_components]</span></dt>
<dd>Y block weights vectors.</dd>
<dt><code class="docutils literal"><span class="pre">x_scores_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_samples, n_components]</span></dt>
<dd>X scores.</dd>
<dt><code class="docutils literal"><span class="pre">y_scores_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_samples, n_components]</span></dt>
<dd>Y scores.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cross_decomposition</span> <span class="kn">import</span> <span class="n">PLSSVD</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
<span class="gp">... </span>    <span class="p">[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="mf">0.</span><span class="p">],</span>
<span class="gp">... </span>    <span class="p">[</span><span class="mf">2.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">2.</span><span class="p">],</span>
<span class="gp">... </span>    <span class="p">[</span><span class="mf">2.</span><span class="p">,</span><span class="mf">5.</span><span class="p">,</span><span class="mf">4.</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">],</span>
<span class="gp">... </span>    <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span>
<span class="gp">... </span>    <span class="p">[</span><span class="mf">6.2</span><span class="p">,</span> <span class="mf">5.9</span><span class="p">],</span>
<span class="gp">... </span>    <span class="p">[</span><span class="mf">11.9</span><span class="p">,</span> <span class="mf">12.3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plsca</span> <span class="o">=</span> <span class="n">PLSSVD</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plsca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="go">PLSSVD(copy=True, n_components=2, scale=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_c</span><span class="p">,</span> <span class="n">Y_c</span> <span class="o">=</span> <span class="n">plsca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_c</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">Y_c</span><span class="o">.</span><span class="n">shape</span>
<span class="go">((4, 2), (4, 2))</span>
</pre></div>
</div>
<p>See also</p>
<p>PLSCanonical
CCA</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.PLSSVDScikitsLearnNode-class.html">PLSSVDScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.GaussianRandomProjectionScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">GaussianRandomProjectionScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.GaussianRandomProjectionScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Reduce dimensionality through Gaussian random projection
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.random_projection.GaussianRandomProjection</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
The components of the random matrix are drawn from N(0, 1 / n_components).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int or ‘auto’, optional (default = ‘auto’)</span></dt>
<dd><p class="first">Dimensionality of the target projection space.</p>
<p>n_components can be automatically adjusted according to the
number of samples in the dataset and the bound given by the
Johnson-Lindenstrauss lemma. In that case the quality of the
embedding is controlled by the <code class="docutils literal"><span class="pre">eps</span></code> parameter.</p>
<p class="last">It should be noted that Johnson-Lindenstrauss lemma can yield
very conservative estimated of the required number of components
as it makes no assumption on the structure of the dataset.</p>
</dd>
<dt>eps <span class="classifier-delimiter">:</span> <span class="classifier">strictly positive float, optional (default=0.1)</span></dt>
<dd><p class="first">Parameter to control the quality of the embedding according to
the Johnson-Lindenstrauss lemma when n_components is set to
‘auto’.</p>
<p class="last">Smaller values lead to better embedding and higher number of
dimensions (n_components) in the target projection space.</p>
</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>Control the pseudo random number generator used to generate the matrix
at fit time.  If int, random_state is the seed used by the random
number generator; If RandomState instance, random_state is the random
number generator; If None, the random number generator is the
RandomState instance used by <cite>np.random</cite>.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">n_component_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Concrete number of components computed when n_components=”auto”.</dd>
<dt><code class="docutils literal"><span class="pre">components_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">numpy array of shape [n_components, n_features]</span></dt>
<dd>Random matrix used for the projection.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.random_projection</span> <span class="kn">import</span> <span class="n">GaussianRandomProjection</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span> <span class="o">=</span> <span class="n">GaussianRandomProjection</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_new</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_new</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(100, 3947)</span>
</pre></div>
</div>
<p>See Also</p>
<p>SparseRandomProjection</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#GaussianRandomProjectionScikitsLearnNode">GaussianRandomProjectionScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.OneHotEncoderScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">OneHotEncoderScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.OneHotEncoderScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Encode categorical integer features as a one-hot numeric array.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.preprocessing._encoders.OneHotEncoder</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
The input to this transformer should be an array-like of integers or
strings, denoting the values taken on by categorical (discrete) features.
The features are encoded using a one-hot (aka ‘one-of-K’ or ‘dummy’)
encoding scheme. This creates a binary column for each category and
returns a sparse matrix or dense array.</p>
<p>By default, the encoder derives the categories based on the unique values
in each feature. Alternatively, you can also specify the <cite>categories</cite>
manually.
The OneHotEncoder previously assumed that the input features take on
values in the range [0, max(values)). This behaviour is deprecated.</p>
<p>This encoding is needed for feeding categorical data to many scikit-learn
estimators, notably linear models and SVMs with the standard kernels.</p>
<p>Note: a one-hot encoding of y labels should use a LabelBinarizer
instead.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>categories <span class="classifier-delimiter">:</span> <span class="classifier">‘auto’ or a list of lists/arrays of values, default=’auto’.</span></dt>
<dd><p class="first">Categories (unique values) per feature:</p>
<ul class="simple">
<li>‘auto’ : Determine categories automatically from the training data.</li>
<li>list : <code class="docutils literal"><span class="pre">categories[i]</span></code> holds the categories expected in the ith
column. The passed categories should not mix strings and numeric
values within a single feature, and should be sorted in case of
numeric values.</li>
</ul>
<p class="last">The used categories can be found in the <code class="docutils literal"><span class="pre">categories_</span></code> attribute.</p>
</dd>
<dt>sparse <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=True</span></dt>
<dd>Will return sparse matrix if set True else will return an array.</dd>
<dt>dtype <span class="classifier-delimiter">:</span> <span class="classifier">number type, default=np.float</span></dt>
<dd>Desired dtype of output.</dd>
<dt>handle_unknown <span class="classifier-delimiter">:</span> <span class="classifier">‘error’ or ‘ignore’, default=’error’.</span></dt>
<dd>Whether to raise an error or ignore if an unknown categorical feature
is present during transform (default is to raise). When this parameter
is set to ‘ignore’ and an unknown category is encountered during
transform, the resulting one-hot encoded columns for this feature
will be all zeros. In the inverse transform, an unknown category
will be denoted as None.</dd>
<dt>n_values <span class="classifier-delimiter">:</span> <span class="classifier">‘auto’, int or array of ints, default=’auto’</span></dt>
<dd><p class="first">Number of values per feature.</p>
<ul>
<li><p class="first">‘auto’ : determine value range from training data.</p>
</li>
<li><dl class="first docutils">
<dt>int <span class="classifier-delimiter">:</span> <span class="classifier">number of categorical values per feature.</span></dt>
<dd><p class="first last">Each feature value should be in <code class="docutils literal"><span class="pre">range(n_values)</span></code></p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>array <span class="classifier-delimiter">:</span> <span class="classifier"><code class="docutils literal"><span class="pre">n_values[i]</span></code> is the number of categorical values in</span></dt>
<dd><p class="first last"><code class="docutils literal"><span class="pre">X[:,</span> <span class="pre">i]</span></code>. Each feature value should be
in <code class="docutils literal"><span class="pre">range(n_values[i])</span></code></p>
</dd>
</dl>
</li>
</ul>
<div class="last deprecated">
<p><span class="versionmodified">Deprecated since version 0.20: </span>The <cite>n_values</cite> keyword was deprecated in version 0.20 and will
be removed in 0.22. Use <cite>categories</cite> instead.</p>
</div>
</dd>
<dt>categorical_features <span class="classifier-delimiter">:</span> <span class="classifier">‘all’ or array of indices or mask, default=’all’</span></dt>
<dd><p class="first">Specify what features are treated as categorical.</p>
<ul class="simple">
<li>‘all’: All features are treated as categorical.</li>
<li>array of indices: Array of categorical feature indices.</li>
<li>mask: Array of length n_features and with dtype=bool.</li>
</ul>
<p>Non-categorical features are always stacked to the right of the matrix.</p>
<div class="last deprecated">
<p><span class="versionmodified">Deprecated since version 0.20: </span>The <cite>categorical_features</cite> keyword was deprecated in version
0.20 and will be removed in 0.22.
You can use the <code class="docutils literal"><span class="pre">ColumnTransformer</span></code> instead.</p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">categories_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list of arrays</span></dt>
<dd>The categories of each feature determined during fitting
(in order of the features in X and corresponding with the output
of <code class="docutils literal"><span class="pre">transform</span></code>).</dd>
<dt><code class="docutils literal"><span class="pre">active_features_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array</span></dt>
<dd><p class="first">Indices for active features, meaning values that actually occur
in the training set. Only available when n_values is <code class="docutils literal"><span class="pre">'auto'</span></code>.</p>
<div class="last deprecated">
<p><span class="versionmodified">Deprecated since version 0.20: </span>The <code class="docutils literal"><span class="pre">active_features_</span></code> attribute was deprecated in version
0.20 and will be removed in 0.22.</p>
</div>
</dd>
<dt><code class="docutils literal"><span class="pre">feature_indices_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_features,)</span></dt>
<dd><p class="first">Indices to feature ranges.
Feature <code class="docutils literal"><span class="pre">i</span></code> in the original data is mapped to features
from <code class="docutils literal"><span class="pre">feature_indices_[i]</span></code> to <code class="docutils literal"><span class="pre">feature_indices_[i+1]</span></code>
(and then potentially masked by <code class="docutils literal"><span class="pre">active_features_</span></code> afterwards)</p>
<div class="last deprecated">
<p><span class="versionmodified">Deprecated since version 0.20: </span>The <code class="docutils literal"><span class="pre">feature_indices_</span></code> attribute was deprecated in version
0.20 and will be removed in 0.22.</p>
</div>
</dd>
<dt><code class="docutils literal"><span class="pre">n_values_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_features,)</span></dt>
<dd><p class="first">Maximum number of values per feature.</p>
<div class="last deprecated">
<p><span class="versionmodified">Deprecated since version 0.20: </span>The <code class="docutils literal"><span class="pre">n_values_</span></code> attribute was deprecated in version
0.20 and will be removed in 0.22.</p>
</div>
</dd>
</dl>
<p><strong>Examples</strong></p>
<p>Given a dataset with two features, we let the encoder find the unique
values per feature and transform the data to a binary one-hot encoding.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">handle_unknown</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">&#39;Male&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;Female&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;Female&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">OneHotEncoder(categorical_features=None, categories=None,</span>
<span class="go">       dtype=&lt;... &#39;numpy.float64&#39;&gt;, handle_unknown=&#39;ignore&#39;,</span>
<span class="go">       n_values=None, sparse=True)</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span><span class="o">.</span><span class="n">categories_</span>
<span class="go">[array([&#39;Female&#39;, &#39;Male&#39;], dtype=object), array([1, 2, 3], dtype=object)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span><span class="o">.</span><span class="n">transform</span><span class="p">([[</span><span class="s1">&#39;Female&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;Male&#39;</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="go">array([[1., 0., 1., 0., 0.],</span>
<span class="go">       [0., 1., 0., 0., 0.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="go">array([[&#39;Male&#39;, 1],</span>
<span class="go">       [None, 2]], dtype=object)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span>
<span class="go">array([&#39;x0_Female&#39;, &#39;x0_Male&#39;, &#39;x1_1&#39;, &#39;x1_2&#39;, &#39;x1_3&#39;], dtype=object)</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>sklearn.preprocessing.OrdinalEncoder <span class="classifier-delimiter">:</span> <span class="classifier">performs an ordinal (integer)</span></dt>
<dd>encoding of the categorical features.</dd>
<dt>sklearn.feature_extraction.DictVectorizer <span class="classifier-delimiter">:</span> <span class="classifier">performs a one-hot encoding of</span></dt>
<dd>dictionary items (also handles string-valued features).</dd>
<dt>sklearn.feature_extraction.FeatureHasher <span class="classifier-delimiter">:</span> <span class="classifier">performs an approximate one-hot</span></dt>
<dd>encoding of dictionary items or strings.</dd>
<dt>sklearn.preprocessing.LabelBinarizer <span class="classifier-delimiter">:</span> <span class="classifier">binarizes labels in a one-vs-all</span></dt>
<dd>fashion.</dd>
<dt>sklearn.preprocessing.MultiLabelBinarizer <span class="classifier-delimiter">:</span> <span class="classifier">transforms between iterable of</span></dt>
<dd>iterables and a multilabel format, e.g. a (samples x classes) binary
matrix indicating the presence of a class label.</dd>
</dl>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#OneHotEncoderScikitsLearnNode">OneHotEncoderScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.KNeighborsRegressorScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">KNeighborsRegressorScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.KNeighborsRegressorScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Regression based on k-nearest neighbors.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.neighbors.regression.KNeighborsRegressor</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
The target is predicted by local interpolation of the targets
associated of the nearest neighbors in the training set.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_neighbors <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default = 5)</span></dt>
<dd>Number of neighbors to use by default for <code class="xref py py-meth docutils literal"><span class="pre">kneighbors()</span></code> queries.</dd>
<dt>weights <span class="classifier-delimiter">:</span> <span class="classifier">str or callable</span></dt>
<dd><p class="first">weight function used in prediction.  Possible values:</p>
<ul class="simple">
<li>‘uniform’ : uniform weights.  All points in each neighborhood
are weighted equally.</li>
<li>‘distance’ : weight points by the inverse of their distance.
in this case, closer neighbors of a query point will have a
greater influence than neighbors which are further away.</li>
<li>[callable] : a user-defined function which accepts an
array of distances, and returns an array of the same shape
containing the weights.</li>
</ul>
<p class="last">Uniform weights are used by default.</p>
</dd>
<dt>algorithm <span class="classifier-delimiter">:</span> <span class="classifier">{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, optional</span></dt>
<dd><p class="first">Algorithm used to compute the nearest neighbors:</p>
<ul class="simple">
<li>‘ball_tree’ will use <code class="xref py py-class docutils literal"><span class="pre">BallTree</span></code></li>
<li>‘kd_tree’ will use <code class="xref py py-class docutils literal"><span class="pre">KDTree</span></code></li>
<li>‘brute’ will use a brute-force search.</li>
<li>‘auto’ will attempt to decide the most appropriate algorithm
based on the values passed to <code class="xref py py-meth docutils literal"><span class="pre">fit()</span></code> method.</li>
</ul>
<p class="last">Note: fitting on sparse input will override the setting of
this parameter, using brute force.</p>
</dd>
<dt>leaf_size <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default = 30)</span></dt>
<dd>Leaf size passed to BallTree or KDTree.  This can affect the
speed of the construction and query, as well as the memory
required to store the tree.  The optimal value depends on the
nature of the problem.</dd>
<dt>p <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default = 2)</span></dt>
<dd>Power parameter for the Minkowski metric. When p = 1, this is
equivalent to using manhattan_distance (l1), and euclidean_distance
(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.</dd>
<dt>metric <span class="classifier-delimiter">:</span> <span class="classifier">string or callable, default ‘minkowski’</span></dt>
<dd>the distance metric to use for the tree.  The default metric is
minkowski, and with p=2 is equivalent to the standard Euclidean
metric. See the documentation of the DistanceMetric class for a
list of available metrics.</dd>
<dt>metric_params <span class="classifier-delimiter">:</span> <span class="classifier">dict, optional (default = None)</span></dt>
<dd>Additional keyword arguments for the metric function.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The number of parallel jobs to run for neighbors search.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.
Doesn’t affect <code class="xref py py-meth docutils literal"><span class="pre">fit()</span></code> method.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">KNeighborsRegressor(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">neigh</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">1.5</span><span class="p">]]))</span>
<span class="go">[0.5]</span>
</pre></div>
</div>
<p>See also</p>
<p>NearestNeighbors
RadiusNeighborsRegressor
KNeighborsClassifier
RadiusNeighborsClassifier</p>
<p><strong>Notes</strong></p>
<p>See <span class="xref std std-ref">Nearest Neighbors</span> in the online documentation
for a discussion of the choice of <code class="docutils literal"><span class="pre">algorithm</span></code> and <code class="docutils literal"><span class="pre">leaf_size</span></code>.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Regarding the Nearest Neighbors algorithms, if it is found that two
neighbors, neighbor <cite>k+1</cite> and <cite>k</cite>, have identical distances but
different labels, the results will depend on the ordering of the
training data.</p>
</div>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm">https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm</a></p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#KNeighborsRegressorScikitsLearnNode">KNeighborsRegressorScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.LocalOutlierFactorScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">LocalOutlierFactorScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.LocalOutlierFactorScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Unsupervised Outlier Detection using Local Outlier Factor (LOF)
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.neighbors.lof.LocalOutlierFactor</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
The anomaly score of each sample is called Local Outlier Factor.
It measures the local deviation of density of a given sample with
respect to its neighbors.
It is local in that the anomaly score depends on how isolated the object
is with respect to the surrounding neighborhood.
More precisely, locality is given by k-nearest neighbors, whose distance
is used to estimate the local density.
By comparing the local density of a sample to the local densities of
its neighbors, one can identify samples that have a substantially lower
density than their neighbors. These are considered outliers.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_neighbors <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=20)</span></dt>
<dd>Number of neighbors to use by default for <code class="xref py py-meth docutils literal"><span class="pre">kneighbors()</span></code> queries.
If n_neighbors is larger than the number of samples provided,
all samples will be used.</dd>
<dt>algorithm <span class="classifier-delimiter">:</span> <span class="classifier">{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, optional</span></dt>
<dd><p class="first">Algorithm used to compute the nearest neighbors:</p>
<ul class="simple">
<li>‘ball_tree’ will use <code class="xref py py-class docutils literal"><span class="pre">BallTree</span></code></li>
<li>‘kd_tree’ will use <code class="xref py py-class docutils literal"><span class="pre">KDTree</span></code></li>
<li>‘brute’ will use a brute-force search.</li>
<li>‘auto’ will attempt to decide the most appropriate algorithm
based on the values passed to <code class="xref py py-meth docutils literal"><span class="pre">fit()</span></code> method.</li>
</ul>
<p class="last">Note: fitting on sparse input will override the setting of
this parameter, using brute force.</p>
</dd>
<dt>leaf_size <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=30)</span></dt>
<dd>Leaf size passed to <code class="xref py py-class docutils literal"><span class="pre">BallTree</span></code> or <code class="xref py py-class docutils literal"><span class="pre">KDTree</span></code>. This can
affect the speed of the construction and query, as well as the memory
required to store the tree. The optimal value depends on the
nature of the problem.</dd>
<dt>metric <span class="classifier-delimiter">:</span> <span class="classifier">string or callable, default ‘minkowski’</span></dt>
<dd><p class="first">metric used for the distance computation. Any metric from scikit-learn
or scipy.spatial.distance can be used.</p>
<p>If ‘precomputed’, the training input X is expected to be a distance
matrix.</p>
<p>If metric is a callable function, it is called on each
pair of instances (rows) and the resulting value recorded. The callable
should take two arrays as input and return one value indicating the
distance between them. This works for Scipy’s metrics, but is less
efficient than passing the metric name as a string.</p>
<p>Valid values for metric are:</p>
<ul class="simple">
<li>from scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’,
‘manhattan’]</li>
<li>from scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’,
‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’,
‘mahalanobis’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’,
‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’,
‘yule’]</li>
</ul>
<p>See the documentation for scipy.spatial.distance for details on these
metrics:</p>
<ul class="last simple">
<li><a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/spatial.distance.html">https://docs.scipy.org/doc/scipy/reference/spatial.distance.html</a></li>
</ul>
</dd>
<dt>p <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=2)</span></dt>
<dd>Parameter for the Minkowski metric from
<code class="xref py py-func docutils literal"><span class="pre">sklearn.metrics.pairwise.pairwise_distances()</span></code>. When p = 1, this
is equivalent to using manhattan_distance (l1), and euclidean_distance
(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.</dd>
<dt>metric_params <span class="classifier-delimiter">:</span> <span class="classifier">dict, optional (default=None)</span></dt>
<dd>Additional keyword arguments for the metric function.</dd>
<dt>contamination <span class="classifier-delimiter">:</span> <span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt>
<dd><p class="first">The amount of contamination of the data set, i.e. the proportion
of outliers in the data set. When fitting this is used to define the
threshold on the decision function. If “auto”, the decision function
threshold is determined as in the original paper.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.20: </span>The default value of <code class="docutils literal"><span class="pre">contamination</span></code> will change from 0.1 in 0.20
to <code class="docutils literal"><span class="pre">'auto'</span></code> in 0.22.</p>
</div>
</dd>
<dt>novelty <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default False</span></dt>
<dd>By default, LocalOutlierFactor is only meant to be used for outlier
detection (novelty=False). Set novelty to True if you want to use
LocalOutlierFactor for novelty detection. In this case be aware that
that you should only use predict, decision_function and score_samples
on new unseen data and not on the training set.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The number of parallel jobs to run for neighbors search.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.
Affects only <code class="xref py py-meth docutils literal"><span class="pre">kneighbors()</span></code> and <code class="xref py py-meth docutils literal"><span class="pre">kneighbors_graph()</span></code> methods.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">negative_outlier_factor_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">numpy array, shape (n_samples,)</span></dt>
<dd><p class="first">The opposite LOF of the training samples. The higher, the more normal.
Inliers tend to have a LOF score close to 1 (<code class="docutils literal"><span class="pre">negative_outlier_factor_</span></code>
close to -1), while outliers tend to have a larger LOF score.</p>
<p class="last">The local outlier factor (LOF) of a sample captures its
supposed ‘degree of abnormality’.
It is the average of the ratio of the local reachability density of
a sample and those of its k-nearest neighbors.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">n_neighbors_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd>The actual number of neighbors used for <code class="xref py py-meth docutils literal"><span class="pre">kneighbors()</span></code> queries.</dd>
<dt><code class="docutils literal"><span class="pre">offset_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Offset used to obtain binary labels from the raw scores.
Observations having a negative_outlier_factor smaller than <cite>offset_</cite>
are detected as abnormal.
The offset is set to -1.5 (inliers score around -1), except when a
contamination parameter different than “auto” is provided. In that
case, the offset is defined in such a way we obtain the expected
number of outliers in training.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id29" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Breunig, M. M., Kriegel, H. P., Ng, R. T., &amp; Sander, J. (2000, May).
LOF: identifying density-based local outliers. In ACM sigmod record.</td></tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#LocalOutlierFactorScikitsLearnNode">LocalOutlierFactorScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.GaussianProcessRegressorScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">GaussianProcessRegressorScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.GaussianProcessRegressorScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Gaussian process regression (GPR).
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.gaussian_process.gpr.GaussianProcessRegressor</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
The implementation is based on Algorithm 2.1 of Gaussian Processes
for Machine Learning (GPML) by Rasmussen and Williams.</p>
<p>In addition to standard scikit-learn estimator API,
GaussianProcessRegressor:</p>
<blockquote>
<div><ul class="simple">
<li>allows prediction without prior fitting (based on the GP prior)</li>
<li>provides an additional method sample_y(X), which evaluates samples
drawn from the GPR (prior or posterior) at given inputs</li>
<li>exposes a method log_marginal_likelihood(theta), which can be used
externally for other ways of selecting hyperparameters, e.g., via
Markov chain Monte Carlo.</li>
</ul>
</div></blockquote>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.18.</span></p>
</div>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>kernel <span class="classifier-delimiter">:</span> <span class="classifier">kernel object</span></dt>
<dd>The kernel specifying the covariance function of the GP. If None is
passed, the kernel “1.0 * RBF(1.0)” is used as default. Note that
the kernel’s hyperparameters are optimized during fitting.</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float or array-like, optional (default: 1e-10)</span></dt>
<dd>Value added to the diagonal of the kernel matrix during fitting.
Larger values correspond to increased noise level in the observations.
This can also prevent a potential numerical issue during fitting, by
ensuring that the calculated values form a positive definite matrix.
If an array is passed, it must have the same number of entries as the
data used for fitting and is used as datapoint-dependent noise level.
Note that this is equivalent to adding a WhiteKernel with c=alpha.
Allowing to specify the noise level directly as a parameter is mainly
for convenience and for consistency with Ridge.</dd>
<dt>optimizer <span class="classifier-delimiter">:</span> <span class="classifier">string or callable, optional (default: “fmin_l_bfgs_b”)</span></dt>
<dd><p class="first">Can either be one of the internally supported optimizers for optimizing
the kernel’s parameters, specified by a string, or an externally
defined optimizer passed as a callable. If a callable is passed, it
must have the signature:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">optimizer</span><span class="p">(</span><span class="n">obj_func</span><span class="p">,</span> <span class="n">initial_theta</span><span class="p">,</span> <span class="n">bounds</span><span class="p">):</span>

    <span class="o">-</span> <span class="c1"># * &#39;obj_func&#39; is the objective function to be minimized, which</span>
    <span class="o">-</span> <span class="c1">#   takes the hyperparameters theta as parameter and an</span>
    <span class="o">-</span> <span class="c1">#   optional flag eval_gradient, which determines if the</span>
    <span class="o">-</span> <span class="c1">#   gradient is returned additionally to the function value</span>
    <span class="o">-</span> <span class="c1"># * &#39;initial_theta&#39;: the initial value for theta, which can be</span>
    <span class="o">-</span> <span class="c1">#   used by local optimizers</span>
    <span class="o">-</span> <span class="c1"># * &#39;bounds&#39;: the bounds on the values of theta</span>
    <span class="o">-</span> <span class="o">....</span>
    <span class="o">-</span> <span class="c1"># Returned are the best found hyperparameters theta and</span>
    <span class="o">-</span> <span class="c1"># the corresponding value of the target function.</span>
    <span class="o">-</span> <span class="k">return</span> <span class="n">theta_opt</span><span class="p">,</span> <span class="n">func_min</span>
</pre></div>
</div>
<p>Per default, the ‘fmin_l_bfgs_b’ algorithm from scipy.optimize
is used. If None is passed, the kernel’s parameters are kept fixed.
Available internal optimizers are:</p>
<div class="last highlight-default"><div class="highlight"><pre><span></span><span class="s1">&#39;fmin_l_bfgs_b&#39;</span>
</pre></div>
</div>
</dd>
<dt>n_restarts_optimizer <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default: 0)</span></dt>
<dd>The number of restarts of the optimizer for finding the kernel’s
parameters which maximize the log-marginal likelihood. The first run
of the optimizer is performed from the kernel’s initial parameters,
the remaining ones (if any) from thetas sampled log-uniform randomly
from the space of allowed theta-values. If greater than 0, all bounds
must be finite. Note that n_restarts_optimizer == 0 implies that one
run is performed.</dd>
<dt>normalize_y <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default: False)</span></dt>
<dd>Whether the target values y are normalized, i.e., the mean of the
observed target values become zero. This parameter should be set to
True if the target values’ mean is expected to differ considerable from
zero. When enabled, the normalization effectively modifies the GP’s
prior based on the data, which contradicts the likelihood principle;
normalization is thus disabled per default.</dd>
<dt>copy_X_train <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default: True)</span></dt>
<dd>If True, a persistent copy of the training data is stored in the
object. Otherwise, just a reference to the training data is stored,
which might cause predictions to change if the data is modified
externally.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default: None)</span></dt>
<dd>The generator used to initialize the centers. If int, random_state is
the seed used by the random number generator; If RandomState instance,
random_state is the random number generator; If None, the random number
generator is the RandomState instance used by <cite>np.random</cite>.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">X_train_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = (n_samples, n_features)</span></dt>
<dd>Feature values in training data (also required for prediction)</dd>
<dt><code class="docutils literal"><span class="pre">y_train_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = (n_samples, [n_output_dims])</span></dt>
<dd>Target values in training data (also required for prediction)</dd>
<dt><code class="docutils literal"><span class="pre">kernel_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">kernel object</span></dt>
<dd>The kernel used for prediction. The structure of the kernel is the
same as the one passed as parameter but with optimized hyperparameters</dd>
<dt><code class="docutils literal"><span class="pre">L_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = (n_samples, n_samples)</span></dt>
<dd>Lower-triangular Cholesky decomposition of the kernel in <code class="docutils literal"><span class="pre">X_train_</span></code></dd>
<dt><code class="docutils literal"><span class="pre">alpha_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = (n_samples,)</span></dt>
<dd>Dual coefficients of training data points in kernel space</dd>
<dt><code class="docutils literal"><span class="pre">log_marginal_likelihood_value_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The log-marginal-likelihood of <code class="docutils literal"><span class="pre">self.kernel_.theta</span></code></dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_friedman2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.gaussian_process</span> <span class="kn">import</span> <span class="n">GaussianProcessRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.gaussian_process.kernels</span> <span class="kn">import</span> <span class="n">DotProduct</span><span class="p">,</span> <span class="n">WhiteKernel</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_friedman2</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kernel</span> <span class="o">=</span> <span class="n">DotProduct</span><span class="p">()</span> <span class="o">+</span> <span class="n">WhiteKernel</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gpr</span> <span class="o">=</span> <span class="n">GaussianProcessRegressor</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gpr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">0.3680...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gpr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">2</span><span class="p">,:],</span> <span class="n">return_std</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
<span class="go">(array([653.0..., 592.1...]), array([316.6..., 316.6...]))</span>
</pre></div>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#GaussianProcessRegressorScikitsLearnNode">GaussianProcessRegressorScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.KMeansScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">KMeansScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.KMeansScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>K-Means clustering
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.cluster.k_means_.KMeans</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_clusters <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default: 8</span></dt>
<dd>The number of clusters to form as well as the number of
centroids to generate.</dd>
<dt>init <span class="classifier-delimiter">:</span> <span class="classifier">{‘k-means++’, ‘random’ or an ndarray}</span></dt>
<dd><p class="first">Method for initialization, defaults to ‘k-means++’:</p>
<p>‘k-means++’ : selects initial cluster centers for k-mean
clustering in a smart way to speed up convergence. See section
Notes in k_init for more details.</p>
<p>‘random’: choose k observations (rows) at random from data for
the initial centroids.</p>
<p class="last">If an ndarray is passed, it should be of shape (n_clusters, n_features)
and gives the initial centers.</p>
</dd>
<dt>n_init <span class="classifier-delimiter">:</span> <span class="classifier">int, default: 10</span></dt>
<dd>Number of time the k-means algorithm will be run with different
centroid seeds. The final results will be the best output of
n_init consecutive runs in terms of inertia.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, default: 300</span></dt>
<dd>Maximum number of iterations of the k-means algorithm for a
single run.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, default: 1e-4</span></dt>
<dd>Relative tolerance with regards to inertia to declare convergence</dd>
<dt>precompute_distances <span class="classifier-delimiter">:</span> <span class="classifier">{‘auto’, True, False}</span></dt>
<dd><p class="first">Precompute distances (faster but takes more memory).</p>
<p>‘auto’ : do not precompute distances if n_samples * n_clusters &gt; 12
million. This corresponds to about 100MB overhead per job using
double precision.</p>
<p>True : always precompute distances</p>
<p class="last">False : never precompute distances</p>
</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, default 0</span></dt>
<dd>Verbosity mode.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None (default)</span></dt>
<dd>Determines random number generation for centroid initialization. Use
an int to make the randomness deterministic.
See <span class="xref std std-term">Glossary</span>.</dd>
<dt>copy_x <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>When pre-computing distances it is more numerically accurate to center
the data first.  If copy_x is True (default), then the original data is
not modified, ensuring X is C-contiguous.  If False, the original data
is modified, and put back before the function returns, but small
numerical differences may be introduced by subtracting and then adding
the data mean, in this case it will also not ensure that data is
C-contiguous which may cause a significant slowdown.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd><p class="first">The number of jobs to use for the computation. This works by computing
each of the n_init runs in parallel.</p>
<p class="last"><code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p>
</dd>
<dt>algorithm <span class="classifier-delimiter">:</span> <span class="classifier">“auto”, “full” or “elkan”, default=”auto”</span></dt>
<dd>K-means algorithm to use. The classical EM-style algorithm is “full”.
The “elkan” variation is more efficient by using the triangle
inequality, but currently doesn’t support sparse data. “auto” chooses
“elkan” for dense data and “full” for sparse data.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">cluster_centers_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_clusters, n_features]</span></dt>
<dd>Coordinates of cluster centers. If the algorithm stops before fully
converging (see <code class="docutils literal"><span class="pre">tol</span></code> and <code class="docutils literal"><span class="pre">max_iter</span></code>), these will not be
consistent with <code class="docutils literal"><span class="pre">labels_</span></code>.</dd>
</dl>
<p><code class="docutils literal"><span class="pre">labels_</span></code> :</p>
<blockquote>
<div><ul class="simple">
<li>Labels of each point</li>
</ul>
</div></blockquote>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">inertia_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Sum of squared distances of samples to their closest cluster center.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of iterations run.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>              <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span>
<span class="go">array([1, 1, 1, 0, 0, 0], dtype=int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kmeans</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="go">array([1, 0], dtype=int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span>
<span class="go">array([[10.,  2.],</span>
<span class="go">       [ 1.,  2.]])</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>MiniBatchKMeans</dt>
<dd>Alternative online implementation that does incremental updates
of the centers positions using mini-batches.
For large scale learning (say n_samples &gt; 10k) MiniBatchKMeans is
probably much faster than the default batch implementation.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>The k-means problem is solved using either Lloyd’s or Elkan’s algorithm.</p>
<p>The average complexity is given by O(k n T), were n is the number of
samples and T is the number of iteration.</p>
<p>The worst case complexity is given by O(n^(k+2/p)) with
n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii,
‘How slow is the k-means method?’ SoCG2006)</p>
<p>In practice, the k-means algorithm is very fast (one of the fastest
clustering algorithms available), but it falls in local minima. That’s why
it can be useful to restart it several times.</p>
<p>If the algorithm stops before fully converging (because of <code class="docutils literal"><span class="pre">tol</span></code> or
<code class="docutils literal"><span class="pre">max_iter</span></code>), <code class="docutils literal"><span class="pre">labels_</span></code> and <code class="docutils literal"><span class="pre">cluster_centers_</span></code> will not be consistent,
i.e. the <code class="docutils literal"><span class="pre">cluster_centers_</span></code> will not be the means of the points in each
cluster. Also, the estimator will reassign <code class="docutils literal"><span class="pre">labels_</span></code> after the last
iteration to make <code class="docutils literal"><span class="pre">labels_</span></code> consistent with <code class="docutils literal"><span class="pre">predict</span></code> on the training
set.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#KMeansScikitsLearnNode">KMeansScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.OutputCodeClassifierScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">OutputCodeClassifierScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.OutputCodeClassifierScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>(Error-Correcting) Output-Code multiclass strategy
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.multiclass.OutputCodeClassifier</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Output-code based strategies consist in representing each class with a
binary code (an array of 0s and 1s). At fitting time, one binary
classifier per bit in the code book is fitted.  At prediction time, the
classifiers are used to project new points in the class space and the class
closest to the points is chosen. The main advantage of these strategies is
that the number of classifiers used can be controlled by the user, either
for compressing the model (0 &lt; code_size &lt; 1) or for making the model more
robust to errors (code_size &gt; 1). See the documentation for more details.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>estimator <span class="classifier-delimiter">:</span> <span class="classifier">estimator object</span></dt>
<dd>An estimator object implementing <cite>fit</cite> and one of <cite>decision_function</cite>
or <cite>predict_proba</cite>.</dd>
<dt>code_size <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Percentage of the number of classes to be used to create the code book.
A number between 0 and 1 will require fewer classifiers than
one-vs-the-rest. A number greater than 1 will require more classifiers
than one-vs-the-rest.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional, default: None</span></dt>
<dd>The generator used to initialize the codebook.  If int, random_state is
the seed used by the random number generator; If RandomState instance,
random_state is the random number generator; If None, the random number
generator is the RandomState instance used by <cite>np.random</cite>.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The number of jobs to use for the computation.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">estimators_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list of <cite>int(n_classes * code_size)</cite> estimators</span></dt>
<dd>Estimators used for predictions.</dd>
<dt><code class="docutils literal"><span class="pre">classes_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">numpy array of shape [n_classes]</span></dt>
<dd>Array containing labels.</dd>
<dt><code class="docutils literal"><span class="pre">code_book_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">numpy array of shape [n_classes, code_size]</span></dt>
<dd>Binary array containing the code of each class.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id30" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>“Solving multiclass learning problems via error-correcting output
codes”,
Dietterich T., Bakiri G.,
Journal of Artificial Intelligence Research 2,
1995.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id31" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>“The error coding method and PICTs”,
James G., Hastie T.,
Journal of Computational and Graphical statistics 7,
1998.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id32" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td>“The Elements of Statistical Learning”,
Hastie T., Tibshirani R., Friedman J., page 606 (second-edition)
2008.</td></tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#OutputCodeClassifierScikitsLearnNode">OutputCodeClassifierScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.PassiveAggressiveRegressorScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">PassiveAggressiveRegressorScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.PassiveAggressiveRegressorScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Passive Aggressive Regressor
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.passive_aggressive.PassiveAggressiveRegressor</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>C <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Maximum step size (regularization). Defaults to 1.0.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Whether the intercept should be estimated or not. If False, the
data is assumed to be already centered. Defaults to True.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd><p class="first">The maximum number of passes over the training data (aka epochs).
It only impacts the behavior in the <code class="docutils literal"><span class="pre">fit</span></code> method, and not the
<cite>partial_fit</cite>.
Defaults to 5. Defaults to 1000 from 0.21, or if tol is not None.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19.</span></p>
</div>
</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float or None, optional</span></dt>
<dd><p class="first">The stopping criterion. If it is not None, the iterations will stop
when (loss &gt; previous_loss - tol). Defaults to None.
Defaults to 1e-3 from 0.21.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19.</span></p>
</div>
</dd>
<dt>early_stopping <span class="classifier-delimiter">:</span> <span class="classifier">bool, default=False</span></dt>
<dd><p class="first">Whether to use early stopping to terminate training when validation.
score is not improving. If set to True, it will automatically set aside
a fraction of training data as validation and terminate training when
validation score is not improving by at least tol for
n_iter_no_change consecutive epochs.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
<dt>validation_fraction <span class="classifier-delimiter">:</span> <span class="classifier">float, default=0.1</span></dt>
<dd><p class="first">The proportion of training data to set aside as validation set for
early stopping. Must be between 0 and 1.
Only used if early_stopping is True.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
<dt>n_iter_no_change <span class="classifier-delimiter">:</span> <span class="classifier">int, default=5</span></dt>
<dd><p class="first">Number of iterations with no improvement to wait before early stopping.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
<dt>shuffle <span class="classifier-delimiter">:</span> <span class="classifier">bool, default=True</span></dt>
<dd>Whether or not the training data should be shuffled after each epoch.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span></dt>
<dd>The verbosity level</dd>
<dt>loss <span class="classifier-delimiter">:</span> <span class="classifier">string, optional</span></dt>
<dd><p class="first">The loss function to be used:</p>
<ul class="last simple">
<li>epsilon_insensitive: equivalent to PA-I in the reference paper.</li>
<li>squared_epsilon_insensitive: equivalent to PA-II in the reference</li>
<li>paper.</li>
</ul>
</dd>
<dt>epsilon <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>If the difference between the current prediction and the correct label
is below this threshold, the model is not updated.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional, default=None</span></dt>
<dd>The seed of the pseudo random number generator to use when shuffling
the data.  If int, random_state is the seed used by the random number
generator; If RandomState instance, random_state is the random number
generator; If None, the random number generator is the RandomState
instance used by <cite>np.random</cite>.</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd><p class="first">When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.
See <span class="xref std std-term">the Glossary</span>.</p>
<p class="last">Repeatedly calling fit or partial_fit when warm_start is True can
result in a different solution than when calling fit a single time
because of the way the data is shuffled.</p>
</dd>
<dt>average <span class="classifier-delimiter">:</span> <span class="classifier">bool or int, optional</span></dt>
<dd><p class="first">When set to True, computes the averaged SGD weights and stores the
result in the <code class="docutils literal"><span class="pre">coef_</span></code> attribute. If set to an int greater than 1,
averaging will begin once the total number of samples seen reaches
average. So average=10 will begin averaging after seeing 10 samples.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19: </span>parameter <em>average</em> to use weights averaging in SGD</p>
</div>
</dd>
<dt>n_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd><p class="first">The number of passes over the training data (aka epochs).
Defaults to None. Deprecated, will be removed in 0.21.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.19: </span>Deprecated</p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]</span></dt>
<dd>Weights assigned to the features.</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1] if n_classes == 2 else [n_classes]</span></dt>
<dd>Constants in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The actual number of iterations to reach the stopping criterion.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">PassiveAggressiveRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span> <span class="o">=</span> <span class="n">PassiveAggressiveRegressor</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span><span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">PassiveAggressiveRegressor(C=1.0, average=False, early_stopping=False,</span>
<span class="go">              epsilon=0.1, fit_intercept=True, loss=&#39;epsilon_insensitive&#39;,</span>
<span class="go">              max_iter=100, n_iter=None, n_iter_no_change=5,</span>
<span class="go">              random_state=0, shuffle=True, tol=0.001,</span>
<span class="go">              validation_fraction=0.1, verbose=0, warm_start=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">[20.48736655 34.18818427 67.59122734 87.94731329]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="go">[-0.02306214]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>
<span class="go">[-0.02306214]</span>
</pre></div>
</div>
<p>See also</p>
<p>SGDRegressor</p>
<p><strong>References</strong></p>
<p>Online Passive-Aggressive Algorithms
&lt;<a class="reference external" href="http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf">http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf</a>&gt;
K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#PassiveAggressiveRegressorScikitsLearnNode">PassiveAggressiveRegressorScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.RandomForestClassifierScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">RandomForestClassifierScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.RandomForestClassifierScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>A random forest classifier.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.ensemble.forest.RandomForestClassifier</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
A random forest is a meta estimator that fits a number of decision tree
classifiers on various sub-samples of the dataset and uses averaging to
improve the predictive accuracy and control over-fitting.
The sub-sample size is always the same as the original
input sample size but the samples are drawn with replacement if
<cite>bootstrap=True</cite> (default).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_estimators <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=10)</span></dt>
<dd><p class="first">The number of trees in the forest.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.20: </span>The default value of <code class="docutils literal"><span class="pre">n_estimators</span></code> will change from 10 in
version 0.20 to 100 in version 0.22.</p>
</div>
</dd>
<dt>criterion <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=”gini”)</span></dt>
<dd>The function to measure the quality of a split. Supported criteria are
“gini” for the Gini impurity and “entropy” for the information gain.
Note: this parameter is tree-specific.</dd>
<dt>max_depth <span class="classifier-delimiter">:</span> <span class="classifier">integer or None, optional (default=None)</span></dt>
<dd>The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</dd>
<dt>min_samples_split <span class="classifier-delimiter">:</span> <span class="classifier">int, float, optional (default=2)</span></dt>
<dd><p class="first">The minimum number of samples required to split an internal node:</p>
<ul class="simple">
<li>If int, then consider <cite>min_samples_split</cite> as the minimum number.</li>
<li>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</li>
</ul>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</dd>
<dt>min_samples_leaf <span class="classifier-delimiter">:</span> <span class="classifier">int, float, optional (default=1)</span></dt>
<dd><p class="first">The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul class="simple">
<li>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</li>
<li>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</li>
</ul>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</dd>
<dt>min_weight_fraction_leaf <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span></dt>
<dd>The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int, float, string or None, optional (default=”auto”)</span></dt>
<dd><p class="first">The number of features to consider when looking for the best split:</p>
<ul class="simple">
<li>If int, then consider <cite>max_features</cite> features at each split.</li>
<li>If float, then <cite>max_features</cite> is a fraction and
<cite>int(max_features * n_features)</cite> features are considered at each
split.</li>
<li>If “auto”, then <cite>max_features=sqrt(n_features)</cite>.</li>
<li>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite> (same as “auto”).</li>
<li>If “log2”, then <cite>max_features=log2(n_features)</cite>.</li>
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
<p class="last">Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal"><span class="pre">max_features</span></code> features.</p>
</dd>
<dt>max_leaf_nodes <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Grow trees with <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</dd>
<dt>min_impurity_decrease <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span></dt>
<dd><p class="first">A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal"><span class="pre">N</span></code>, <code class="docutils literal"><span class="pre">N_t</span></code>, <code class="docutils literal"><span class="pre">N_t_R</span></code> and <code class="docutils literal"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19.</span></p>
</div>
</dd>
<dt>min_impurity_split <span class="classifier-delimiter">:</span> <span class="classifier">float, (default=1e-7)</span></dt>
<dd><p class="first">Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="last deprecated">
<p><span class="versionmodified">Deprecated since version 0.19: </span><code class="docutils literal"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal"><span class="pre">min_impurity_split</span></code> will change from 1e-7 to 0 in 0.23 and it
will be removed in 0.25. Use <code class="docutils literal"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</dd>
<dt>bootstrap <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span></dt>
<dd>Whether bootstrap samples are used when building trees. If False, the
whole datset is used to build each tree.</dd>
<dt>oob_score <span class="classifier-delimiter">:</span> <span class="classifier">bool (default=False)</span></dt>
<dd>Whether to use out-of-bag samples to estimate
the generalization accuracy.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The number of jobs to run in parallel for both <cite>fit</cite> and <cite>predict</cite>.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=0)</span></dt>
<dd>Controls the verbosity when fitting and predicting.</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default=False)</span></dt>
<dd>When set to <code class="docutils literal"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit a whole
new forest. See <span class="xref std std-term">the Glossary</span>.</dd>
<dt>class_weight <span class="classifier-delimiter">:</span> <span class="classifier">dict, list of dicts, “balanced”, “balanced_subsample” or     None, optional (default=None)</span></dt>
<dd><p class="first">Weights associated with classes in the form <code class="docutils literal"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
If not given, all classes are supposed to have weight one. For
multi-output problems, a list of dicts can be provided in the same
order as the columns of y.</p>
<p>Note that for multioutput (including multilabel) weights should be
defined for each class of every column in its own dict. For example,
for four-class multilabel classification weights should be
[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
[{1:1}, {2:5}, {3:1}, {4:1}].</p>
<p>The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
<p>The “balanced_subsample” mode is the same as “balanced” except that
weights are computed based on the bootstrap sample for every tree
grown.</p>
<p>For multi-output, the weights of each column of y will be multiplied.</p>
<p class="last">Note that these weights will be multiplied with sample_weight (passed
through the fit method) if sample_weight is specified.</p>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">estimators_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list of DecisionTreeClassifier</span></dt>
<dd>The collection of fitted sub-estimators.</dd>
<dt><code class="docutils literal"><span class="pre">classes_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_classes] or a list of such arrays</span></dt>
<dd>The classes labels (single output problem), or a list of arrays of
class labels (multi-output problem).</dd>
<dt><code class="docutils literal"><span class="pre">n_classes_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int or list</span></dt>
<dd>The number of classes (single output problem), or a list containing the
number of classes for each output (multi-output problem).</dd>
<dt><code class="docutils literal"><span class="pre">n_features_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The number of features when <code class="docutils literal"><span class="pre">fit</span></code> is performed.</dd>
<dt><code class="docutils literal"><span class="pre">n_outputs_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The number of outputs when <code class="docutils literal"><span class="pre">fit</span></code> is performed.</dd>
<dt><code class="docutils literal"><span class="pre">feature_importances_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span></dt>
<dd>The feature importances (the higher, the more important the feature).</dd>
<dt><code class="docutils literal"><span class="pre">oob_score_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Score of the training dataset obtained using an out-of-bag estimate.</dd>
<dt><code class="docutils literal"><span class="pre">oob_decision_function_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_samples, n_classes]</span></dt>
<dd>Decision function computed with out-of-bag estimate on the training
set. If n_estimators is small it might be possible that a data point
was never left out during the bootstrap. In this case,
<cite>oob_decision_function_</cite> might contain NaN.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_redundant</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>                             <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;,</span>
<span class="go">            max_depth=2, max_features=&#39;auto&#39;, max_leaf_nodes=None,</span>
<span class="go">            min_impurity_decrease=0.0, min_impurity_split=None,</span>
<span class="go">            min_samples_leaf=1, min_samples_split=2,</span>
<span class="go">            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,</span>
<span class="go">            oob_score=False, random_state=0, verbose=0, warm_start=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">)</span>
<span class="go">[0.14205973 0.76664038 0.0282433  0.06305659]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code class="docutils literal"><span class="pre">max_depth</span></code>, <code class="docutils literal"><span class="pre">min_samples_leaf</span></code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<p>The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data,
<code class="docutils literal"><span class="pre">max_features=n_features</span></code> and <code class="docutils literal"><span class="pre">bootstrap=False</span></code>, if the improvement
of the criterion is identical for several splits enumerated during the
search of the best split. To obtain a deterministic behaviour during
fitting, <code class="docutils literal"><span class="pre">random_state</span></code> has to be fixed.</p>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id33" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><ol class="first last upperalpha simple" start="12">
<li>Breiman, “Random Forests”, Machine Learning, 45(1), 5-32, 2001.</li>
</ol>
</td></tr>
</tbody>
</table>
<p>See also</p>
<p>DecisionTreeClassifier, ExtraTreesClassifier</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#RandomForestClassifierScikitsLearnNode">RandomForestClassifierScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.ForestRegressorScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">ForestRegressorScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.ForestRegressorScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Base class for forest of trees-based regressors.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.ensemble.forest.ForestRegressor</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Warning: This class should not be used directly. Use derived classes
instead.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#ForestRegressorScikitsLearnNode">ForestRegressorScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.RidgeScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">RidgeScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.RidgeScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Linear least squares with l2 regularization.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.ridge.Ridge</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Minimizes the objective function:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2_2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||^</span><span class="mi">2_2</span>
</pre></div>
</div>
<p>This model solves a regression model where the loss function is
the linear least squares function and regularization is given by
the l2-norm. Also known as Ridge Regression or Tikhonov regularization.
This estimator has built-in support for multi-variate regression
(i.e., when y is a 2d-array of shape [n_samples, n_targets]).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">{float, array-like}, shape (n_targets)</span></dt>
<dd>Regularization strength; must be a positive float. Regularization
improves the conditioning of the problem and reduces the variance of
the estimates. Larger values specify stronger regularization.
Alpha corresponds to <code class="docutils literal"><span class="pre">C^-1</span></code> in other linear models such as
LogisticRegression or LinearSVC. If an array is passed, penalties are
assumed to be specific to the targets. Hence they must correspond in
number.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span></dt>
<dd>This parameter is ignored when <code class="docutils literal"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<code class="xref py py-class docutils literal"><span class="pre">sklearn.preprocessing.StandardScaler</span></code> before calling <code class="docutils literal"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal"><span class="pre">normalize=False</span></code>.</dd>
<dt>copy_X <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>If True, X will be copied; else, it may be overwritten.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Maximum number of iterations for conjugate gradient solver.
For ‘sparse_cg’ and ‘lsqr’ solvers, the default value is determined
by scipy.sparse.linalg. For ‘sag’ solver, the default value is 1000.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Precision of the solution.</dd>
<dt>solver <span class="classifier-delimiter">:</span> <span class="classifier">{‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’, ‘sag’, ‘saga’}</span></dt>
<dd><p class="first">Solver to use in the computational routines:</p>
<ul class="simple">
<li>‘auto’ chooses the solver automatically based on the type of data.</li>
<li>‘svd’ uses a Singular Value Decomposition of X to compute the Ridge
coefficients. More stable for singular matrices than
‘cholesky’.</li>
<li>‘cholesky’ uses the standard scipy.linalg.solve function to
obtain a closed-form solution.</li>
<li>‘sparse_cg’ uses the conjugate gradient solver as found in
scipy.sparse.linalg.cg. As an iterative algorithm, this solver is
more appropriate than ‘cholesky’ for large-scale data
(possibility to set <cite>tol</cite> and <cite>max_iter</cite>).</li>
<li>‘lsqr’ uses the dedicated regularized least-squares routine
scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative
procedure.</li>
<li>‘sag’ uses a Stochastic Average Gradient descent, and ‘saga’ uses
its improved, unbiased version named SAGA. Both methods also use an
iterative procedure, and are often faster than other solvers when
both n_samples and n_features are large. Note that ‘sag’ and
‘saga’ fast convergence is only guaranteed on features with
approximately the same scale. You can preprocess the data with a
scaler from sklearn.preprocessing.</li>
</ul>
<p>All last five solvers support both dense and sparse data. However,
only ‘sag’ and ‘saga’ supports sparse input when <cite>fit_intercept</cite> is
True.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17: </span>Stochastic Average Gradient descent solver.</p>
</div>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19: </span>SAGA solver.</p>
</div>
</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional, default None</span></dt>
<dd><p class="first">The seed of the pseudo random number generator to use when shuffling
the data.  If int, random_state is the seed used by the random number
generator; If RandomState instance, random_state is the random number
generator; If None, the random number generator is the RandomState
instance used by <cite>np.random</cite>. Used when <code class="docutils literal"><span class="pre">solver</span></code> == ‘sag’.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>random_state</em> to support Stochastic Average Gradient.</p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,) or (n_targets, n_features)</span></dt>
<dd>Weight vector(s).</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float | array, shape = (n_targets,)</span></dt>
<dd>Independent term in decision function. Set to 0.0 if
<code class="docutils literal"><span class="pre">fit_intercept</span> <span class="pre">=</span> <span class="pre">False</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array or None, shape (n_targets,)</span></dt>
<dd><p class="first">Actual number of iterations for each target. Available only for
sag and lsqr solvers. Other solvers will return None.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17.</span></p>
</div>
</dd>
</dl>
<p>See also</p>
<p>RidgeClassifier : Ridge classifier
RidgeCV : Ridge regression with built-in cross validation
<code class="xref py py-class docutils literal"><span class="pre">sklearn.kernel_ridge.KernelRidge</span></code> : Kernel ridge regression</p>
<blockquote>
<div>combines ridge regression with the kernel trick</div></blockquote>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,</span>
<span class="go">      normalize=False, random_state=None, solver=&#39;auto&#39;, tol=0.001)</span>
</pre></div>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.RidgeScikitsLearnNode-class.html">RidgeScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.ElasticNetScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">ElasticNetScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.ElasticNetScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Linear regression with combined L1 and L2 priors as regularizer.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.coordinate_descent.ElasticNet</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Minimizes the objective function:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2_2</span>
<span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
<span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||^</span><span class="mi">2_2</span>
</pre></div>
</div>
<p>If you are interested in controlling the L1 and L2 penalty
separately, keep in mind that this is equivalent to:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">*</span> <span class="n">L1</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">L2</span>
</pre></div>
</div>
<p>where:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="ow">and</span> <span class="n">l1_ratio</span> <span class="o">=</span> <span class="n">a</span> <span class="o">/</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
<p>The parameter l1_ratio corresponds to alpha in the glmnet R package while
alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio
= 1 is the lasso penalty. Currently, l1_ratio &lt;= 0.01 is not reliable,
unless you supply your own sequence of alpha.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Constant that multiplies the penalty terms. Defaults to 1.0.
See the notes for the exact mathematical meaning of this
parameter.``alpha = 0`` is equivalent to an ordinary least square,
solved by the <code class="xref py py-class docutils literal"><span class="pre">LinearRegression</span></code> object. For numerical
reasons, using <code class="docutils literal"><span class="pre">alpha</span> <span class="pre">=</span> <span class="pre">0</span></code> with the <code class="docutils literal"><span class="pre">Lasso</span></code> object is not advised.
Given this, you should use the <code class="xref py py-class docutils literal"><span class="pre">LinearRegression</span></code> object.</dd>
<dt>l1_ratio <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The ElasticNet mixing parameter, with <code class="docutils literal"><span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">l1_ratio</span> <span class="pre">&lt;=</span> <span class="pre">1</span></code>. For
<code class="docutils literal"><span class="pre">l1_ratio</span> <span class="pre">=</span> <span class="pre">0</span></code> the penalty is an L2 penalty. <code class="docutils literal"><span class="pre">For</span> <span class="pre">l1_ratio</span> <span class="pre">=</span> <span class="pre">1</span></code> it
is an L1 penalty.  For <code class="docutils literal"><span class="pre">0</span> <span class="pre">&lt;</span> <span class="pre">l1_ratio</span> <span class="pre">&lt;</span> <span class="pre">1</span></code>, the penalty is a
combination of L1 and L2.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Whether the intercept should be estimated or not. If <code class="docutils literal"><span class="pre">False</span></code>, the
data is assumed to be already centered.</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span></dt>
<dd>This parameter is ignored when <code class="docutils literal"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<code class="xref py py-class docutils literal"><span class="pre">sklearn.preprocessing.StandardScaler</span></code> before calling <code class="docutils literal"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal"><span class="pre">normalize=False</span></code>.</dd>
<dt>precompute <span class="classifier-delimiter">:</span> <span class="classifier">True | False | array-like</span></dt>
<dd>Whether to use a precomputed Gram matrix to speed up
calculations. The Gram matrix can also be passed as argument.
For sparse input this option is always <code class="docutils literal"><span class="pre">True</span></code> to preserve sparsity.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>The maximum number of iterations</dd>
<dt>copy_X <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>If <code class="docutils literal"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The tolerance for the optimization: if the updates are
smaller than <code class="docutils literal"><span class="pre">tol</span></code>, the optimization code checks the
dual gap for optimality and continues until it is smaller
than <code class="docutils literal"><span class="pre">tol</span></code>.</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd>When set to <code class="docutils literal"><span class="pre">True</span></code>, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.
See <span class="xref std std-term">the Glossary</span>.</dd>
<dt>positive <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd>When set to <code class="docutils literal"><span class="pre">True</span></code>, forces the coefficients to be positive.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional, default None</span></dt>
<dd>The seed of the pseudo random number generator that selects a random
feature to update.  If int, random_state is the seed used by the random
number generator; If RandomState instance, random_state is the random
number generator; If None, the random number generator is the
RandomState instance used by <cite>np.random</cite>. Used when <code class="docutils literal"><span class="pre">selection</span></code> ==
‘random’.</dd>
<dt>selection <span class="classifier-delimiter">:</span> <span class="classifier">str, default ‘cyclic’</span></dt>
<dd>If set to ‘random’, a random coefficient is updated every iteration
rather than looping over features sequentially by default. This
(setting to ‘random’) often leads to significantly faster convergence
especially when tol is higher than 1e-4.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,) | (n_targets, n_features)</span></dt>
<dd>parameter vector (w in the cost function formula)</dd>
<dt><code class="docutils literal"><span class="pre">sparse_coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">scipy.sparse matrix, shape (n_features, 1) |             (n_targets, n_features)</span></dt>
<dd><code class="docutils literal"><span class="pre">sparse_coef_</span></code> is a readonly property derived from <code class="docutils literal"><span class="pre">coef_</span></code></dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float | array, shape (n_targets,)</span></dt>
<dd>independent term in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_targets,)</span></dt>
<dd>number of iterations run by the coordinate descent solver to reach
the specified tolerance.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNet</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span> <span class="o">=</span> <span class="n">ElasticNet</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,</span>
<span class="go">      max_iter=1000, normalize=False, positive=False, precompute=False,</span>
<span class="go">      random_state=0, selection=&#39;cyclic&#39;, tol=0.0001, warm_start=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span> 
<span class="go">[18.83816048 64.55968825]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span> 
<span class="go">1.451...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span> 
<span class="go">[1.451...]</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.</p>
<p>See also</p>
<dl class="docutils">
<dt>ElasticNetCV <span class="classifier-delimiter">:</span> <span class="classifier">Elastic net model with best model selection by</span></dt>
<dd>cross-validation.</dd>
</dl>
<p>SGDRegressor: implements elastic net regression with incremental training.
SGDClassifier: implements logistic regression with elastic net penalty</p>
<blockquote>
<div>(<code class="docutils literal"><span class="pre">SGDClassifier(loss=&quot;log&quot;,</span> <span class="pre">penalty=&quot;elasticnet&quot;)</span></code>).</div></blockquote>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.ElasticNetScikitsLearnNode-class.html">ElasticNetScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.IsomapScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">IsomapScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.IsomapScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Isomap Embedding
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.manifold.isomap.Isomap</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Non-linear dimensionality reduction through Isometric Mapping</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_neighbors <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd>number of neighbors to consider for each point.</dd>
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd>number of coordinates for the manifold</dd>
<dt>eigen_solver <span class="classifier-delimiter">:</span> <span class="classifier">[‘auto’|’arpack’|’dense’]</span></dt>
<dd><p class="first">‘auto’ : Attempt to choose the most efficient solver
for the given problem.</p>
<p>‘arpack’ : Use Arnoldi decomposition to find the eigenvalues
and eigenvectors.</p>
<p class="last">‘dense’ : Use a direct solver (i.e. LAPACK)
for the eigenvalue decomposition.</p>
</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Convergence tolerance passed to arpack or lobpcg.
not used if eigen_solver == ‘dense’.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd>Maximum number of iterations for the arpack solver.
not used if eigen_solver == ‘dense’.</dd>
<dt>path_method <span class="classifier-delimiter">:</span> <span class="classifier">string [‘auto’|’FW’|’D’]</span></dt>
<dd><p class="first">Method to use in finding shortest path.</p>
<p>‘auto’ : attempt to choose the best algorithm automatically.</p>
<p>‘FW’ : Floyd-Warshall algorithm.</p>
<p class="last">‘D’ : Dijkstra’s algorithm.</p>
</dd>
<dt>neighbors_algorithm <span class="classifier-delimiter">:</span> <span class="classifier">string [‘auto’|’brute’|’kd_tree’|’ball_tree’]</span></dt>
<dd>Algorithm to use for nearest neighbors search,
passed to neighbors.NearestNeighbors instance.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The number of parallel jobs to run.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">embedding_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_samples, n_components)</span></dt>
<dd>Stores the embedding vectors.</dd>
<dt><code class="docutils literal"><span class="pre">kernel_pca_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">object</span></dt>
<dd><cite>KernelPCA</cite> object used to implement the embedding.</dd>
<dt><code class="docutils literal"><span class="pre">training_data_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_samples, n_features)</span></dt>
<dd>Stores the training data.</dd>
<dt><code class="docutils literal"><span class="pre">nbrs_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">sklearn.neighbors.NearestNeighbors instance</span></dt>
<dd>Stores nearest neighbors instance, including BallTree or KDtree
if applicable.</dd>
<dt><code class="docutils literal"><span class="pre">dist_matrix_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_samples, n_samples)</span></dt>
<dd>Stores the geodesic distance matrix of training data.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">Isomap</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(1797, 64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span> <span class="o">=</span> <span class="n">Isomap</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_transformed</span> <span class="o">=</span> <span class="n">embedding</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">100</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_transformed</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(100, 2)</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id34" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Tenenbaum, J.B.; De Silva, V.; &amp; Langford, J.C. A global geometric
framework for nonlinear dimensionality reduction. Science 290 (5500)</td></tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#IsomapScikitsLearnNode">IsomapScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.BinarizerScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">BinarizerScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.BinarizerScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Binarize data (set feature values to 0 or 1) according to a threshold
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.preprocessing.data.Binarizer</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Values greater than the threshold map to 1, while values less than
or equal to the threshold map to 0. With the default threshold of 0,
only positive values map to 1.</p>
<p>Binarization is a common operation on text count data where the
analyst can decide to only consider the presence or absence of a
feature rather than a quantified number of occurrences for instance.</p>
<p>It can also be used as a pre-processing step for estimators that
consider boolean random variables (e.g. modelled using the Bernoulli
distribution in a Bayesian setting).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>threshold <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (0.0 by default)</span></dt>
<dd>Feature values below or equal to this are replaced by 0, above it by 1.
Threshold may not be less than 0 for operations on sparse matrices.</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>set to False to perform inplace binarization and avoid a copy (if
the input is already a numpy array or a scipy.sparse CSR matrix).</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">Binarizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span> <span class="mf">2.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span> <span class="o">=</span> <span class="n">Binarizer</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1"># fit does nothing.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span>
<span class="go">Binarizer(copy=True, threshold=0.0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([[1., 0., 1.],</span>
<span class="go">       [1., 0., 0.],</span>
<span class="go">       [0., 1., 0.]])</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>If the input is a sparse matrix, only the non-zero values are subject
to update by the Binarizer class.</p>
<p>This estimator is stateless (besides constructor parameters), the
fit method does nothing but is useful when used in a pipeline.</p>
<p>See also</p>
<p>binarize: Equivalent function without the estimator API.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.BinarizerScikitsLearnNode-class.html">BinarizerScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.MiniBatchDictionaryLearningScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">MiniBatchDictionaryLearningScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.MiniBatchDictionaryLearningScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Mini-batch dictionary learning
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.decomposition.dict_learning.MiniBatchDictionaryLearning</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Finds a dictionary (a set of atoms) that can best be used to represent data
using a sparse code.</p>
<p>Solves the optimization problem:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">U</span><span class="o">^*</span><span class="p">,</span><span class="n">V</span><span class="o">^*</span><span class="p">)</span> <span class="o">=</span> <span class="n">argmin</span> <span class="mf">0.5</span> <span class="o">||</span> <span class="n">Y</span> <span class="o">-</span> <span class="n">U</span> <span class="n">V</span> <span class="o">||</span><span class="n">_2</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span> <span class="n">U</span> <span class="o">||</span><span class="n">_1</span>
             <span class="p">(</span><span class="n">U</span><span class="p">,</span><span class="n">V</span><span class="p">)</span>
             <span class="k">with</span> <span class="o">||</span> <span class="n">V_k</span> <span class="o">||</span><span class="n">_2</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">for</span> <span class="nb">all</span>  <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">n_components</span>
</pre></div>
</div>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>number of dictionary elements to extract</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float,</span></dt>
<dd>sparsity controlling parameter</dd>
<dt>n_iter <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>total number of iterations to perform</dd>
<dt>fit_algorithm <span class="classifier-delimiter">:</span> <span class="classifier">{‘lars’, ‘cd’}</span></dt>
<dd>lars: uses the least angle regression method to solve the lasso problem
(linear_model.lars_path)
cd: uses the coordinate descent method to compute the
Lasso solution (linear_model.Lasso). Lars will be faster if
the estimated components are sparse.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Number of parallel jobs to run.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>batch_size <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>number of samples in each mini-batch</dd>
<dt>shuffle <span class="classifier-delimiter">:</span> <span class="classifier">bool,</span></dt>
<dd>whether to shuffle the samples before forming batches</dd>
<dt>dict_init <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_components, n_features),</span></dt>
<dd>initial value of the dictionary for warm restart scenarios</dd>
<dt>transform_algorithm <span class="classifier-delimiter">:</span> <span class="classifier">{‘lasso_lars’, ‘lasso_cd’, ‘lars’, ‘omp’,     ‘threshold’}</span></dt>
<dd>Algorithm used to transform the data.
lars: uses the least angle regression method (linear_model.lars_path)
lasso_lars: uses Lars to compute the Lasso solution
lasso_cd: uses the coordinate descent method to compute the
Lasso solution (linear_model.Lasso). lasso_lars will be faster if
the estimated components are sparse.
omp: uses orthogonal matching pursuit to estimate the sparse solution
threshold: squashes to zero all coefficients less than alpha from
the projection dictionary * X’</dd>
<dt>transform_n_nonzero_coefs <span class="classifier-delimiter">:</span> <span class="classifier">int, <code class="docutils literal"><span class="pre">0.1</span> <span class="pre">*</span> <span class="pre">n_features</span></code> by default</span></dt>
<dd>Number of nonzero coefficients to target in each column of the
solution. This is only used by <cite>algorithm=’lars’</cite> and <cite>algorithm=’omp’</cite>
and is overridden by <cite>alpha</cite> in the <cite>omp</cite> case.</dd>
<dt>transform_alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, 1. by default</span></dt>
<dd>If <cite>algorithm=’lasso_lars’</cite> or <cite>algorithm=’lasso_cd’</cite>, <cite>alpha</cite> is the
penalty applied to the L1 norm.
If <cite>algorithm=’threshold’</cite>, <cite>alpha</cite> is the absolute value of the
threshold below which coefficients will be squashed to zero.
If <cite>algorithm=’omp’</cite>, <cite>alpha</cite> is the tolerance parameter: the value of
the reconstruction error targeted. In this case, it overrides
<cite>n_nonzero_coefs</cite>.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default: False)</span></dt>
<dd>To control the verbosity of the procedure.</dd>
<dt>split_sign <span class="classifier-delimiter">:</span> <span class="classifier">bool, False by default</span></dt>
<dd>Whether to split the sparse feature vector into the concatenation of
its negative part and its positive part. This can improve the
performance of downstream classifiers.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>positive_code <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd><p class="first">Whether to enforce positivity when finding the code.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
<dt>positive_dict <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd><p class="first">Whether to enforce positivity when finding the dictionary.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">components_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>components extracted from the data</dd>
<dt><code class="docutils literal"><span class="pre">inner_stats_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">tuple of (A, B) ndarrays</span></dt>
<dd>Internal sufficient statistics that are kept by the algorithm.
Keeping them is useful in online settings, to avoid loosing the
history of the evolution, but they shouldn’t have any use for the
end user.
A (n_components, n_components) is the dictionary covariance matrix.
B (n_features, n_components) is the data approximation matrix</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of iterations run.</dd>
</dl>
<p><strong>Notes</strong></p>
<p><strong>References:</strong></p>
<p>J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning
for sparse coding (<a class="reference external" href="http://www.di.ens.fr/sierra/pdfs/icml09.pdf">http://www.di.ens.fr/sierra/pdfs/icml09.pdf</a>)</p>
<p>See also</p>
<p>SparseCoder
DictionaryLearning
SparsePCA
MiniBatchSparsePCA</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#MiniBatchDictionaryLearningScikitsLearnNode">MiniBatchDictionaryLearningScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.TfidfVectorizerScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">TfidfVectorizerScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.TfidfVectorizerScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert a collection of raw documents to a matrix of TF-IDF features.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.feature_extraction.text.TfidfVectorizer</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Equivalent to <code class="xref py py-class docutils literal"><span class="pre">CountVectorizer</span></code> followed by
<code class="xref py py-class docutils literal"><span class="pre">TfidfTransformer</span></code>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>input <span class="classifier-delimiter">:</span> <span class="classifier">string {‘filename’, ‘file’, ‘content’}</span></dt>
<dd><p class="first">If ‘filename’, the sequence passed as an argument to fit is
expected to be a list of filenames that need reading to fetch
the raw content to analyze.</p>
<p>If ‘file’, the sequence items must have a ‘read’ method (file-like
object) that is called to fetch the bytes in memory.</p>
<p class="last">Otherwise the input is expected to be the sequence strings or
bytes items are expected to be analyzed directly.</p>
</dd>
<dt>encoding <span class="classifier-delimiter">:</span> <span class="classifier">string, ‘utf-8’ by default.</span></dt>
<dd>If bytes or files are given to analyze, this encoding is used to
decode.</dd>
<dt>decode_error <span class="classifier-delimiter">:</span> <span class="classifier">{‘strict’, ‘ignore’, ‘replace’} (default=’strict’)</span></dt>
<dd>Instruction on what to do if a byte sequence is given to analyze that
contains characters not of the given <cite>encoding</cite>. By default, it is
‘strict’, meaning that a UnicodeDecodeError will be raised. Other
values are ‘ignore’ and ‘replace’.</dd>
<dt>strip_accents <span class="classifier-delimiter">:</span> <span class="classifier">{‘ascii’, ‘unicode’, None} (default=None)</span></dt>
<dd><p class="first">Remove accents and perform other character normalization
during the preprocessing step.
‘ascii’ is a fast method that only works on characters that have
an direct ASCII mapping.
‘unicode’ is a slightly slower method that works on any characters.
None (default) does nothing.</p>
<p class="last">Both ‘ascii’ and ‘unicode’ use NFKD normalization from
<code class="xref py py-func docutils literal"><span class="pre">unicodedata.normalize()</span></code>.</p>
</dd>
<dt>lowercase <span class="classifier-delimiter">:</span> <span class="classifier">boolean (default=True)</span></dt>
<dd>Convert all characters to lowercase before tokenizing.</dd>
<dt>preprocessor <span class="classifier-delimiter">:</span> <span class="classifier">callable or None (default=None)</span></dt>
<dd>Override the preprocessing (string transformation) stage while
preserving the tokenizing and n-grams generation steps.</dd>
<dt>tokenizer <span class="classifier-delimiter">:</span> <span class="classifier">callable or None (default=None)</span></dt>
<dd>Override the string tokenization step while preserving the
preprocessing and n-grams generation steps.
Only applies if <code class="docutils literal"><span class="pre">analyzer</span> <span class="pre">==</span> <span class="pre">'word'</span></code>.</dd>
<dt>analyzer <span class="classifier-delimiter">:</span> <span class="classifier">string, {‘word’, ‘char’, ‘char_wb’} or callable</span></dt>
<dd><p class="first">Whether the feature should be made of word or character n-grams.
Option ‘char_wb’ creates character n-grams only from text inside
word boundaries; n-grams at the edges of words are padded with space.</p>
<p class="last">If a callable is passed it is used to extract the sequence of features
out of the raw, unprocessed input.</p>
</dd>
<dt>stop_words <span class="classifier-delimiter">:</span> <span class="classifier">string {‘english’}, list, or None (default=None)</span></dt>
<dd><p class="first">If a string, it is passed to _check_stop_list and the appropriate stop
list is returned. ‘english’ is currently the only supported string
value.
There are several known issues with ‘english’ and you should
consider an alternative (see <span class="xref std std-ref">stop_words</span>).</p>
<p>If a list, that list is assumed to contain stop words, all of which
will be removed from the resulting tokens.
Only applies if <code class="docutils literal"><span class="pre">analyzer</span> <span class="pre">==</span> <span class="pre">'word'</span></code>.</p>
<p class="last">If None, no stop words will be used. max_df can be set to a value
in the range [0.7, 1.0) to automatically detect and filter stop
words based on intra corpus document frequency of terms.</p>
</dd>
<dt>token_pattern <span class="classifier-delimiter">:</span> <span class="classifier">string</span></dt>
<dd>Regular expression denoting what constitutes a “token”, only used
if <code class="docutils literal"><span class="pre">analyzer</span> <span class="pre">==</span> <span class="pre">'word'</span></code>. The default regexp selects tokens of 2
or more alphanumeric characters (punctuation is completely ignored
and always treated as a token separator).</dd>
<dt>ngram_range <span class="classifier-delimiter">:</span> <span class="classifier">tuple (min_n, max_n) (default=(1, 1))</span></dt>
<dd>The lower and upper boundary of the range of n-values for different
n-grams to be extracted. All values of n such that min_n &lt;= n &lt;= max_n
will be used.</dd>
<dt>max_df <span class="classifier-delimiter">:</span> <span class="classifier">float in range [0.0, 1.0] or int (default=1.0)</span></dt>
<dd>When building the vocabulary ignore terms that have a document
frequency strictly higher than the given threshold (corpus-specific
stop words).
If float, the parameter represents a proportion of documents, integer
absolute counts.
This parameter is ignored if vocabulary is not None.</dd>
<dt>min_df <span class="classifier-delimiter">:</span> <span class="classifier">float in range [0.0, 1.0] or int (default=1)</span></dt>
<dd>When building the vocabulary ignore terms that have a document
frequency strictly lower than the given threshold. This value is also
called cut-off in the literature.
If float, the parameter represents a proportion of documents, integer
absolute counts.
This parameter is ignored if vocabulary is not None.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int or None (default=None)</span></dt>
<dd><p class="first">If not None, build a vocabulary that only consider the top
max_features ordered by term frequency across the corpus.</p>
<p class="last">This parameter is ignored if vocabulary is not None.</p>
</dd>
<dt>vocabulary <span class="classifier-delimiter">:</span> <span class="classifier">Mapping or iterable, optional (default=None)</span></dt>
<dd>Either a Mapping (e.g., a dict) where keys are terms and values are
indices in the feature matrix, or an iterable over terms. If not
given, a vocabulary is determined from the input documents.</dd>
<dt>binary <span class="classifier-delimiter">:</span> <span class="classifier">boolean (default=False)</span></dt>
<dd>If True, all non-zero term counts are set to 1. This does not mean
outputs will have only 0/1 values, only that the tf term in tf-idf
is binary. (Set idf and normalization to False to get 0/1 outputs.)</dd>
<dt>dtype <span class="classifier-delimiter">:</span> <span class="classifier">type, optional (default=float64)</span></dt>
<dd>Type of the matrix returned by fit_transform() or transform().</dd>
<dt>norm <span class="classifier-delimiter">:</span> <span class="classifier">‘l1’, ‘l2’ or None, optional (default=’l2’)</span></dt>
<dd><p class="first">Each output row will have unit norm, either:</p>
<ul class="last simple">
<li><ul class="first">
<li>‘l2’: Sum of squares of vector elements is 1. The cosine</li>
</ul>
</li>
<li>similarity between two vectors is their dot product when l2 norm has</li>
<li>been applied.</li>
<li><ul class="first">
<li>‘l1’: Sum of absolute values of vector elements is 1.</li>
</ul>
</li>
<li>See <code class="xref py py-func docutils literal"><span class="pre">preprocessing.normalize()</span></code></li>
</ul>
</dd>
<dt>use_idf <span class="classifier-delimiter">:</span> <span class="classifier">boolean (default=True)</span></dt>
<dd>Enable inverse-document-frequency reweighting.</dd>
<dt>smooth_idf <span class="classifier-delimiter">:</span> <span class="classifier">boolean (default=True)</span></dt>
<dd>Smooth idf weights by adding one to document frequencies, as if an
extra document was seen containing every term in the collection
exactly once. Prevents zero divisions.</dd>
<dt>sublinear_tf <span class="classifier-delimiter">:</span> <span class="classifier">boolean (default=False)</span></dt>
<dd>Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">vocabulary_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd>A mapping of terms to feature indices.</dd>
<dt><code class="docutils literal"><span class="pre">idf_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features)</span></dt>
<dd>The inverse document frequency (IDF) vector; only defined
if  <code class="docutils literal"><span class="pre">use_idf</span></code> is True.</dd>
<dt><code class="docutils literal"><span class="pre">stop_words_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">set</span></dt>
<dd><p class="first">Terms that were ignored because they either:</p>
<blockquote>
<div><ul class="simple">
<li>occurred in too many documents (<cite>max_df</cite>)</li>
<li>occurred in too few documents (<cite>min_df</cite>)</li>
<li>were cut off by feature selection (<cite>max_features</cite>).</li>
</ul>
</div></blockquote>
<p class="last">This is only available if no vocabulary was given.</p>
</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
<span class="gp">... </span>    <span class="s1">&#39;This is the first document.&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;This document is the second document.&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;And this is the third one.&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;Is this the first document?&#39;</span><span class="p">,</span>
<span class="gp">... </span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
<span class="go">[&#39;and&#39;, &#39;document&#39;, &#39;first&#39;, &#39;is&#39;, &#39;one&#39;, &#39;second&#39;, &#39;the&#39;, &#39;third&#39;, &#39;this&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(4, 9)</span>
</pre></div>
</div>
<p>See also</p>
<p>CountVectorizer : Transforms text into a sparse matrix of n-gram counts.</p>
<dl class="docutils">
<dt>TfidfTransformer <span class="classifier-delimiter">:</span> <span class="classifier">Performs the TF-IDF transformation from a provided</span></dt>
<dd>matrix of counts.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>The <code class="docutils literal"><span class="pre">stop_words_</span></code> attribute can get large and increase the model size
when pickling. This attribute is provided only for introspection and can
be safely removed using delattr or set to None before pickling.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#TfidfVectorizerScikitsLearnNode">TfidfVectorizerScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.KBinsDiscretizerScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">KBinsDiscretizerScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.KBinsDiscretizerScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bin continuous data into intervals.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.preprocessing._discretization.KBinsDiscretizer</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_bins <span class="classifier-delimiter">:</span> <span class="classifier">int or array-like, shape (n_features,) (default=5)</span></dt>
<dd>The number of bins to produce. Raises ValueError if <code class="docutils literal"><span class="pre">n_bins</span> <span class="pre">&lt;</span> <span class="pre">2</span></code>.</dd>
<dt>encode <span class="classifier-delimiter">:</span> <span class="classifier">{‘onehot’, ‘onehot-dense’, ‘ordinal’}, (default=’onehot’)</span></dt>
<dd><p class="first">Method used to encode the transformed result.</p>
<dl class="last docutils">
<dt>onehot</dt>
<dd>Encode the transformed result with one-hot encoding
and return a sparse matrix. Ignored features are always
stacked to the right.</dd>
<dt>onehot-dense</dt>
<dd>Encode the transformed result with one-hot encoding
and return a dense array. Ignored features are always
stacked to the right.</dd>
<dt>ordinal</dt>
<dd>Return the bin identifier encoded as an integer value.</dd>
</dl>
</dd>
<dt>strategy <span class="classifier-delimiter">:</span> <span class="classifier">{‘uniform’, ‘quantile’, ‘kmeans’}, (default=’quantile’)</span></dt>
<dd><p class="first">Strategy used to define the widths of the bins.</p>
<dl class="last docutils">
<dt>uniform</dt>
<dd>All bins in each feature have identical widths.</dd>
<dt>quantile</dt>
<dd>All bins in each feature have the same number of points.</dd>
<dt>kmeans</dt>
<dd>Values in each bin have the same nearest center of a 1D k-means
cluster.</dd>
</dl>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">n_bins_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int array, shape (n_features,)</span></dt>
<dd>Number of bins per feature. Bins whose width are too small
(i.e., &lt;= 1e-8) are removed with a warning.</dd>
<dt><code class="docutils literal"><span class="pre">bin_edges_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of arrays, shape (n_features, )</span></dt>
<dd>The edges of each bin. Contain arrays of varying shapes <code class="docutils literal"><span class="pre">(n_bins_,</span> <span class="pre">)</span></code>
Ignored features will have empty arrays.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">,</span>   <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span>  <span class="mf">0.5</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>    <span class="mi">2</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">est</span> <span class="o">=</span> <span class="n">KBinsDiscretizer</span><span class="p">(</span><span class="n">n_bins</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">encode</span><span class="o">=</span><span class="s1">&#39;ordinal&#39;</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;uniform&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">est</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  
<span class="go">KBinsDiscretizer(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Xt</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Xt</span>  
<span class="go">array([[ 0., 0., 0., 0.],</span>
<span class="go">       [ 1., 1., 1., 0.],</span>
<span class="go">       [ 2., 2., 2., 1.],</span>
<span class="go">       [ 2., 2., 2., 2.]])</span>
</pre></div>
</div>
<p>Sometimes it may be useful to convert the data back into the original
feature space. The <code class="docutils literal"><span class="pre">inverse_transform</span></code> function converts the binned
data into the original feature space. Each value will be equal to the mean
of the two bin edges.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">est</span><span class="o">.</span><span class="n">bin_edges_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="go">array([-2., -1.,  0.,  1.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">est</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">Xt</span><span class="p">)</span>
<span class="go">array([[-1.5,  1.5, -3.5, -0.5],</span>
<span class="go">       [-0.5,  2.5, -2.5, -0.5],</span>
<span class="go">       [ 0.5,  3.5, -1.5,  0.5],</span>
<span class="go">       [ 0.5,  3.5, -1.5,  1.5]])</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>In bin edges for feature <code class="docutils literal"><span class="pre">i</span></code>, the first and last values are used only for
<code class="docutils literal"><span class="pre">inverse_transform</span></code>. During transform, bin edges are extended to:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">bin_edges_</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">])</span>
</pre></div>
</div>
<p>You can combine <code class="docutils literal"><span class="pre">KBinsDiscretizer</span></code> with
<code class="xref py py-class docutils literal"><span class="pre">sklearn.compose.ColumnTransformer</span></code> if you only want to preprocess
part of the features.</p>
<p><code class="docutils literal"><span class="pre">KBinsDiscretizer</span></code> might produce constant features (e.g., when
<code class="docutils literal"><span class="pre">encode</span> <span class="pre">=</span> <span class="pre">'onehot'</span></code> and certain bins do not contain any data).
These features can be removed with feature selection algorithms
(e.g., <code class="xref py py-class docutils literal"><span class="pre">sklearn.feature_selection.VarianceThreshold</span></code>).</p>
<p>See also</p>
<blockquote>
<div><dl class="docutils">
<dt>sklearn.preprocessing.Binarizer <span class="classifier-delimiter">:</span> <span class="classifier">class used to bin values as <code class="docutils literal"><span class="pre">0</span></code> or</span></dt>
<dd><code class="docutils literal"><span class="pre">1</span></code> based on a parameter <code class="docutils literal"><span class="pre">threshold</span></code>.</dd>
</dl>
</div></blockquote>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#KBinsDiscretizerScikitsLearnNode">KBinsDiscretizerScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.IncrementalPCAScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">IncrementalPCAScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.IncrementalPCAScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Incremental principal components analysis (IPCA).
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.decomposition.incremental_pca.IncrementalPCA</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Linear dimensionality reduction using Singular Value Decomposition of
centered data, keeping only the most significant singular vectors to
project the data to a lower dimensional space.</p>
<p>Depending on the size of the input data, this algorithm can be much more
memory efficient than a PCA.</p>
<p>This algorithm has constant memory complexity, on the order
of <code class="docutils literal"><span class="pre">batch_size</span></code>, enabling use of np.memmap files without loading the
entire file into memory.</p>
<p>The computational overhead of each SVD is
<code class="docutils literal"><span class="pre">O(batch_size</span> <span class="pre">*</span> <span class="pre">n_features</span> <span class="pre">**</span> <span class="pre">2)</span></code>, but only 2 * batch_size samples
remain in memory at a time. There will be <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">batch_size</span></code> SVD
computations to get the principal components, versus 1 large SVD of
complexity <code class="docutils literal"><span class="pre">O(n_samples</span> <span class="pre">*</span> <span class="pre">n_features</span> <span class="pre">**</span> <span class="pre">2)</span></code> for PCA.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int or None, (default=None)</span></dt>
<dd>Number of components to keep. If <code class="docutils literal"><span class="pre">n_components</span> <span class="pre">``</span> <span class="pre">is</span> <span class="pre">``None</span></code>,
then <code class="docutils literal"><span class="pre">n_components</span></code> is set to <code class="docutils literal"><span class="pre">min(n_samples,</span> <span class="pre">n_features)</span></code>.</dd>
<dt>whiten <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd><p class="first">When True (False by default) the <code class="docutils literal"><span class="pre">components_</span></code> vectors are divided
by <code class="docutils literal"><span class="pre">n_samples</span></code> times <code class="docutils literal"><span class="pre">components_</span></code> to ensure uncorrelated outputs
with unit component-wise variances.</p>
<p class="last">Whitening will remove some information from the transformed signal
(the relative variance scales of the components) but can sometimes
improve the predictive accuracy of the downstream estimators by
making data respect some hard-wired assumptions.</p>
</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">bool, (default=True)</span></dt>
<dd>If False, X will be overwritten. <code class="docutils literal"><span class="pre">copy=False</span></code> can be used to
save memory but is unsafe for general use.</dd>
<dt>batch_size <span class="classifier-delimiter">:</span> <span class="classifier">int or None, (default=None)</span></dt>
<dd>The number of samples to use for each batch. Only used when calling
<code class="docutils literal"><span class="pre">fit</span></code>. If <code class="docutils literal"><span class="pre">batch_size</span></code> is <code class="docutils literal"><span class="pre">None</span></code>, then <code class="docutils literal"><span class="pre">batch_size</span></code>
is inferred from the data and set to <code class="docutils literal"><span class="pre">5</span> <span class="pre">*</span> <span class="pre">n_features</span></code>, to provide a
balance between approximation accuracy and memory consumption.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">components_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_components, n_features)</span></dt>
<dd>Components with maximum variance.</dd>
<dt><code class="docutils literal"><span class="pre">explained_variance_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_components,)</span></dt>
<dd>Variance explained by each of the selected components.</dd>
<dt><code class="docutils literal"><span class="pre">explained_variance_ratio_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_components,)</span></dt>
<dd>Percentage of variance explained by each of the selected components.
If all components are stored, the sum of explained variances is equal
to 1.0.</dd>
<dt><code class="docutils literal"><span class="pre">singular_values_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_components,)</span></dt>
<dd>The singular values corresponding to each of the selected components.
The singular values are equal to the 2-norms of the <code class="docutils literal"><span class="pre">n_components</span></code>
variables in the lower-dimensional space.</dd>
<dt><code class="docutils literal"><span class="pre">mean_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,)</span></dt>
<dd>Per-feature empirical mean, aggregate over calls to <code class="docutils literal"><span class="pre">partial_fit</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">var_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,)</span></dt>
<dd>Per-feature empirical variance, aggregate over calls to
<code class="docutils literal"><span class="pre">partial_fit</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">noise_variance_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The estimated noise covariance following the Probabilistic PCA model
from Tipping and Bishop 1999. See “Pattern Recognition and
Machine Learning” by C. Bishop, 12.2.1 p. 574 or
<a class="reference external" href="http://www.miketipping.com/papers/met-mppca.pdf">http://www.miketipping.com/papers/met-mppca.pdf</a>.</dd>
<dt><code class="docutils literal"><span class="pre">n_components_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The estimated number of components. Relevant when
<code class="docutils literal"><span class="pre">n_components=None</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">n_samples_seen_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The number of samples processed by the estimator. Will be reset on
new calls to fit, but increments across <code class="docutils literal"><span class="pre">partial_fit</span></code> calls.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">IncrementalPCA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span> <span class="o">=</span> <span class="n">IncrementalPCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># either partially fit on smaller batches of data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span><span class="o">.</span><span class="n">partial_fit</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">100</span><span class="p">,</span> <span class="p">:])</span>
<span class="go">IncrementalPCA(batch_size=200, copy=True, n_components=7, whiten=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># or let the fit function itself divide the data into batches</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_transformed</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_transformed</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(1797, 7)</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>Implements the incremental PCA model from:</p>
<p><cite>D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for Robust Visual
Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3,
pp. 125-141, May 2008.</cite>
See <a class="reference external" href="http://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf">http://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf</a></p>
<p>This model is an extension of the Sequential Karhunen-Loeve Transform from:</p>
<p><cite>A. Levy and M. Lindenbaum, Sequential Karhunen-Loeve Basis Extraction and
its Application to Images, IEEE Transactions on Image Processing, Volume 9,
Number 8, pp. 1371-1374, August 2000.</cite>
See <a class="reference external" href="http://www.cs.technion.ac.il/~mic/doc/skl-ip.pdf">http://www.cs.technion.ac.il/~mic/doc/skl-ip.pdf</a></p>
<p>We have specifically abstained from an optimization used by authors of both
papers, a QR decomposition used in specific situations to reduce the
algorithmic complexity of the SVD. The source for this technique is
<cite>Matrix Computations, Third Edition, G. Holub and C. Van Loan, Chapter 5,
section 5.4.4, pp 252-253.</cite>. This technique has been omitted because it is
advantageous only when decomposing a matrix with <code class="docutils literal"><span class="pre">n_samples</span></code> (rows)
&gt;= 5/3 * <code class="docutils literal"><span class="pre">n_features</span></code> (columns), and hurts the readability of the
implemented algorithm. This would be a good opportunity for future
optimization, if it is deemed necessary.</p>
<p><strong>References</strong></p>
<ol class="upperalpha simple" start="4">
<li><dl class="first docutils">
<dt>Ross, J. Lim, R. Lin, M. Yang. Incremental Learning for Robust Visual</dt>
<dd>Tracking, International Journal of Computer Vision, Volume 77,
Issue 1-3, pp. 125-141, May 2008.</dd>
</dl>
</li>
</ol>
<ol class="upperalpha simple" start="7">
<li><dl class="first docutils">
<dt>Golub and C. Van Loan. Matrix Computations, Third Edition, Chapter 5,</dt>
<dd>Section 5.4.4, pp. 252-253.</dd>
</dl>
</li>
</ol>
<p>See also</p>
<p>PCA
KernelPCA
SparsePCA
TruncatedSVD</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#IncrementalPCAScikitsLearnNode">IncrementalPCAScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.MiniBatchSparsePCAScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">MiniBatchSparsePCAScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.MiniBatchSparsePCAScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Mini-batch Sparse Principal Components Analysis
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.decomposition.sparse_pca.MiniBatchSparsePCA</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Finds the set of sparse components that can optimally reconstruct
the data.  The amount of sparseness is controllable by the coefficient
of the L1 penalty, given by the parameter alpha.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>number of sparse atoms to extract</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>Sparsity controlling parameter. Higher values lead to sparser
components.</dd>
<dt>ridge_alpha <span class="classifier-delimiter">:</span> <span class="classifier">float,</span></dt>
<dd>Amount of ridge shrinkage to apply in order to improve
conditioning when calling the transform method.</dd>
<dt>n_iter <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>number of iterations to perform for each mini batch</dd>
<dt>callback <span class="classifier-delimiter">:</span> <span class="classifier">callable or None, optional (default: None)</span></dt>
<dd>callable that gets invoked every five iterations</dd>
<dt>batch_size <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>the number of features to take in each mini batch</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Controls the verbosity; the higher, the more messages. Defaults to 0.</dd>
<dt>shuffle <span class="classifier-delimiter">:</span> <span class="classifier">boolean,</span></dt>
<dd>whether to shuffle the data before splitting it in batches</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Number of parallel jobs to run.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>method <span class="classifier-delimiter">:</span> <span class="classifier">{‘lars’, ‘cd’}</span></dt>
<dd>lars: uses the least angle regression method to solve the lasso problem
(linear_model.lars_path)
cd: uses the coordinate descent method to compute the
Lasso solution (linear_model.Lasso). Lars will be faster if
the estimated components are sparse.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>normalize_components <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=False)</span></dt>
<dd><ul class="first simple">
<li>if False, use a version of Sparse PCA without components
normalization and without data centering. This is likely a bug and
even though it’s the default for backward compatibility,
this should not be used.</li>
<li>if True, use a version of Sparse PCA with components normalization
and data centering.</li>
</ul>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
<div class="last deprecated">
<p><span class="versionmodified">Deprecated since version 0.22: </span><code class="docutils literal"><span class="pre">normalize_components</span></code> was added and set to <code class="docutils literal"><span class="pre">False</span></code> for
backward compatibility. It would be set to <code class="docutils literal"><span class="pre">True</span></code> from 0.22
onwards.</p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">components_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>Sparse components extracted from the data.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of iterations run.</dd>
<dt><code class="docutils literal"><span class="pre">mean_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,)</span></dt>
<dd>Per-feature empirical mean, estimated from the training set.
Equal to <code class="docutils literal"><span class="pre">X.mean(axis=0)</span></code>.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_friedman1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">MiniBatchSparsePCA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">make_friedman1</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span> <span class="o">=</span> <span class="n">MiniBatchSparsePCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">normalize_components</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> 
<span class="go">MiniBatchSparsePCA(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_transformed</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_transformed</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(200, 5)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># most values in the ``components_`` are zero (sparsity)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">transformer</span><span class="o">.</span><span class="n">components_</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
<span class="go">0.94</span>
</pre></div>
</div>
<p>See also</p>
<p>PCA
SparsePCA
DictionaryLearning</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#MiniBatchSparsePCAScikitsLearnNode">MiniBatchSparsePCAScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.FactorAnalysisScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">FactorAnalysisScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.FactorAnalysisScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Factor Analysis (FA)
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.decomposition.factor_analysis.FactorAnalysis</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
A simple linear generative model with Gaussian latent variables.</p>
<p>The observations are assumed to be caused by a linear transformation of
lower dimensional latent factors and added Gaussian noise.
Without loss of generality the factors are distributed according to a
Gaussian with zero mean and unit covariance. The noise is also zero mean
and has an arbitrary diagonal covariance matrix.</p>
<p>If we would restrict the model further, by assuming that the Gaussian
noise is even isotropic (all diagonal entries are the same) we would obtain
<code class="xref py py-class docutils literal"><span class="pre">PPCA</span></code>.</p>
<p>FactorAnalysis performs a maximum likelihood estimate of the so-called
<cite>loading</cite> matrix, the transformation of the latent variables to the
observed ones, using expectation-maximization (EM).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int | None</span></dt>
<dd>Dimensionality of latent space, the number of components
of <code class="docutils literal"><span class="pre">X</span></code> that are obtained after <code class="docutils literal"><span class="pre">transform</span></code>.
If None, n_components is set to the number of features.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Stopping tolerance for EM algorithm.</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Whether to make a copy of X. If <code class="docutils literal"><span class="pre">False</span></code>, the input X gets overwritten
during fitting.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Maximum number of iterations.</dd>
<dt>noise_variance_init <span class="classifier-delimiter">:</span> <span class="classifier">None | array, shape=(n_features,)</span></dt>
<dd>The initial guess of the noise variance for each feature.
If None, it defaults to np.ones(n_features)</dd>
<dt>svd_method <span class="classifier-delimiter">:</span> <span class="classifier">{‘lapack’, ‘randomized’}</span></dt>
<dd>Which SVD method to use. If ‘lapack’ use standard SVD from
scipy.linalg, if ‘randomized’ use fast <code class="docutils literal"><span class="pre">randomized_svd</span></code> function.
Defaults to ‘randomized’. For most applications ‘randomized’ will
be sufficiently precise while providing significant speed gains.
Accuracy can also be improved by setting higher values for
<cite>iterated_power</cite>. If this is not sufficient, for maximum precision
you should choose ‘lapack’.</dd>
<dt>iterated_power <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Number of iterations for the power method. 3 by default. Only used
if <code class="docutils literal"><span class="pre">svd_method</span></code> equals ‘randomized’</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=0)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>. Only used when <code class="docutils literal"><span class="pre">svd_method</span></code> equals ‘randomized’.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">components_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>Components with maximum variance.</dd>
<dt><code class="docutils literal"><span class="pre">loglike_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list, [n_iterations]</span></dt>
<dd>The log likelihood at each iteration.</dd>
<dt><code class="docutils literal"><span class="pre">noise_variance_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape=(n_features,)</span></dt>
<dd>The estimated noise variance for each feature.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of iterations run.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">FactorAnalysis</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span> <span class="o">=</span> <span class="n">FactorAnalysis</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_transformed</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_transformed</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(1797, 7)</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>See also</p>
<dl class="docutils">
<dt>PCA: Principal component analysis is also a latent linear variable model</dt>
<dd>which however assumes equal noise variance for each feature.
This extra assumption makes probabilistic PCA faster as it can be
computed in closed form.</dd>
<dt>FastICA: Independent component analysis, a latent variable model with</dt>
<dd>non-Gaussian latent variables.</dd>
</dl>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#FactorAnalysisScikitsLearnNode">FactorAnalysisScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.FunctionTransformerScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">FunctionTransformerScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.FunctionTransformerScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructs a transformer from an arbitrary callable.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.preprocessing._function_transformer.FunctionTransformer</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
A FunctionTransformer forwards its X (and optionally y) arguments to a
user-defined function or function object and returns the result of this
function. This is useful for stateless transformations such as taking the
log of frequencies, doing custom scaling, etc.</p>
<p>Note: If a lambda is used as the function, then the resulting
transformer will not be pickleable.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17.</span></p>
</div>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>func <span class="classifier-delimiter">:</span> <span class="classifier">callable, optional default=None</span></dt>
<dd>The callable to use for the transformation. This will be passed
the same arguments as transform, with args and kwargs forwarded.
If func is None, then func will be the identity function.</dd>
<dt>inverse_func <span class="classifier-delimiter">:</span> <span class="classifier">callable, optional default=None</span></dt>
<dd>The callable to use for the inverse transformation. This will be
passed the same arguments as inverse transform, with args and
kwargs forwarded. If inverse_func is None, then inverse_func
will be the identity function.</dd>
<dt>validate <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional default=True</span></dt>
<dd><p class="first">Indicate that the input X array should be checked before calling
<code class="docutils literal"><span class="pre">func</span></code>. The possibilities are:</p>
<ul class="simple">
<li>If False, there is no input validation.</li>
<li>If True, then X will be converted to a 2-dimensional NumPy array or
sparse matrix. If the conversion is not possible an exception is
raised.</li>
</ul>
<div class="last deprecated">
<p><span class="versionmodified">Deprecated since version 0.20: </span><code class="docutils literal"><span class="pre">validate=True</span></code> as default will be replaced by
<code class="docutils literal"><span class="pre">validate=False</span></code> in 0.22.</p>
</div>
</dd>
<dt>accept_sparse <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>Indicate that func accepts a sparse matrix as input. If validate is
False, this has no effect. Otherwise, if accept_sparse is false,
sparse matrix inputs will cause an exception to be raised.</dd>
<dt>pass_y <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional default=False</span></dt>
<dd>Indicate that transform should forward the y argument to the
inner callable.</dd>
<dt>check_inverse <span class="classifier-delimiter">:</span> <span class="classifier">bool, default=True</span></dt>
<dd><p class="first">Whether to check that or <code class="docutils literal"><span class="pre">func</span></code> followed by <code class="docutils literal"><span class="pre">inverse_func</span></code> leads to
the original inputs. It can be used for a sanity check, raising a
warning when the condition is not fulfilled.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
<dt>kw_args <span class="classifier-delimiter">:</span> <span class="classifier">dict, optional</span></dt>
<dd>Dictionary of additional keyword arguments to pass to func.</dd>
<dt>inv_kw_args <span class="classifier-delimiter">:</span> <span class="classifier">dict, optional</span></dt>
<dd>Dictionary of additional keyword arguments to pass to inverse_func.</dd>
</dl>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#FunctionTransformerScikitsLearnNode">FunctionTransformerScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.LassoLarsICScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">LassoLarsICScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.LassoLarsICScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Lasso model fit with Lars using BIC or AIC for model selection
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.least_angle.LassoLarsIC</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
The optimization objective for Lasso is:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2_2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
</pre></div>
</div>
<p>AIC is the Akaike information criterion and BIC is the Bayes
Information criterion. Such criteria are useful to select the value
of the regularization parameter by making a trade-off between the
goodness of fit and the complexity of the model. A good model should
explain well the data while being simple.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>criterion <span class="classifier-delimiter">:</span> <span class="classifier">‘bic’ | ‘aic’</span></dt>
<dd>The type of criterion to use.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">boolean or integer, optional</span></dt>
<dd>Sets the verbosity amount</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>This parameter is ignored when <code class="docutils literal"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<code class="xref py py-class docutils literal"><span class="pre">sklearn.preprocessing.StandardScaler</span></code> before calling <code class="docutils literal"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal"><span class="pre">normalize=False</span></code>.</dd>
<dt>precompute <span class="classifier-delimiter">:</span> <span class="classifier">True | False | ‘auto’ | array-like</span></dt>
<dd>Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal"><span class="pre">'auto'</span></code> let us decide. The Gram
matrix can also be passed as argument.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span></dt>
<dd>Maximum number of iterations to perform. Can be used for
early stopping.</dd>
<dt>eps <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The machine-precision regularization in the computation of the
Cholesky diagonal factors. Increase this for very ill-conditioned
systems. Unlike the <code class="docutils literal"><span class="pre">tol</span></code> parameter in some iterative
optimization-based algorithms, this parameter does not control
the tolerance of the optimization.</dd>
<dt>copy_X <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>If True, X will be copied; else, it may be overwritten.</dd>
<dt>positive <span class="classifier-delimiter">:</span> <span class="classifier">boolean (default=False)</span></dt>
<dd>Restrict coefficients to be &gt;= 0. Be aware that you might want to
remove fit_intercept which is set True by default.
Under the positive restriction the model coefficients do not converge
to the ordinary-least-squares solution for small values of alpha.
Only coefficients up to the smallest alpha value (<code class="docutils literal"><span class="pre">alphas_[alphas_</span> <span class="pre">&gt;</span>
<span class="pre">0.].min()</span></code> when fit_path=True) reached by the stepwise Lars-Lasso
algorithm are typically in congruence with the solution of the
coordinate descent Lasso estimator.
As a consequence using LassoLarsIC only makes sense for problems where
a sparse solution is expected and/or reached.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,)</span></dt>
<dd>parameter vector (w in the formulation formula)</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>independent term in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">alpha_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>the alpha parameter chosen by the information criterion</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>number of iterations run by lars_path to find the grid of
alphas.</dd>
<dt><code class="docutils literal"><span class="pre">criterion_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_alphas,)</span></dt>
<dd>The value of the information criteria (‘aic’, ‘bic’) across all
alphas. The alpha which has the smallest information criterion is
chosen. This value is larger by a factor of <code class="docutils literal"><span class="pre">n_samples</span></code> compared to
Eqns. 2.15 and 2.16 in (Zou et al, 2007).</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LassoLarsIC</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;bic&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.1111</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1111</span><span class="p">])</span>
<span class="gp">... </span>
<span class="go">LassoLarsIC(copy_X=True, criterion=&#39;bic&#39;, eps=..., fit_intercept=True,</span>
<span class="go">      max_iter=500, normalize=True, positive=False, precompute=&#39;auto&#39;,</span>
<span class="go">      verbose=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span> 
<span class="go">[ 0.  -1.11...]</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>The estimation of the number of degrees of freedom is given by:</p>
<p>“On the degrees of freedom of the lasso”
Hui Zou, Trevor Hastie, and Robert Tibshirani
Ann. Statist. Volume 35, Number 5 (2007), 2173-2192.</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Akaike_information_criterion">https://en.wikipedia.org/wiki/Akaike_information_criterion</a>
<a class="reference external" href="https://en.wikipedia.org/wiki/Bayesian_information_criterion">https://en.wikipedia.org/wiki/Bayesian_information_criterion</a></p>
<p>See also</p>
<p>lars_path, LassoLars, LassoLarsCV</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#LassoLarsICScikitsLearnNode">LassoLarsICScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.RFEScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">RFEScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.RFEScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Feature ranking with recursive feature elimination.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.feature_selection.rfe.RFE</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Given an external estimator that assigns weights to features (e.g., the
coefficients of a linear model), the goal of recursive feature elimination
(RFE) is to select features by recursively considering smaller and smaller
sets of features. First, the estimator is trained on the initial set of
features and the importance of each feature is obtained either through a
<code class="docutils literal"><span class="pre">coef_</span></code> attribute or through a <code class="docutils literal"><span class="pre">feature_importances_</span></code> attribute.
Then, the least important features are pruned from current set of features.
That procedure is recursively repeated on the pruned set until the desired
number of features to select is eventually reached.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>estimator <span class="classifier-delimiter">:</span> <span class="classifier">object</span></dt>
<dd>A supervised learning estimator with a <code class="docutils literal"><span class="pre">fit</span></code> method that provides
information about feature importance either through a <code class="docutils literal"><span class="pre">coef_</span></code>
attribute or through a <code class="docutils literal"><span class="pre">feature_importances_</span></code> attribute.</dd>
<dt>n_features_to_select <span class="classifier-delimiter">:</span> <span class="classifier">int or None (default=None)</span></dt>
<dd>The number of features to select. If <cite>None</cite>, half of the features
are selected.</dd>
<dt>step <span class="classifier-delimiter">:</span> <span class="classifier">int or float, optional (default=1)</span></dt>
<dd>If greater than or equal to 1, then <code class="docutils literal"><span class="pre">step</span></code> corresponds to the
(integer) number of features to remove at each iteration.
If within (0.0, 1.0), then <code class="docutils literal"><span class="pre">step</span></code> corresponds to the percentage
(rounded down) of features to remove at each iteration.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, (default=0)</span></dt>
<dd>Controls verbosity of output.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">n_features_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The number of selected features.</dd>
<dt><code class="docutils literal"><span class="pre">support_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape [n_features]</span></dt>
<dd>The mask of selected features.</dd>
<dt><code class="docutils literal"><span class="pre">ranking_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape [n_features]</span></dt>
<dd>The feature ranking, such that <code class="docutils literal"><span class="pre">ranking_[i]</span></code> corresponds to the
ranking position of the i-th feature. Selected (i.e., estimated
best) features are assigned rank 1.</dd>
<dt><code class="docutils literal"><span class="pre">estimator_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">object</span></dt>
<dd>The external estimator fit on the reduced dataset.</dd>
</dl>
<p><strong>Examples</strong></p>
<p>The following example shows how to retrieve the 5 right informative
features in the Friedman #1 dataset.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_friedman1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">RFE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVR</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_friedman1</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">estimator</span> <span class="o">=</span> <span class="n">SVR</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span> <span class="o">=</span> <span class="n">selector</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span><span class="o">.</span><span class="n">support_</span> 
<span class="go">array([ True,  True,  True,  True,  True, False, False, False, False,</span>
<span class="go">       False])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span><span class="o">.</span><span class="n">ranking_</span>
<span class="go">array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>RFECV <span class="classifier-delimiter">:</span> <span class="classifier">Recursive feature elimination with built-in cross-validated</span></dt>
<dd>selection of the best number of features</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id35" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Guyon, I., Weston, J., Barnhill, S., &amp; Vapnik, V., “Gene selection
for cancer classification using support vector machines”,
Mach. Learn., 46(1-3), 389–422, 2002.</td></tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.RFEScikitsLearnNode-class.html">RFEScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.PCAScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">PCAScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.PCAScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Principal component analysis (PCA)
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.decomposition.pca.PCA</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Linear dimensionality reduction using Singular Value Decomposition of the
data to project it to a lower dimensional space.</p>
<p>It uses the LAPACK implementation of the full SVD or a randomized truncated
SVD by the method of Halko et al. 2009, depending on the shape of the input
data and the number of components to extract.</p>
<p>It can also use the scipy.sparse.linalg ARPACK implementation of the
truncated SVD.</p>
<p>Notice that this class does not support sparse input. See
<code class="xref py py-class docutils literal"><span class="pre">TruncatedSVD</span></code> for an alternative with sparse data.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int, float, None or string</span></dt>
<dd><p class="first">Number of components to keep.
if n_components is not set all components are kept:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">n_components</span> <span class="o">==</span> <span class="nb">min</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
</pre></div>
</div>
<p>If <code class="docutils literal"><span class="pre">n_components</span> <span class="pre">==</span> <span class="pre">'mle'</span></code> and <code class="docutils literal"><span class="pre">svd_solver</span> <span class="pre">==</span> <span class="pre">'full'</span></code>, Minka’s
MLE is used to guess the dimension. Use of <code class="docutils literal"><span class="pre">n_components</span> <span class="pre">==</span> <span class="pre">'mle'</span></code>
will interpret <code class="docutils literal"><span class="pre">svd_solver</span> <span class="pre">==</span> <span class="pre">'auto'</span></code> as <code class="docutils literal"><span class="pre">svd_solver</span> <span class="pre">==</span> <span class="pre">'full'</span></code>.</p>
<p>If <code class="docutils literal"><span class="pre">0</span> <span class="pre">&lt;</span> <span class="pre">n_components</span> <span class="pre">&lt;</span> <span class="pre">1</span></code> and <code class="docutils literal"><span class="pre">svd_solver</span> <span class="pre">==</span> <span class="pre">'full'</span></code>, select the
number of components such that the amount of variance that needs to be
explained is greater than the percentage specified by n_components.</p>
<p>If <code class="docutils literal"><span class="pre">svd_solver</span> <span class="pre">==</span> <span class="pre">'arpack'</span></code>, the number of components must be
strictly less than the minimum of n_features and n_samples.</p>
<p>Hence, the None case results in:</p>
<div class="last highlight-default"><div class="highlight"><pre><span></span><span class="n">n_components</span> <span class="o">==</span> <span class="nb">min</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
</pre></div>
</div>
</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">bool (default True)</span></dt>
<dd>If False, data passed to fit are overwritten and running
fit(X).transform(X) will not yield the expected results,
use fit_transform(X) instead.</dd>
<dt>whiten <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default False)</span></dt>
<dd><p class="first">When True (False by default) the <cite>components_</cite> vectors are multiplied
by the square root of n_samples and then divided by the singular values
to ensure uncorrelated outputs with unit component-wise variances.</p>
<p class="last">Whitening will remove some information from the transformed signal
(the relative variance scales of the components) but can sometime
improve the predictive accuracy of the downstream estimators by
making their data respect some hard-wired assumptions.</p>
</dd>
<dt>svd_solver <span class="classifier-delimiter">:</span> <span class="classifier">string {‘auto’, ‘full’, ‘arpack’, ‘randomized’}</span></dt>
<dd><p class="first">auto :</p>
<blockquote>
<div><ul class="simple">
<li>the solver is selected by a default policy based on <cite>X.shape</cite> and</li>
<li><cite>n_components</cite>: if the input data is larger than 500x500 and the</li>
<li>number of components to extract is lower than 80% of the smallest</li>
<li>dimension of the data, then the more efficient ‘randomized’</li>
<li>method is enabled. Otherwise the exact full SVD is computed and</li>
<li>optionally truncated afterwards.</li>
</ul>
</div></blockquote>
<p>full :</p>
<blockquote>
<div><ul class="simple">
<li>run exact full SVD calling the standard LAPACK solver via</li>
<li><cite>scipy.linalg.svd</cite> and select the components by postprocessing</li>
</ul>
</div></blockquote>
<p>arpack :</p>
<blockquote>
<div><ul class="simple">
<li>run SVD truncated to n_components calling ARPACK solver via</li>
<li><cite>scipy.sparse.linalg.svds</cite>. It requires strictly</li>
<li>0 &lt; n_components &lt; min(X.shape)</li>
</ul>
</div></blockquote>
<p>randomized :</p>
<blockquote>
<div><ul class="simple">
<li>run randomized SVD by the method of Halko et al.</li>
</ul>
</div></blockquote>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.18.0.</span></p>
</div>
</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float &gt;= 0, optional (default .0)</span></dt>
<dd><p class="first">Tolerance for singular values computed by svd_solver == ‘arpack’.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.18.0.</span></p>
</div>
</dd>
<dt>iterated_power <span class="classifier-delimiter">:</span> <span class="classifier">int &gt;= 0, or ‘auto’, (default ‘auto’)</span></dt>
<dd><p class="first">Number of iterations for the power method computed by
svd_solver == ‘randomized’.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.18.0.</span></p>
</div>
</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default None)</span></dt>
<dd><p class="first">If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>. Used when <code class="docutils literal"><span class="pre">svd_solver</span></code> == ‘arpack’ or ‘randomized’.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.18.0.</span></p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">components_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_components, n_features)</span></dt>
<dd>Principal axes in feature space, representing the directions of
maximum variance in the data. The components are sorted by
<code class="docutils literal"><span class="pre">explained_variance_</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">explained_variance_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_components,)</span></dt>
<dd><p class="first">The amount of variance explained by each of the selected components.</p>
<p>Equal to n_components largest eigenvalues
of the covariance matrix of X.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.18.</span></p>
</div>
</dd>
<dt><code class="docutils literal"><span class="pre">explained_variance_ratio_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_components,)</span></dt>
<dd><p class="first">Percentage of variance explained by each of the selected components.</p>
<p class="last">If <code class="docutils literal"><span class="pre">n_components</span></code> is not set then all components are stored and the
sum of the ratios is equal to 1.0.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">singular_values_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_components,)</span></dt>
<dd>The singular values corresponding to each of the selected components.
The singular values are equal to the 2-norms of the <code class="docutils literal"><span class="pre">n_components</span></code>
variables in the lower-dimensional space.</dd>
<dt><code class="docutils literal"><span class="pre">mean_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,)</span></dt>
<dd><p class="first">Per-feature empirical mean, estimated from the training set.</p>
<p class="last">Equal to <cite>X.mean(axis=0)</cite>.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">n_components_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The estimated number of components. When n_components is set
to ‘mle’ or a number between 0 and 1 (with svd_solver == ‘full’) this
number is estimated from input data. Otherwise it equals the parameter
n_components, or the lesser value of n_features and n_samples
if n_components is None.</dd>
<dt><code class="docutils literal"><span class="pre">noise_variance_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd><p class="first">The estimated noise covariance following the Probabilistic PCA model
from Tipping and Bishop 1999. See “Pattern Recognition and
Machine Learning” by C. Bishop, 12.2.1 p. 574 or
<a class="reference external" href="http://www.miketipping.com/papers/met-mppca.pdf">http://www.miketipping.com/papers/met-mppca.pdf</a>. It is required to
compute the estimated data covariance and score samples.</p>
<p class="last">Equal to the average of (min(n_features, n_samples) - n_components)
smallest eigenvalues of the covariance matrix of X.</p>
</dd>
</dl>
<p><strong>References</strong></p>
<p>For n_components == ‘mle’, this class uses the method of <cite>Minka, T. P.
“Automatic choice of dimensionality for PCA”. In NIPS, pp. 598-604</cite></p>
<p>Implements the probabilistic PCA model from:</p>
<p><a href="#id36"><span class="problematic" id="id37">`</span></a>Tipping, M. E., and Bishop, C. M. (1999). “Probabilistic principal
component analysis”. Journal of the Royal Statistical Society:</p>
<p>Series B (Statistical Methodology), 61(3), 611-622.
via the score and score_samples methods.
See <a class="reference external" href="http://www.miketipping.com/papers/met-mppca.pdf">http://www.miketipping.com/papers/met-mppca.pdf</a></p>
<p>For svd_solver == ‘arpack’, refer to <cite>scipy.sparse.linalg.svds</cite>.</p>
<p>For svd_solver == ‘randomized’, see:</p>
<p><cite>Halko, N., Martinsson, P. G., and Tropp, J. A. (2011).
“Finding structure with randomness: Probabilistic algorithms for
constructing approximate matrix decompositions”.
SIAM review, 53(2), 217-288.</cite> and also
<cite>Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011).
“A randomized algorithm for the decomposition of matrices”.
Applied and Computational Harmonic Analysis, 30(1), 47-68.</cite></p>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">PCA(copy=True, iterated_power=&#39;auto&#39;, n_components=2, random_state=None,</span>
<span class="go">  svd_solver=&#39;auto&#39;, tol=0.0, whiten=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span>  
<span class="go">[0.9924... 0.0075...]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">singular_values_</span><span class="p">)</span>  
<span class="go">[6.30061... 0.54980...]</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">svd_solver</span><span class="o">=</span><span class="s1">&#39;full&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>                 
<span class="go">PCA(copy=True, iterated_power=&#39;auto&#39;, n_components=2, random_state=None,</span>
<span class="go">  svd_solver=&#39;full&#39;, tol=0.0, whiten=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span>  
<span class="go">[0.9924... 0.00755...]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">singular_values_</span><span class="p">)</span>  
<span class="go">[6.30061... 0.54980...]</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">svd_solver</span><span class="o">=</span><span class="s1">&#39;arpack&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">PCA(copy=True, iterated_power=&#39;auto&#39;, n_components=1, random_state=None,</span>
<span class="go">  svd_solver=&#39;arpack&#39;, tol=0.0, whiten=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span>  
<span class="go">[0.99244...]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">singular_values_</span><span class="p">)</span>  
<span class="go">[6.30061...]</span>
</pre></div>
</div>
<p>See also</p>
<p>KernelPCA
SparsePCA
TruncatedSVD
IncrementalPCA</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.PCAScikitsLearnNode-class.html">PCAScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.MultiTaskLassoScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">MultiTaskLassoScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.MultiTaskLassoScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.coordinate_descent.MultiTaskLasso</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
The optimization objective for Lasso is:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">Y</span> <span class="o">-</span> <span class="n">XW</span><span class="o">||^</span><span class="mi">2</span><span class="n">_Fro</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span>
</pre></div>
</div>
<p>Where:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span> <span class="o">=</span> \<span class="n">sum_i</span> \<span class="n">sqrt</span><span class="p">{</span>\<span class="n">sum_j</span> <span class="n">w_</span><span class="p">{</span><span class="n">ij</span><span class="p">}</span><span class="o">^</span><span class="mi">2</span><span class="p">}</span>
</pre></div>
</div>
<p>i.e. the sum of norm of each row.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Constant that multiplies the L1/L2 term. Defaults to 1.0</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span></dt>
<dd>This parameter is ignored when <code class="docutils literal"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<code class="xref py py-class docutils literal"><span class="pre">sklearn.preprocessing.StandardScaler</span></code> before calling <code class="docutils literal"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal"><span class="pre">normalize=False</span></code>.</dd>
<dt>copy_X <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>If <code class="docutils literal"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>The maximum number of iterations</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The tolerance for the optimization: if the updates are
smaller than <code class="docutils literal"><span class="pre">tol</span></code>, the optimization code checks the
dual gap for optimality and continues until it is smaller
than <code class="docutils literal"><span class="pre">tol</span></code>.</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd>When set to <code class="docutils literal"><span class="pre">True</span></code>, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.
See <span class="xref std std-term">the Glossary</span>.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional, default None</span></dt>
<dd>The seed of the pseudo random number generator that selects a random
feature to update.  If int, random_state is the seed used by the random
number generator; If RandomState instance, random_state is the random
number generator; If None, the random number generator is the
RandomState instance used by <cite>np.random</cite>. Used when <code class="docutils literal"><span class="pre">selection</span></code> ==
‘random’.</dd>
<dt>selection <span class="classifier-delimiter">:</span> <span class="classifier">str, default ‘cyclic’</span></dt>
<dd>If set to ‘random’, a random coefficient is updated every iteration
rather than looping over features sequentially by default. This
(setting to ‘random’) often leads to significantly faster convergence
especially when tol is higher than 1e-4</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_tasks, n_features)</span></dt>
<dd>Parameter vector (W in the cost function formula).
Note that <code class="docutils literal"><span class="pre">coef_</span></code> stores the transpose of <code class="docutils literal"><span class="pre">W</span></code>, <code class="docutils literal"><span class="pre">W.T</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_tasks,)</span></dt>
<dd>independent term in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>number of iterations run by the coordinate descent solver to reach
the specified tolerance.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">MultiTaskLasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="go">MultiTaskLasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,</span>
<span class="go">        normalize=False, random_state=None, selection=&#39;cyclic&#39;, tol=0.0001,</span>
<span class="go">        warm_start=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">[[0.89393398 0.        ]</span>
<span class="go"> [0.89393398 0.        ]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="go">[0.10606602 0.10606602]</span>
</pre></div>
</div>
<p>See also</p>
<p>MultiTaskLasso : Multi-task L1/L2 Lasso with built-in cross-validation
Lasso
MultiTaskElasticNet</p>
<p><strong>Notes</strong></p>
<p>The algorithm used to fit the model is coordinate descent.</p>
<p>To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#MultiTaskLassoScikitsLearnNode">MultiTaskLassoScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.RandomizedLogisticRegressionScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">RandomizedLogisticRegressionScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.RandomizedLogisticRegressionScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomized Logistic Regression
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.randomized_l1.RandomizedLogisticRegression</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Randomized Logistic Regression works by subsampling the training
data and fitting a L1-penalized LogisticRegression model where the
penalty of a random subset of coefficients has been scaled. By
performing this double randomization several times, the method
assigns high scores to features that are repeatedly selected across
randomizations. This is known as stability selection. In short,
features selected more often are considered good features.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>C <span class="classifier-delimiter">:</span> <span class="classifier">float or array-like of shape [n_reg_parameter], optional, default=1</span></dt>
<dd>The regularization parameter C in the LogisticRegression.
When C is an array, fit will take each regularization parameter in C
one by one for LogisticRegression and store results for each one
in <code class="docutils literal"><span class="pre">all_scores_</span></code>, where columns and rows represent corresponding
reg_parameters and features.</dd>
<dt>scaling <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default=0.5</span></dt>
<dd>The s parameter used to randomly scale the penalty of different
features.
Should be between 0 and 1.</dd>
<dt>sample_fraction <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default=0.75</span></dt>
<dd>The fraction of samples to be used in each randomized design.
Should be between 0 and 1. If 1, all samples are used.</dd>
<dt>n_resampling <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default=200</span></dt>
<dd>Number of randomized models.</dd>
<dt>selection_threshold <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default=0.25</span></dt>
<dd>The score above which features should be selected.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default=1e-3</span></dt>
<dd>tolerance for stopping criteria of LogisticRegression</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default=True</span></dt>
<dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">boolean or integer, optional</span></dt>
<dd>Sets the verbosity amount</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>If True, the regressors X will be normalized before regression.
This parameter is ignored when <cite>fit_intercept</cite> is set to False.
When the regressors are normalized, note that this makes the
hyperparameters learnt more robust and almost independent of the number
of samples. The same property is not valid for standardized data.
However, if you wish to standardize, please use
<cite>preprocessing.StandardScaler</cite> before calling <cite>fit</cite> on an estimator
with <cite>normalize=False</cite>.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Number of CPUs to use during the resampling.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>pre_dispatch <span class="classifier-delimiter">:</span> <span class="classifier">int, or string, optional</span></dt>
<dd><p class="first">Controls the number of jobs that get dispatched during parallel
execution. Reducing this number can be useful to avoid an
explosion of memory consumption when more jobs get dispatched
than CPUs can process. This parameter can be:</p>
<blockquote class="last">
<div><ul class="simple">
<li>None, in which case all the jobs are immediately
created and spawned. Use this for lightweight and
fast-running jobs, to avoid delays due to on-demand
spawning of the jobs</li>
<li>An int, giving the exact number of total jobs that are
spawned</li>
<li>A string, giving an expression as a function of n_jobs,
as in ‘2*n_jobs’</li>
</ul>
</div></blockquote>
</dd>
<dt>memory <span class="classifier-delimiter">:</span> <span class="classifier">None, str or object with the joblib.Memory interface, optional             (default=None)</span></dt>
<dd>Used for internal caching. By default, no caching is done.
If a string is given, it is the path to the caching directory.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">scores_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features]</span></dt>
<dd>Feature scores between 0 and 1.</dd>
<dt><code class="docutils literal"><span class="pre">all_scores_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features, n_reg_parameter]</span></dt>
<dd>Feature scores between 0 and 1 for all values of the regularization         parameter. The reference article suggests <code class="docutils literal"><span class="pre">scores_</span></code> is the max         of <code class="docutils literal"><span class="pre">all_scores_</span></code>.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RandomizedLogisticRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">randomized_logistic</span> <span class="o">=</span> <span class="n">RandomizedLogisticRegression</span><span class="p">()</span> 
</pre></div>
</div>
<p><strong>References</strong></p>
<p>Stability selection
Nicolai Meinshausen, Peter Buhlmann
Journal of the Royal Statistical Society: Series B
Volume 72, Issue 4, pages 417-473, September 2010
DOI: 10.1111/j.1467-9868.2010.00740.x</p>
<p>See also</p>
<p>RandomizedLasso, LogisticRegression</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#RandomizedLogisticRegressionScikitsLearnNode">RandomizedLogisticRegressionScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.SelectFweScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">SelectFweScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.SelectFweScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Filter: Select the p-values corresponding to Family-wise error rate
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.feature_selection.univariate_selection.SelectFwe</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>score_func <span class="classifier-delimiter">:</span> <span class="classifier">callable</span></dt>
<dd>Function taking two arrays X and y, and returning a pair of arrays
(scores, pvalues).
Default is f_classif (see below “See also”). The default function only
works with classification tasks.</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The highest uncorrected p-value for features to keep.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectFwe</span><span class="p">,</span> <span class="n">chi2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(569, 30)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_new</span> <span class="o">=</span> <span class="n">SelectFwe</span><span class="p">(</span><span class="n">chi2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_new</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(569, 15)</span>
</pre></div>
</div>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">scores_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>Scores of features.</dd>
<dt><code class="docutils literal"><span class="pre">pvalues_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>p-values of feature scores.</dd>
</dl>
<p>See also</p>
<p>f_classif: ANOVA F-value between label/feature for classification tasks.
chi2: Chi-squared stats of non-negative features for classification tasks.
f_regression: F-value between label/feature for regression tasks.
SelectPercentile: Select features based on percentile of the highest scores.
SelectKBest: Select features based on the k highest scores.
SelectFpr: Select features based on a false positive rate test.
SelectFdr: Select features based on an estimated false discovery rate.
GenericUnivariateSelect: Univariate feature selector with configurable mode.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.SelectFweScikitsLearnNode-class.html">SelectFweScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.MultiTaskElasticNetScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">MultiTaskElasticNetScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.MultiTaskElasticNetScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.coordinate_descent.MultiTaskElasticNet</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
The optimization objective for MultiTaskElasticNet is:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">Y</span> <span class="o">-</span> <span class="n">XW</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span>
<span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span>
<span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span>
</pre></div>
</div>
<p>Where:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span> <span class="o">=</span> <span class="n">sum_i</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">sum_j</span> <span class="n">w_ij</span> <span class="o">^</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>i.e. the sum of norm of each row.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Constant that multiplies the L1/L2 term. Defaults to 1.0</dd>
<dt>l1_ratio <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The ElasticNet mixing parameter, with 0 &lt; l1_ratio &lt;= 1.
For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it
is an L2 penalty.
For <code class="docutils literal"><span class="pre">0</span> <span class="pre">&lt;</span> <span class="pre">l1_ratio</span> <span class="pre">&lt;</span> <span class="pre">1</span></code>, the penalty is a combination of L1/L2 and L2.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span></dt>
<dd>This parameter is ignored when <code class="docutils literal"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<code class="xref py py-class docutils literal"><span class="pre">sklearn.preprocessing.StandardScaler</span></code> before calling <code class="docutils literal"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal"><span class="pre">normalize=False</span></code>.</dd>
<dt>copy_X <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>If <code class="docutils literal"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>The maximum number of iterations</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The tolerance for the optimization: if the updates are
smaller than <code class="docutils literal"><span class="pre">tol</span></code>, the optimization code checks the
dual gap for optimality and continues until it is smaller
than <code class="docutils literal"><span class="pre">tol</span></code>.</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd>When set to <code class="docutils literal"><span class="pre">True</span></code>, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.
See <span class="xref std std-term">the Glossary</span>.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional, default None</span></dt>
<dd>The seed of the pseudo random number generator that selects a random
feature to update.  If int, random_state is the seed used by the random
number generator; If RandomState instance, random_state is the random
number generator; If None, the random number generator is the
RandomState instance used by <cite>np.random</cite>. Used when <code class="docutils literal"><span class="pre">selection</span></code> ==
‘random’.</dd>
<dt>selection <span class="classifier-delimiter">:</span> <span class="classifier">str, default ‘cyclic’</span></dt>
<dd>If set to ‘random’, a random coefficient is updated every iteration
rather than looping over features sequentially by default. This
(setting to ‘random’) often leads to significantly faster convergence
especially when tol is higher than 1e-4.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_tasks,)</span></dt>
<dd>Independent term in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_tasks, n_features)</span></dt>
<dd>Parameter vector (W in the cost function formula). If a 1D y is
passed in at fit (non multi-task usage), <code class="docutils literal"><span class="pre">coef_</span></code> is then a 1D array.
Note that <code class="docutils literal"><span class="pre">coef_</span></code> stores the transpose of <code class="docutils literal"><span class="pre">W</span></code>, <code class="docutils literal"><span class="pre">W.T</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>number of iterations run by the coordinate descent solver to reach
the specified tolerance.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">MultiTaskElasticNet</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">... </span>
<span class="go">MultiTaskElasticNet(alpha=0.1, copy_X=True, fit_intercept=True,</span>
<span class="go">        l1_ratio=0.5, max_iter=1000, normalize=False, random_state=None,</span>
<span class="go">        selection=&#39;cyclic&#39;, tol=0.0001, warm_start=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">[[0.45663524 0.45612256]</span>
<span class="go"> [0.45663524 0.45612256]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="go">[0.0872422 0.0872422]</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>MultiTaskElasticNet <span class="classifier-delimiter">:</span> <span class="classifier">Multi-task L1/L2 ElasticNet with built-in</span></dt>
<dd>cross-validation.</dd>
</dl>
<p>ElasticNet
MultiTaskLasso</p>
<p><strong>Notes</strong></p>
<p>The algorithm used to fit the model is coordinate descent.</p>
<p>To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#MultiTaskElasticNetScikitsLearnNode">MultiTaskElasticNetScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.SparseCoderScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">SparseCoderScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.SparseCoderScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Sparse coding
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.decomposition.dict_learning.SparseCoder</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Finds a sparse representation of data against a fixed, precomputed
dictionary.</p>
<p>Each row of the result is the solution to a sparse coding problem.
The goal is to find a sparse array <cite>code</cite> such that:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">~=</span> <span class="n">code</span> <span class="o">*</span> <span class="n">dictionary</span>
</pre></div>
</div>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>dictionary <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>The dictionary atoms used for sparse coding. Lines are assumed to be
normalized to unit norm.</dd>
<dt>transform_algorithm <span class="classifier-delimiter">:</span> <span class="classifier">{‘lasso_lars’, ‘lasso_cd’, ‘lars’, ‘omp’,     ‘threshold’}</span></dt>
<dd><p class="first">Algorithm used to transform the data:</p>
<ul class="last simple">
<li>lars: uses the least angle regression method (linear_model.lars_path)</li>
<li>lasso_lars: uses Lars to compute the Lasso solution</li>
<li>lasso_cd: uses the coordinate descent method to compute the</li>
<li>Lasso solution (linear_model.Lasso). lasso_lars will be faster if</li>
<li>the estimated components are sparse.</li>
<li>omp: uses orthogonal matching pursuit to estimate the sparse solution</li>
<li>threshold: squashes to zero all coefficients less than alpha from</li>
<li>the projection <code class="docutils literal"><span class="pre">dictionary</span> <span class="pre">*</span> <span class="pre">X'</span></code></li>
</ul>
</dd>
<dt>transform_n_nonzero_coefs <span class="classifier-delimiter">:</span> <span class="classifier">int, <code class="docutils literal"><span class="pre">0.1</span> <span class="pre">*</span> <span class="pre">n_features</span></code> by default</span></dt>
<dd>Number of nonzero coefficients to target in each column of the
solution. This is only used by <cite>algorithm=’lars’</cite> and <cite>algorithm=’omp’</cite>
and is overridden by <cite>alpha</cite> in the <cite>omp</cite> case.</dd>
<dt>transform_alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, 1. by default</span></dt>
<dd>If <cite>algorithm=’lasso_lars’</cite> or <cite>algorithm=’lasso_cd’</cite>, <cite>alpha</cite> is the
penalty applied to the L1 norm.
If <cite>algorithm=’threshold’</cite>, <cite>alpha</cite> is the absolute value of the
threshold below which coefficients will be squashed to zero.
If <cite>algorithm=’omp’</cite>, <cite>alpha</cite> is the tolerance parameter: the value of
the reconstruction error targeted. In this case, it overrides
<cite>n_nonzero_coefs</cite>.</dd>
<dt>split_sign <span class="classifier-delimiter">:</span> <span class="classifier">bool, False by default</span></dt>
<dd>Whether to split the sparse feature vector into the concatenation of
its negative part and its positive part. This can improve the
performance of downstream classifiers.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Number of parallel jobs to run.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>positive_code <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd><p class="first">Whether to enforce positivity when finding the code.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">components_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>The unchanged dictionary atoms</dd>
</dl>
<p>See also</p>
<p>DictionaryLearning
MiniBatchDictionaryLearning
SparsePCA
MiniBatchSparsePCA
sparse_encode</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#SparseCoderScikitsLearnNode">SparseCoderScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.StandardScalerScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">StandardScalerScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.StandardScalerScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Standardize features by removing the mean and scaling to unit variance
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.preprocessing.data.StandardScaler</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
The standard score of a sample <cite>x</cite> is calculated as:</p>
<blockquote>
<div>z = (x - u) / s</div></blockquote>
<p>where <cite>u</cite> is the mean of the training samples or zero if <cite>with_mean=False</cite>,
and <cite>s</cite> is the standard deviation of the training samples or one if
<cite>with_std=False</cite>.</p>
<p>Centering and scaling happen independently on each feature by computing
the relevant statistics on the samples in the training set. Mean and
standard deviation are then stored to be used on later data using the
<cite>transform</cite> method.</p>
<p>Standardization of a dataset is a common requirement for many
machine learning estimators: they might behave badly if the
individual features do not more or less look like standard normally
distributed data (e.g. Gaussian with 0 mean and unit variance).</p>
<p>For instance many elements used in the objective function of
a learning algorithm (such as the RBF kernel of Support Vector
Machines or the L1 and L2 regularizers of linear models) assume that
all features are centered around 0 and have variance in the same
order. If a feature has a variance that is orders of magnitude larger
that others, it might dominate the objective function and make the
estimator unable to learn from other features correctly as expected.</p>
<p>This scaler can also be applied to sparse CSR or CSC matrices by passing
<cite>with_mean=False</cite> to avoid breaking the sparsity structure of the data.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>If False, try to avoid a copy and do inplace scaling instead.
This is not guaranteed to always work inplace; e.g. if the data is
not a NumPy array or scipy.sparse CSR matrix, a copy may still be
returned.</dd>
<dt>with_mean <span class="classifier-delimiter">:</span> <span class="classifier">boolean, True by default</span></dt>
<dd>If True, center the data before scaling.
This does not work (and will raise an exception) when attempted on
sparse matrices, because centering them entails building a dense
matrix which in common use cases is likely to be too large to fit in
memory.</dd>
<dt>with_std <span class="classifier-delimiter">:</span> <span class="classifier">boolean, True by default</span></dt>
<dd>If True, scale the data to unit variance (or equivalently,
unit standard deviation).</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">scale_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">ndarray or None, shape (n_features,)</span></dt>
<dd><p class="first">Per feature relative scaling of the data. This is calculated using
<cite>np.sqrt(var_)</cite>. Equal to <code class="docutils literal"><span class="pre">None</span></code> when <code class="docutils literal"><span class="pre">with_std=False</span></code>.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>scale_</em></p>
</div>
</dd>
<dt><code class="docutils literal"><span class="pre">mean_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">ndarray or None, shape (n_features,)</span></dt>
<dd>The mean value for each feature in the training set.
Equal to <code class="docutils literal"><span class="pre">None</span></code> when <code class="docutils literal"><span class="pre">with_mean=False</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">var_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">ndarray or None, shape (n_features,)</span></dt>
<dd>The variance for each feature in the training set. Used to compute
<cite>scale_</cite>. Equal to <code class="docutils literal"><span class="pre">None</span></code> when <code class="docutils literal"><span class="pre">with_std=False</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">n_samples_seen_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int or array, shape (n_features,)</span></dt>
<dd>The number of samples processed by the estimator for each feature.
If there are not missing samples, the <code class="docutils literal"><span class="pre">n_samples_seen</span></code> will be an
integer, otherwise it will be an array.
Will be reset on new calls to fit, but increments across
<code class="docutils literal"><span class="pre">partial_fit</span></code> calls.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
<span class="go">StandardScaler(copy=True, with_mean=True, with_std=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">mean_</span><span class="p">)</span>
<span class="go">[0.5 0.5]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
<span class="go">[[-1. -1.]</span>
<span class="go"> [-1. -1.]</span>
<span class="go"> [ 1.  1.]</span>
<span class="go"> [ 1.  1.]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]))</span>
<span class="go">[[3. 3.]]</span>
</pre></div>
</div>
<p>See also</p>
<p>scale: Equivalent function without the estimator API.</p>
<dl class="docutils">
<dt><code class="xref py py-class docutils literal"><span class="pre">sklearn.decomposition.PCA</span></code></dt>
<dd>Further removes the linear correlation across features with ‘whiten=True’.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>NaNs are treated as missing values: disregarded in fit, and maintained in
transform.</p>
<p>We use a biased estimator for the standard deviation, equivalent to
<cite>numpy.std(x, ddof=0)</cite>. Note that the choice of <cite>ddof</cite> is unlikely to
affect model performance.</p>
<p>For a comparison of the different scalers, transformers, and normalizers,
see <span class="xref std std-ref">examples/preprocessing/plot_all_scaling.py</span>.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#StandardScalerScikitsLearnNode">StandardScalerScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.DecisionTreeClassifierScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">DecisionTreeClassifierScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.DecisionTreeClassifierScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>A decision tree classifier.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.tree.tree.DecisionTreeClassifier</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>criterion <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=”gini”)</span></dt>
<dd>The function to measure the quality of a split. Supported criteria are
“gini” for the Gini impurity and “entropy” for the information gain.</dd>
<dt>splitter <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=”best”)</span></dt>
<dd>The strategy used to choose the split at each node. Supported
strategies are “best” to choose the best split and “random” to choose
the best random split.</dd>
<dt>max_depth <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</dd>
<dt>min_samples_split <span class="classifier-delimiter">:</span> <span class="classifier">int, float, optional (default=2)</span></dt>
<dd><p class="first">The minimum number of samples required to split an internal node:</p>
<ul class="simple">
<li>If int, then consider <cite>min_samples_split</cite> as the minimum number.</li>
<li>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</li>
</ul>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</dd>
<dt>min_samples_leaf <span class="classifier-delimiter">:</span> <span class="classifier">int, float, optional (default=1)</span></dt>
<dd><p class="first">The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul class="simple">
<li>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</li>
<li>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</li>
</ul>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</dd>
<dt>min_weight_fraction_leaf <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span></dt>
<dd>The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int, float, string or None, optional (default=None)</span></dt>
<dd><p class="first">The number of features to consider when looking for the best split:</p>
<blockquote>
<div><ul class="simple">
<li>If int, then consider <cite>max_features</cite> features at each split.</li>
<li>If float, then <cite>max_features</cite> is a fraction and
<cite>int(max_features * n_features)</cite> features are considered at each
split.</li>
<li>If “auto”, then <cite>max_features=sqrt(n_features)</cite>.</li>
<li>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite>.</li>
<li>If “log2”, then <cite>max_features=log2(n_features)</cite>.</li>
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
</div></blockquote>
<p class="last">Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal"><span class="pre">max_features</span></code> features.</p>
</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>max_leaf_nodes <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Grow a tree with <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</dd>
<dt>min_impurity_decrease <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span></dt>
<dd><p class="first">A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal"><span class="pre">N</span></code>, <code class="docutils literal"><span class="pre">N_t</span></code>, <code class="docutils literal"><span class="pre">N_t_R</span></code> and <code class="docutils literal"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19.</span></p>
</div>
</dd>
<dt>min_impurity_split <span class="classifier-delimiter">:</span> <span class="classifier">float, (default=1e-7)</span></dt>
<dd><p class="first">Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="last deprecated">
<p><span class="versionmodified">Deprecated since version 0.19: </span><code class="docutils literal"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal"><span class="pre">min_impurity_split</span></code> will change from 1e-7 to 0 in 0.23 and it
will be removed in 0.25. Use <code class="docutils literal"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</dd>
<dt>class_weight <span class="classifier-delimiter">:</span> <span class="classifier">dict, list of dicts, “balanced” or None, default=None</span></dt>
<dd><p class="first">Weights associated with classes in the form <code class="docutils literal"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
If not given, all classes are supposed to have weight one. For
multi-output problems, a list of dicts can be provided in the same
order as the columns of y.</p>
<p>Note that for multioutput (including multilabel) weights should be
defined for each class of every column in its own dict. For example,
for four-class multilabel classification weights should be
[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
[{1:1}, {2:5}, {3:1}, {4:1}].</p>
<p>The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
<p>For multi-output, the weights of each column of y will be multiplied.</p>
<p class="last">Note that these weights will be multiplied with sample_weight (passed
through the fit method) if sample_weight is specified.</p>
</dd>
<dt>presort <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default=False)</span></dt>
<dd>Whether to presort the data to speed up the finding of best splits in
fitting. For the default settings of a decision tree on large
datasets, setting this to true may slow down the training process.
When using either a smaller dataset or a restricted depth, this may
speed up the training.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">classes_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_classes] or a list of such arrays</span></dt>
<dd>The classes labels (single output problem),
or a list of arrays of class labels (multi-output problem).</dd>
<dt><code class="docutils literal"><span class="pre">feature_importances_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span></dt>
<dd>The feature importances. The higher, the more important the
feature. The importance of a feature is computed as the (normalized)
total reduction of the criterion brought by that feature.  It is also
known as the Gini importance <a href="#id91"><span class="problematic" id="id38">[4]_</span></a>.</dd>
<dt><code class="docutils literal"><span class="pre">max_features_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>The inferred value of max_features.</dd>
<dt><code class="docutils literal"><span class="pre">n_classes_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int or list</span></dt>
<dd>The number of classes (for single output problems),
or a list containing the number of classes for each
output (for multi-output problems).</dd>
<dt><code class="docutils literal"><span class="pre">n_features_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The number of features when <code class="docutils literal"><span class="pre">fit</span></code> is performed.</dd>
<dt><code class="docutils literal"><span class="pre">n_outputs_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The number of outputs when <code class="docutils literal"><span class="pre">fit</span></code> is performed.</dd>
<dt><code class="docutils literal"><span class="pre">tree_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">Tree object</span></dt>
<dd>The underlying Tree object. Please refer to
<code class="docutils literal"><span class="pre">help(sklearn.tree._tree.Tree)</span></code> for attributes of Tree object and
<span class="xref std std-ref">sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py</span>
for basic usage of these attributes.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code class="docutils literal"><span class="pre">max_depth</span></code>, <code class="docutils literal"><span class="pre">min_samples_leaf</span></code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<p>The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data and
<code class="docutils literal"><span class="pre">max_features=n_features</span></code>, if the improvement of the criterion is
identical for several splits enumerated during the search of the best
split. To obtain a deterministic behaviour during fitting,
<code class="docutils literal"><span class="pre">random_state</span></code> has to be fixed.</p>
<p>See also</p>
<p>DecisionTreeRegressor</p>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id39" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><a class="reference external" href="https://en.wikipedia.org/wiki/Decision_tree_learning">https://en.wikipedia.org/wiki/Decision_tree_learning</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id40" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>L. Breiman, J. Friedman, R. Olshen, and C. Stone, “Classification
and Regression Trees”, Wadsworth, Belmont, CA, 1984.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id41" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td>T. Hastie, R. Tibshirani and J. Friedman. “Elements of Statistical
Learning”, Springer, 2009.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id42" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td>L. Breiman, and A. Cutler, “Random Forests”,
<a class="reference external" href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm">https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm</a></td></tr>
</tbody>
</table>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">... </span>                            
<span class="gp">...</span>
<span class="go">array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,</span>
<span class="go">        0.93...,  0.93...,  1.     ,  0.93...,  1.      ])</span>
</pre></div>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#DecisionTreeClassifierScikitsLearnNode">DecisionTreeClassifierScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.GenericUnivariateSelectScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">GenericUnivariateSelectScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.GenericUnivariateSelectScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Univariate feature selector with configurable strategy.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.feature_selection.univariate_selection.GenericUnivariateSelect</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>score_func <span class="classifier-delimiter">:</span> <span class="classifier">callable</span></dt>
<dd>Function taking two arrays X and y, and returning a pair of arrays
(scores, pvalues). For modes ‘percentile’ or ‘kbest’ it can return
a single array scores.</dd>
<dt>mode <span class="classifier-delimiter">:</span> <span class="classifier">{‘percentile’, ‘k_best’, ‘fpr’, ‘fdr’, ‘fwe’}</span></dt>
<dd>Feature selection mode.</dd>
<dt>param <span class="classifier-delimiter">:</span> <span class="classifier">float or int depending on the feature selection mode</span></dt>
<dd>Parameter of the corresponding mode.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">scores_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>Scores of features.</dd>
<dt><code class="docutils literal"><span class="pre">pvalues_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>p-values of feature scores, None if <cite>score_func</cite> returned scores only.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">GenericUnivariateSelect</span><span class="p">,</span> <span class="n">chi2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(569, 30)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span> <span class="o">=</span> <span class="n">GenericUnivariateSelect</span><span class="p">(</span><span class="n">chi2</span><span class="p">,</span> <span class="s1">&#39;k_best&#39;</span><span class="p">,</span> <span class="n">param</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_new</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_new</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(569, 20)</span>
</pre></div>
</div>
<p>See also</p>
<p>f_classif: ANOVA F-value between label/feature for classification tasks.
mutual_info_classif: Mutual information for a discrete target.
chi2: Chi-squared stats of non-negative features for classification tasks.
f_regression: F-value between label/feature for regression tasks.
mutual_info_regression: Mutual information for a continuous target.
SelectPercentile: Select features based on percentile of the highest scores.
SelectKBest: Select features based on the k highest scores.
SelectFpr: Select features based on a false positive rate test.
SelectFdr: Select features based on an estimated false discovery rate.
SelectFwe: Select features based on family-wise error rate.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.GenericUnivariateSelectScikitsLearnNode-class.html">GenericUnivariateSelectScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.BernoulliNBScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">BernoulliNBScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.BernoulliNBScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Naive Bayes classifier for multivariate Bernoulli models.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.naive_bayes.BernoulliNB</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Like MultinomialNB, this classifier is suitable for discrete data. The
difference is that while MultinomialNB works with occurrence counts,
BernoulliNB is designed for binary/boolean features.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span></dt>
<dd>Additive (Laplace/Lidstone) smoothing parameter
(0 for no smoothing).</dd>
<dt>binarize <span class="classifier-delimiter">:</span> <span class="classifier">float or None, optional (default=0.0)</span></dt>
<dd>Threshold for binarizing (mapping to booleans) of sample features.
If None, input is presumed to already consist of binary vectors.</dd>
<dt>fit_prior <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span></dt>
<dd>Whether to learn class prior probabilities or not.
If false, a uniform prior will be used.</dd>
<dt>class_prior <span class="classifier-delimiter">:</span> <span class="classifier">array-like, size=[n_classes,], optional (default=None)</span></dt>
<dd>Prior probabilities of the classes. If specified the priors are not
adjusted according to the data.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">class_log_prior_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes]</span></dt>
<dd>Log probability of each class (smoothed).</dd>
<dt><code class="docutils literal"><span class="pre">feature_log_prob_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes, n_features]</span></dt>
<dd>Empirical log probability of features given a class, P(x_i|y).</dd>
<dt><code class="docutils literal"><span class="pre">class_count_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes]</span></dt>
<dd>Number of samples encountered for each class during fitting. This
value is weighted by the sample weight when provided.</dd>
<dt><code class="docutils literal"><span class="pre">feature_count_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes, n_features]</span></dt>
<dd>Number of samples encountered for each (class, feature)
during fitting. This value is weighted by the sample weight when
provided.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">BernoulliNB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">BernoulliNB</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="go">BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">3</span><span class="p">]))</span>
<span class="go">[3]</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to
Information Retrieval. Cambridge University Press, pp. 234-265.
<a class="reference external" href="http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html">http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html</a></p>
<p>A. McCallum and K. Nigam (1998). A comparison of event models for naive
Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for
Text Categorization, pp. 41-48.</p>
<p>V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with
naive Bayes – Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS).</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#BernoulliNBScikitsLearnNode">BernoulliNBScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.LogisticRegressionScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">LogisticRegressionScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.LogisticRegressionScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Logistic Regression (aka logit, MaxEnt) classifier.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.logistic.LogisticRegression</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
In the multiclass case, the training algorithm uses the one-vs-rest (OvR)
scheme if the ‘multi_class’ option is set to ‘ovr’, and uses the cross-
entropy loss if the ‘multi_class’ option is set to ‘multinomial’.
(Currently the ‘multinomial’ option is supported only by the ‘lbfgs’,
‘sag’ and ‘newton-cg’ solvers.)</p>
<p>This class implements regularized logistic regression using the
‘liblinear’ library, ‘newton-cg’, ‘sag’ and ‘lbfgs’ solvers. It can handle
both dense and sparse input. Use C-ordered arrays or CSR matrices
containing 64-bit floats for optimal performance; any other input format
will be converted (and copied).</p>
<p>The ‘newton-cg’, ‘sag’, and ‘lbfgs’ solvers support only L2 regularization
with primal formulation. The ‘liblinear’ solver supports both L1 and L2
regularization, with a dual formulation only for the L2 penalty.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>penalty <span class="classifier-delimiter">:</span> <span class="classifier">str, ‘l1’ or ‘l2’, default: ‘l2’</span></dt>
<dd><p class="first">Used to specify the norm used in the penalization. The ‘newton-cg’,
‘sag’ and ‘lbfgs’ solvers support only l2 penalties.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19: </span>l1 penalty with SAGA solver (allowing ‘multinomial’ + L1)</p>
</div>
</dd>
<dt>dual <span class="classifier-delimiter">:</span> <span class="classifier">bool, default: False</span></dt>
<dd>Dual or primal formulation. Dual formulation is only implemented for
l2 penalty with liblinear solver. Prefer dual=False when
n_samples &gt; n_features.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, default: 1e-4</span></dt>
<dd>Tolerance for stopping criteria.</dd>
<dt>C <span class="classifier-delimiter">:</span> <span class="classifier">float, default: 1.0</span></dt>
<dd>Inverse of regularization strength; must be a positive float.
Like in support vector machines, smaller values specify stronger
regularization.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">bool, default: True</span></dt>
<dd>Specifies if a constant (a.k.a. bias or intercept) should be
added to the decision function.</dd>
<dt>intercept_scaling <span class="classifier-delimiter">:</span> <span class="classifier">float, default 1.</span></dt>
<dd><p class="first">Useful only when the solver ‘liblinear’ is used
and self.fit_intercept is set to True. In this case, x becomes
[x, self.intercept_scaling],
i.e. a “synthetic” feature with constant value equal to
intercept_scaling is appended to the instance vector.
The intercept becomes <code class="docutils literal"><span class="pre">intercept_scaling</span> <span class="pre">*</span> <span class="pre">synthetic_feature_weight</span></code>.</p>
<p class="last">Note! the synthetic feature weight is subject to l1/l2 regularization
as all other features.
To lessen the effect of regularization on synthetic feature weight
(and therefore on the intercept) intercept_scaling has to be increased.</p>
</dd>
<dt>class_weight <span class="classifier-delimiter">:</span> <span class="classifier">dict or ‘balanced’, default: None</span></dt>
<dd><p class="first">Weights associated with classes in the form <code class="docutils literal"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
If not given, all classes are supposed to have weight one.</p>
<p>The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code>.</p>
<p>Note that these weights will be multiplied with sample_weight (passed
through the fit method) if sample_weight is specified.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>class_weight=’balanced’</em></p>
</div>
</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional, default: None</span></dt>
<dd>The seed of the pseudo random number generator to use when shuffling
the data.  If int, random_state is the seed used by the random number
generator; If RandomState instance, random_state is the random number
generator; If None, the random number generator is the RandomState
instance used by <cite>np.random</cite>. Used when <code class="docutils literal"><span class="pre">solver</span></code> == ‘sag’ or
‘liblinear’.</dd>
</dl>
<p>solver : str, {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’},              default: ‘liblinear’.</p>
<blockquote>
<div><p>Algorithm to use in the optimization problem.</p>
<ul class="simple">
<li>For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and
‘saga’ are faster for large ones.</li>
<li>For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’
handle multinomial loss; ‘liblinear’ is limited to one-versus-rest
schemes.</li>
<li>‘newton-cg’, ‘lbfgs’ and ‘sag’ only handle L2 penalty, whereas
‘liblinear’ and ‘saga’ handle L1 penalty.</li>
</ul>
<p>Note that ‘sag’ and ‘saga’ fast convergence is only guaranteed on
features with approximately the same scale. You can
preprocess the data with a scaler from sklearn.preprocessing.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17: </span>Stochastic Average Gradient descent solver.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.19: </span>SAGA solver.</p>
</div>
<div class="versionchanged">
<p><span class="versionmodified">Changed in version 0.20: </span>Default will change from ‘liblinear’ to ‘lbfgs’ in 0.22.</p>
</div>
</div></blockquote>
<dl class="docutils">
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, default: 100</span></dt>
<dd>Useful only for the newton-cg, sag and lbfgs solvers.
Maximum number of iterations taken for the solvers to converge.</dd>
<dt>multi_class <span class="classifier-delimiter">:</span> <span class="classifier">str, {‘ovr’, ‘multinomial’, ‘auto’}, default: ‘ovr’</span></dt>
<dd><p class="first">If the option chosen is ‘ovr’, then a binary problem is fit for each
label. For ‘multinomial’ the loss minimised is the multinomial loss fit
across the entire probability distribution, <em>even when the data is
binary</em>. ‘multinomial’ is unavailable when solver=’liblinear’.
‘auto’ selects ‘ovr’ if the data is binary, or if solver=’liblinear’,
and otherwise selects ‘multinomial’.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.18: </span>Stochastic Average Gradient descent solver for ‘multinomial’ case.</p>
</div>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.20: </span>Default will change from ‘ovr’ to ‘auto’ in 0.22.</p>
</div>
</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, default: 0</span></dt>
<dd>For the liblinear and lbfgs solvers set verbose to any positive
number for verbosity.</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, default: False</span></dt>
<dd><p class="first">When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.
Useless for liblinear solver. See <span class="xref std std-term">the Glossary</span>.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>warm_start</em> to support <em>lbfgs</em>, <em>newton-cg</em>, <em>sag</em>, <em>saga</em> solvers.</p>
</div>
</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Number of CPU cores used when parallelizing over classes if
multi_class=’ovr’”. This parameter is ignored when the <code class="docutils literal"><span class="pre">solver</span></code> is
set to ‘liblinear’ regardless of whether ‘multi_class’ is specified or
not. <code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code>
context. <code class="docutils literal"><span class="pre">-1</span></code> means using all processors.
See <span class="xref std std-term">Glossary</span> for more details.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">classes_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes, )</span></dt>
<dd>A list of class labels known to the classifier.</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (1, n_features) or (n_classes, n_features)</span></dt>
<dd><p class="first">Coefficient of the features in the decision function.</p>
<p class="last"><cite>coef_</cite> is of shape (1, n_features) when the given problem is binary.
In particular, when <cite>multi_class=’multinomial’</cite>, <cite>coef_</cite> corresponds
to outcome 1 (True) and <cite>-coef_</cite> corresponds to outcome 0 (False).</p>
</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (1,) or (n_classes,)</span></dt>
<dd><p class="first">Intercept (a.k.a. bias) added to the decision function.</p>
<p class="last">If <cite>fit_intercept</cite> is set to False, the intercept is set to zero.
<cite>intercept_</cite> is of shape (1,) when the given problem is binary.
In particular, when <cite>multi_class=’multinomial’</cite>, <cite>intercept_</cite>
corresponds to outcome 1 (True) and <cite>-intercept_</cite> corresponds to
outcome 0 (False).</p>
</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes,) or (1, )</span></dt>
<dd><p class="first">Actual number of iterations for all classes. If binary or multinomial,
it returns only 1 element. For liblinear solver, only the maximum
number of iteration across all classes is given.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.20: </span>In SciPy &lt;= 1.0.0 the number of lbfgs iterations may exceed
<code class="docutils literal"><span class="pre">max_iter</span></code>. <code class="docutils literal"><span class="pre">n_iter_</span></code> will now report at most <code class="docutils literal"><span class="pre">max_iter</span></code>.</p>
</div>
</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span>
<span class="gp">... </span>                         <span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;multinomial&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">2</span><span class="p">,</span> <span class="p">:])</span>
<span class="go">array([0, 0])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">2</span><span class="p">,</span> <span class="p">:])</span> 
<span class="go">array([[9.8...e-01, 1.8...e-02, 1.4...e-08],</span>
<span class="go">       [9.7...e-01, 2.8...e-02, ...e-08]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">0.97...</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>SGDClassifier <span class="classifier-delimiter">:</span> <span class="classifier">incrementally trained logistic regression (when given</span></dt>
<dd>the parameter <code class="docutils literal"><span class="pre">loss=&quot;log&quot;</span></code>).</dd>
</dl>
<p>LogisticRegressionCV : Logistic regression with built-in cross validation</p>
<p><strong>Notes</strong></p>
<p>The underlying C implementation uses a random number generator to
select features when fitting the model. It is thus not uncommon,
to have slightly different results for the same input data. If
that happens, try with a smaller tol parameter.</p>
<p>Predict output may not match that of standalone liblinear in certain
cases. See <span class="xref std std-ref">differences from liblinear</span>
in the narrative documentation.</p>
<p><strong>References</strong></p>
<dl class="docutils">
<dt>LIBLINEAR – A Library for Large Linear Classification</dt>
<dd><a class="reference external" href="https://www.csie.ntu.edu.tw/~cjlin/liblinear/">https://www.csie.ntu.edu.tw/~cjlin/liblinear/</a></dd>
<dt>SAG – Mark Schmidt, Nicolas Le Roux, and Francis Bach</dt>
<dd>Minimizing Finite Sums with the Stochastic Average Gradient
<a class="reference external" href="https://hal.inria.fr/hal-00860051/document">https://hal.inria.fr/hal-00860051/document</a></dd>
<dt>SAGA – Defazio, A., Bach F. &amp; Lacoste-Julien S. (2014).</dt>
<dd>SAGA: A Fast Incremental Gradient Method With Support
for Non-Strongly Convex Composite Objectives
<a class="reference external" href="https://arxiv.org/abs/1407.0202">https://arxiv.org/abs/1407.0202</a></dd>
<dt>Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent</dt>
<dd>methods for logistic regression and maximum entropy models.
Machine Learning 85(1-2):41-75.
<a class="reference external" href="https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf">https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf</a></dd>
</dl>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.LogisticRegressionScikitsLearnNode-class.html">LogisticRegressionScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.ComplementNBScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">ComplementNBScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.ComplementNBScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>The Complement Naive Bayes classifier described in Rennie et al. (2003).
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.naive_bayes.ComplementNB</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
The Complement Naive Bayes classifier was designed to correct the “severe
assumptions” made by the standard Multinomial Naive Bayes classifier. It is
particularly suited for imbalanced data sets.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span></dt>
<dd>Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).</dd>
<dt>fit_prior <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span></dt>
<dd>Only used in edge case with a single class in the training set.</dd>
<dt>class_prior <span class="classifier-delimiter">:</span> <span class="classifier">array-like, size (n_classes,), optional (default=None)</span></dt>
<dd>Prior probabilities of the classes. Not used.</dd>
<dt>norm <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=False)</span></dt>
<dd>Whether or not a second normalization of the weights is performed. The
default behavior mirrors the implementations found in Mahout and Weka,
which do not follow the full algorithm described in Table 9 of the
paper.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">class_log_prior_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes, )</span></dt>
<dd>Smoothed empirical log probability for each class. Only used in edge
case with a single class in the training set.</dd>
<dt><code class="docutils literal"><span class="pre">feature_log_prob_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes, n_features)</span></dt>
<dd>Empirical weights for class complements.</dd>
<dt><code class="docutils literal"><span class="pre">class_count_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes,)</span></dt>
<dd>Number of samples encountered for each class during fitting. This
value is weighted by the sample weight when provided.</dd>
<dt><code class="docutils literal"><span class="pre">feature_count_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes, n_features)</span></dt>
<dd>Number of samples encountered for each (class, feature) during fitting.
This value is weighted by the sample weight when provided.</dd>
<dt><code class="docutils literal"><span class="pre">feature_all_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,)</span></dt>
<dd>Number of samples encountered for each feature during fitting. This
value is weighted by the sample weight when provided.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">ComplementNB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">ComplementNB</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">3</span><span class="p">]))</span>
<span class="go">[3]</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>Rennie, J. D., Shih, L., Teevan, J., &amp; Karger, D. R. (2003).
Tackling the poor assumptions of naive bayes text classifiers. In ICML
(Vol. 3, pp. 616-623).
<a class="reference external" href="https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf">https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf</a></p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#ComplementNBScikitsLearnNode">ComplementNBScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.NuSVCScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">NuSVCScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.NuSVCScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Nu-Support Vector Classification.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.svm.classes.NuSVC</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Similar to SVC but uses a parameter to control the number of support
vectors.</p>
<p>The implementation is based on libsvm.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>nu <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.5)</span></dt>
<dd>An upper bound on the fraction of training errors and a lower
bound of the fraction of support vectors. Should be in the
interval (0, 1].</dd>
<dt>kernel <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=’rbf’)</span></dt>
<dd>Specifies the kernel type to be used in the algorithm.
It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or
a callable.
If none is given, ‘rbf’ will be used. If a callable is given it is
used to precompute the kernel matrix.</dd>
<dt>degree <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=3)</span></dt>
<dd>Degree of the polynomial kernel function (‘poly’).
Ignored by all other kernels.</dd>
<dt>gamma <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=’auto’)</span></dt>
<dd><p class="first">Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.</p>
<p class="last">Current default is ‘auto’ which uses 1 / n_features,
if <code class="docutils literal"><span class="pre">gamma='scale'</span></code> is passed then it uses 1 / (n_features * X.var())
as value of gamma. The current default of gamma, ‘auto’, will change
to ‘scale’ in version 0.22. ‘auto_deprecated’, a deprecated version of
‘auto’ is used as a default indicating that no explicit value of gamma
was passed.</p>
</dd>
<dt>coef0 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.0)</span></dt>
<dd>Independent term in kernel function.
It is only significant in ‘poly’ and ‘sigmoid’.</dd>
<dt>shrinking <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span></dt>
<dd>Whether to use the shrinking heuristic.</dd>
<dt>probability <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=False)</span></dt>
<dd>Whether to enable probability estimates. This must be enabled prior
to calling <cite>fit</cite>, and will slow down that method.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1e-3)</span></dt>
<dd>Tolerance for stopping criterion.</dd>
<dt>cache_size <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Specify the size of the kernel cache (in MB).</dd>
<dt>class_weight <span class="classifier-delimiter">:</span> <span class="classifier">{dict, ‘balanced’}, optional</span></dt>
<dd>Set the parameter C of class i to class_weight[i]*C for
SVC. If not given, all classes are supposed to have
weight one. The “balanced” mode uses the values of y to automatically
adjust weights inversely proportional to class frequencies as
<code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">bool, default: False</span></dt>
<dd>Enable verbose output. Note that this setting takes advantage of a
per-process runtime setting in libsvm that, if enabled, may not work
properly in a multithreaded context.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=-1)</span></dt>
<dd>Hard limit on iterations within solver, or -1 for no limit.</dd>
<dt>decision_function_shape <span class="classifier-delimiter">:</span> <span class="classifier">‘ovo’, ‘ovr’, default=’ovr’</span></dt>
<dd><p class="first">Whether to return a one-vs-rest (‘ovr’) decision function of shape
(n_samples, n_classes) as all other classifiers, or the original
one-vs-one (‘ovo’) decision function of libsvm which has shape
(n_samples, n_classes * (n_classes - 1) / 2).</p>
<div class="versionchanged">
<p><span class="versionmodified">Changed in version 0.19: </span>decision_function_shape is ‘ovr’ by default.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>decision_function_shape=’ovr’</em> is recommended.</p>
</div>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.17: </span>Deprecated <em>decision_function_shape=’ovo’ and None</em>.</p>
</div>
</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>The seed of the pseudo random number generator used when shuffling
the data for probability estimates. If int, random_state is the seed
used by the random number generator; If RandomState instance,
random_state is the random number generator; If None, the random
number generator is the RandomState instance used by <cite>np.random</cite>.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">support_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_SV]</span></dt>
<dd>Indices of support vectors.</dd>
<dt><code class="docutils literal"><span class="pre">support_vectors_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_SV, n_features]</span></dt>
<dd>Support vectors.</dd>
<dt><code class="docutils literal"><span class="pre">n_support_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, dtype=int32, shape = [n_class]</span></dt>
<dd>Number of support vectors for each class.</dd>
<dt><code class="docutils literal"><span class="pre">dual_coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_class-1, n_SV]</span></dt>
<dd>Coefficients of the support vector in the decision function.
For multiclass, coefficient for all 1-vs-1 classifiers.
The layout of the coefficients in the multiclass case is somewhat
non-trivial. See the section about multi-class classification in
the SVM section of the User Guide for details.</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_class * (n_class-1) / 2, n_features]</span></dt>
<dd><p class="first">Weights assigned to the features (coefficients in the primal
problem). This is only available in the case of a linear kernel.</p>
<p class="last"><cite>coef_</cite> is readonly property derived from <cite>dual_coef_</cite> and
<cite>support_vectors_</cite>.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_class * (n_class-1) / 2]</span></dt>
<dd>Constants in decision function.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">NuSVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">NuSVC</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;scale&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">NuSVC(cache_size=200, class_weight=None, coef0=0.0,</span>
<span class="go">      decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;scale&#39;, kernel=&#39;rbf&#39;,</span>
<span class="go">      max_iter=-1, nu=0.5, probability=False, random_state=None,</span>
<span class="go">      shrinking=True, tol=0.001, verbose=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>SVC</dt>
<dd>Support Vector Machine for classification using libsvm.</dd>
<dt>LinearSVC</dt>
<dd>Scalable linear Support Vector Machine for classification using
liblinear.</dd>
</dl>
<p><strong>Notes</strong></p>
<p><strong>References:</strong>
<a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf">LIBSVM: A Library for Support Vector Machines</a></p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.NuSVCScikitsLearnNode-class.html">NuSVCScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.NearestCentroidScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">NearestCentroidScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.NearestCentroidScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Nearest centroid classifier.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.neighbors.nearest_centroid.NearestCentroid</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Each class is represented by its centroid, with test samples classified to
the class with the nearest centroid.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>metric <span class="classifier-delimiter">:</span> <span class="classifier">string, or callable</span></dt>
<dd>The metric to use when calculating distance between instances in a
feature array. If metric is a string or callable, it must be one of
the options allowed by metrics.pairwise.pairwise_distances for its
metric parameter.
The centroids for the samples corresponding to each class is the point
from which the sum of the distances (according to the metric) of all
samples that belong to that particular class are minimized.
If the “manhattan” metric is provided, this centroid is the median and
for all other metrics, the centroid is now set to be the mean.</dd>
<dt>shrink_threshold <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default = None)</span></dt>
<dd>Threshold for shrinking centroids to remove features.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">centroids_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_classes, n_features]</span></dt>
<dd>Centroid of each class</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors.nearest_centroid</span> <span class="kn">import</span> <span class="n">NearestCentroid</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">NearestCentroid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">NearestCentroid(metric=&#39;euclidean&#39;, shrink_threshold=None)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<p>See also</p>
<p>sklearn.neighbors.KNeighborsClassifier: nearest neighbors classifier</p>
<p><strong>Notes</strong></p>
<p>When used for text classification with tf-idf vectors, this classifier is
also known as the Rocchio classifier.</p>
<p><strong>References</strong></p>
<p>Tibshirani, R., Hastie, T., Narasimhan, B., &amp; Chu, G. (2002). Diagnosis of
multiple cancer types by shrunken centroids of gene expression. Proceedings
of the National Academy of Sciences of the United States of America,
99(10), 6567-6572. The National Academy of Sciences.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#NearestCentroidScikitsLearnNode">NearestCentroidScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.MiniBatchKMeansScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">MiniBatchKMeansScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.MiniBatchKMeansScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Mini-Batch K-Means clustering
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.cluster.k_means_.MiniBatchKMeans</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_clusters <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default: 8</span></dt>
<dd>The number of clusters to form as well as the number of
centroids to generate.</dd>
<dt>init <span class="classifier-delimiter">:</span> <span class="classifier">{‘k-means++’, ‘random’ or an ndarray}, default: ‘k-means++’</span></dt>
<dd><p class="first">Method for initialization, defaults to ‘k-means++’:</p>
<p>‘k-means++’ : selects initial cluster centers for k-mean
clustering in a smart way to speed up convergence. See section
Notes in k_init for more details.</p>
<p>‘random’: choose k observations (rows) at random from data for
the initial centroids.</p>
<p class="last">If an ndarray is passed, it should be of shape (n_clusters, n_features)
and gives the initial centers.</p>
</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Maximum number of iterations over the complete dataset before
stopping independently of any early stopping criterion heuristics.</dd>
<dt>batch_size <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default: 100</span></dt>
<dd>Size of the mini batches.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>Verbosity mode.</dd>
<dt>compute_labels <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=True</span></dt>
<dd>Compute label assignment and inertia for the complete dataset
once the minibatch optimization has converged in fit.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None (default)</span></dt>
<dd>Determines random number generation for centroid initialization and
random reassignment. Use an int to make the randomness deterministic.
See <span class="xref std std-term">Glossary</span>.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, default: 0.0</span></dt>
<dd><p class="first">Control early stopping based on the relative center changes as
measured by a smoothed, variance-normalized of the mean center
squared position changes. This early stopping heuristics is
closer to the one used for the batch variant of the algorithms
but induces a slight computational and memory overhead over the
inertia heuristic.</p>
<p class="last">To disable convergence detection based on normalized center
change, set tol to 0.0 (default).</p>
</dd>
<dt>max_no_improvement <span class="classifier-delimiter">:</span> <span class="classifier">int, default: 10</span></dt>
<dd><p class="first">Control early stopping based on the consecutive number of mini
batches that does not yield an improvement on the smoothed inertia.</p>
<p class="last">To disable convergence detection based on inertia, set
max_no_improvement to None.</p>
</dd>
<dt>init_size <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default: 3 * batch_size</span></dt>
<dd>Number of samples to randomly sample for speeding up the
initialization (sometimes at the expense of accuracy): the
only algorithm is initialized by running a batch KMeans on a
random subset of the data. This needs to be larger than n_clusters.</dd>
<dt>n_init <span class="classifier-delimiter">:</span> <span class="classifier">int, default=3</span></dt>
<dd>Number of random initializations that are tried.
In contrast to KMeans, the algorithm is only run once, using the
best of the <code class="docutils literal"><span class="pre">n_init</span></code> initializations as measured by inertia.</dd>
<dt>reassignment_ratio <span class="classifier-delimiter">:</span> <span class="classifier">float, default: 0.01</span></dt>
<dd>Control the fraction of the maximum number of counts for a
center to be reassigned. A higher value means that low count
centers are more easily reassigned, which means that the
model will take longer to converge, but should converge in a
better clustering.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">cluster_centers_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_clusters, n_features]</span></dt>
<dd>Coordinates of cluster centers</dd>
</dl>
<p><code class="docutils literal"><span class="pre">labels_</span></code> :</p>
<blockquote>
<div><ul class="simple">
<li>Labels of each point (if compute_labels is set to True).</li>
</ul>
</div></blockquote>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">inertia_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The value of the inertia criterion associated with the chosen
partition (if compute_labels is set to True). The inertia is
defined as the sum of square distances of samples to their nearest
neighbor.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">MiniBatchKMeans</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>              <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
<span class="gp">... </span>              <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
<span class="gp">... </span>              <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># manually fit on batches</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kmeans</span> <span class="o">=</span> <span class="n">MiniBatchKMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">batch_size</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kmeans</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">partial_fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">6</span><span class="p">,:])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kmeans</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">partial_fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">6</span><span class="p">:</span><span class="mi">12</span><span class="p">,:])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span>
<span class="go">array([[1, 1],</span>
<span class="go">       [3, 4]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kmeans</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="go">array([0, 1], dtype=int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># fit on the whole data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kmeans</span> <span class="o">=</span> <span class="n">MiniBatchKMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">batch_size</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">max_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span>
<span class="go">array([[3.95918367, 2.40816327],</span>
<span class="go">       [1.12195122, 1.3902439 ]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kmeans</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="go">array([1, 0], dtype=int32)</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>KMeans</dt>
<dd>The classic implementation of the clustering method based on the
Lloyd’s algorithm. It consumes the whole set of input data at each
iteration.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>See <a class="reference external" href="http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf">http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf</a></p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#MiniBatchKMeansScikitsLearnNode">MiniBatchKMeansScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.LassoLarsScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">LassoLarsScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.LassoLarsScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Lasso model fit with Least Angle Regression a.k.a. Lars
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.least_angle.LassoLars</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
It is a Linear Model trained with an L1 prior as regularizer.</p>
<p>The optimization objective for Lasso is:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2_2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
</pre></div>
</div>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Constant that multiplies the penalty term. Defaults to 1.0.
<code class="docutils literal"><span class="pre">alpha</span> <span class="pre">=</span> <span class="pre">0</span></code> is equivalent to an ordinary least square, solved
by <code class="xref py py-class docutils literal"><span class="pre">LinearRegression</span></code>. For numerical reasons, using
<code class="docutils literal"><span class="pre">alpha</span> <span class="pre">=</span> <span class="pre">0</span></code> with the LassoLars object is not advised and you
should prefer the LinearRegression object.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">boolean or integer, optional</span></dt>
<dd>Sets the verbosity amount</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>This parameter is ignored when <code class="docutils literal"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<code class="xref py py-class docutils literal"><span class="pre">sklearn.preprocessing.StandardScaler</span></code> before calling <code class="docutils literal"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal"><span class="pre">normalize=False</span></code>.</dd>
<dt>precompute <span class="classifier-delimiter">:</span> <span class="classifier">True | False | ‘auto’ | array-like</span></dt>
<dd>Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal"><span class="pre">'auto'</span></code> let us decide. The Gram
matrix can also be passed as argument.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span></dt>
<dd>Maximum number of iterations to perform.</dd>
<dt>eps <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The machine-precision regularization in the computation of the
Cholesky diagonal factors. Increase this for very ill-conditioned
systems. Unlike the <code class="docutils literal"><span class="pre">tol</span></code> parameter in some iterative
optimization-based algorithms, this parameter does not control
the tolerance of the optimization.</dd>
<dt>copy_X <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>If True, X will be copied; else, it may be overwritten.</dd>
<dt>fit_path <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>If <code class="docutils literal"><span class="pre">True</span></code> the full path is stored in the <code class="docutils literal"><span class="pre">coef_path_</span></code> attribute.
If you compute the solution for a large problem or many targets,
setting <code class="docutils literal"><span class="pre">fit_path</span></code> to <code class="docutils literal"><span class="pre">False</span></code> will lead to a speedup, especially
with a small alpha.</dd>
<dt>positive <span class="classifier-delimiter">:</span> <span class="classifier">boolean (default=False)</span></dt>
<dd>Restrict coefficients to be &gt;= 0. Be aware that you might want to
remove fit_intercept which is set True by default.
Under the positive restriction the model coefficients will not converge
to the ordinary-least-squares solution for small values of alpha.
Only coefficients up to the smallest alpha value (<code class="docutils literal"><span class="pre">alphas_[alphas_</span> <span class="pre">&gt;</span>
<span class="pre">0.].min()</span></code> when fit_path=True) reached by the stepwise Lars-Lasso
algorithm are typically in congruence with the solution of the
coordinate descent Lasso estimator.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">alphas_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_alphas + 1,) | list of n_targets such arrays</span></dt>
<dd>Maximum of covariances (in absolute value) at each iteration.         <code class="docutils literal"><span class="pre">n_alphas</span></code> is either <code class="docutils literal"><span class="pre">max_iter</span></code>, <code class="docutils literal"><span class="pre">n_features</span></code>, or the number of         nodes in the path with correlation greater than <code class="docutils literal"><span class="pre">alpha</span></code>, whichever         is smaller.</dd>
<dt><code class="docutils literal"><span class="pre">active_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list, length = n_alphas | list of n_targets such lists</span></dt>
<dd>Indices of active variables at the end of the path.</dd>
<dt><code class="docutils literal"><span class="pre">coef_path_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features, n_alphas + 1) or list</span></dt>
<dd>If a list is passed it’s expected to be one of n_targets such arrays.
The varying values of the coefficients along the path. It is not
present if the <code class="docutils literal"><span class="pre">fit_path</span></code> parameter is <code class="docutils literal"><span class="pre">False</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,) or (n_targets, n_features)</span></dt>
<dd>Parameter vector (w in the formulation formula).</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float | array, shape (n_targets,)</span></dt>
<dd>Independent term in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like or int.</span></dt>
<dd>The number of iterations taken by lars_path to find the
grid of alphas for each target.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LassoLars</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="gp">... </span>
<span class="go">LassoLars(alpha=0.01, copy_X=True, eps=..., fit_intercept=True,</span>
<span class="go">     fit_path=True, max_iter=500, normalize=True, positive=False,</span>
<span class="go">     precompute=&#39;auto&#39;, verbose=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span> 
<span class="go">[ 0.         -0.963257...]</span>
</pre></div>
</div>
<p>See also</p>
<p>lars_path
lasso_path
Lasso
LassoCV
LassoLarsCV
LassoLarsIC
sklearn.decomposition.sparse_encode</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#LassoLarsScikitsLearnNode">LassoLarsScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.LassoScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">LassoScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.LassoScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Linear Model trained with L1 prior as regularizer (aka the Lasso)
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.coordinate_descent.Lasso</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
The optimization objective for Lasso is:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2_2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
</pre></div>
</div>
<p>Technically the Lasso model is optimizing the same objective function as
the Elastic Net with <code class="docutils literal"><span class="pre">l1_ratio=1.0</span></code> (no L2 penalty).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Constant that multiplies the L1 term. Defaults to 1.0.
<code class="docutils literal"><span class="pre">alpha</span> <span class="pre">=</span> <span class="pre">0</span></code> is equivalent to an ordinary least square, solved
by the <code class="xref py py-class docutils literal"><span class="pre">LinearRegression</span></code> object. For numerical
reasons, using <code class="docutils literal"><span class="pre">alpha</span> <span class="pre">=</span> <span class="pre">0</span></code> with the <code class="docutils literal"><span class="pre">Lasso</span></code> object is not advised.
Given this, you should use the <code class="xref py py-class docutils literal"><span class="pre">LinearRegression</span></code> object.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>Whether to calculate the intercept for this model. If set
to False, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span></dt>
<dd>This parameter is ignored when <code class="docutils literal"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<code class="xref py py-class docutils literal"><span class="pre">sklearn.preprocessing.StandardScaler</span></code> before calling <code class="docutils literal"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal"><span class="pre">normalize=False</span></code>.</dd>
<dt>precompute <span class="classifier-delimiter">:</span> <span class="classifier">True | False | array-like, default=False</span></dt>
<dd>Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal"><span class="pre">'auto'</span></code> let us decide. The Gram
matrix can also be passed as argument. For sparse input
this option is always <code class="docutils literal"><span class="pre">True</span></code> to preserve sparsity.</dd>
<dt>copy_X <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>If <code class="docutils literal"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>The maximum number of iterations</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The tolerance for the optimization: if the updates are
smaller than <code class="docutils literal"><span class="pre">tol</span></code>, the optimization code checks the
dual gap for optimality and continues until it is smaller
than <code class="docutils literal"><span class="pre">tol</span></code>.</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd>When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.
See <span class="xref std std-term">the Glossary</span>.</dd>
<dt>positive <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd>When set to <code class="docutils literal"><span class="pre">True</span></code>, forces the coefficients to be positive.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional, default None</span></dt>
<dd>The seed of the pseudo random number generator that selects a random
feature to update.  If int, random_state is the seed used by the random
number generator; If RandomState instance, random_state is the random
number generator; If None, the random number generator is the
RandomState instance used by <cite>np.random</cite>. Used when <code class="docutils literal"><span class="pre">selection</span></code> ==
‘random’.</dd>
<dt>selection <span class="classifier-delimiter">:</span> <span class="classifier">str, default ‘cyclic’</span></dt>
<dd>If set to ‘random’, a random coefficient is updated every iteration
rather than looping over features sequentially by default. This
(setting to ‘random’) often leads to significantly faster convergence
especially when tol is higher than 1e-4.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,) | (n_targets, n_features)</span></dt>
<dd>parameter vector (w in the cost function formula)</dd>
<dt><code class="docutils literal"><span class="pre">sparse_coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">scipy.sparse matrix, shape (n_features, 1) |             (n_targets, n_features)</span></dt>
<dd><code class="docutils literal"><span class="pre">sparse_coef_</span></code> is a readonly property derived from <code class="docutils literal"><span class="pre">coef_</span></code></dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float | array, shape (n_targets,)</span></dt>
<dd>independent term in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int | array-like, shape (n_targets,)</span></dt>
<dd>number of iterations run by the coordinate descent solver to reach
the specified tolerance.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="go">Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,</span>
<span class="go">   normalize=False, positive=False, precompute=False, random_state=None,</span>
<span class="go">   selection=&#39;cyclic&#39;, tol=0.0001, warm_start=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">[0.85 0.  ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>  
<span class="go">0.15...</span>
</pre></div>
</div>
<p>See also</p>
<p>lars_path
lasso_path
LassoLars
LassoCV
LassoLarsCV
sklearn.decomposition.sparse_encode</p>
<p><strong>Notes</strong></p>
<p>The algorithm used to fit the model is coordinate descent.</p>
<p>To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.LassoScikitsLearnNode-class.html">LassoScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.RANSACRegressorScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">RANSACRegressorScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.RANSACRegressorScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>RANSAC (RANdom SAmple Consensus) algorithm.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.ransac.RANSACRegressor</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
RANSAC is an iterative algorithm for the robust estimation of parameters
from a subset of inliers from the complete data set. More information can
be found in the general documentation of linear models.</p>
<p>A detailed description of the algorithm can be found in the documentation
of the <code class="docutils literal"><span class="pre">linear_model</span></code> sub-package.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>base_estimator <span class="classifier-delimiter">:</span> <span class="classifier">object, optional</span></dt>
<dd><p class="first">Base estimator object which implements the following methods:</p>
<blockquote>
<div><ul class="simple">
<li><cite>fit(X, y)</cite>: Fit model to given training data and target values.</li>
<li><cite>score(X, y)</cite>: Returns the mean accuracy on the given test data,
which is used for the stop criterion defined by <cite>stop_score</cite>.
Additionally, the score is used to decide which of two equally
large consensus sets is chosen as the better one.</li>
<li><cite>predict(X)</cite>: Returns predicted values using the linear model,
which is used to compute residual error using loss function.</li>
</ul>
</div></blockquote>
<p>If <cite>base_estimator</cite> is None, then
<code class="docutils literal"><span class="pre">base_estimator=sklearn.linear_model.LinearRegression()</span></code> is used for
target values of dtype float.</p>
<p class="last">Note that the current implementation only supports regression
estimators.</p>
</dd>
<dt>min_samples <span class="classifier-delimiter">:</span> <span class="classifier">int (&gt;= 1) or float ([0, 1]), optional</span></dt>
<dd>Minimum number of samples chosen randomly from original data. Treated
as an absolute number of samples for <cite>min_samples &gt;= 1</cite>, treated as a
relative number <cite>ceil(min_samples * X.shape[0]</cite>) for
<cite>min_samples &lt; 1</cite>. This is typically chosen as the minimal number of
samples necessary to estimate the given <cite>base_estimator</cite>. By default a
<code class="docutils literal"><span class="pre">sklearn.linear_model.LinearRegression()</span></code> estimator is assumed and
<cite>min_samples</cite> is chosen as <code class="docutils literal"><span class="pre">X.shape[1]</span> <span class="pre">+</span> <span class="pre">1</span></code>.</dd>
<dt>residual_threshold <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Maximum residual for a data sample to be classified as an inlier.
By default the threshold is chosen as the MAD (median absolute
deviation) of the target values <cite>y</cite>.</dd>
<dt>is_data_valid <span class="classifier-delimiter">:</span> <span class="classifier">callable, optional</span></dt>
<dd>This function is called with the randomly selected data before the
model is fitted to it: <cite>is_data_valid(X, y)</cite>. If its return value is
False the current randomly chosen sub-sample is skipped.</dd>
<dt>is_model_valid <span class="classifier-delimiter">:</span> <span class="classifier">callable, optional</span></dt>
<dd>This function is called with the estimated model and the randomly
selected data: <cite>is_model_valid(model, X, y)</cite>. If its return value is
False the current randomly chosen sub-sample is skipped.
Rejecting samples with this function is computationally costlier than
with <cite>is_data_valid</cite>. <cite>is_model_valid</cite> should therefore only be used if
the estimated model is needed for making the rejection decision.</dd>
<dt>max_trials <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Maximum number of iterations for random sample selection.</dd>
<dt>max_skips <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd><p class="first">Maximum number of iterations that can be skipped due to finding zero
inliers or invalid data defined by <code class="docutils literal"><span class="pre">is_data_valid</span></code> or invalid models
defined by <code class="docutils literal"><span class="pre">is_model_valid</span></code>.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19.</span></p>
</div>
</dd>
<dt>stop_n_inliers <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Stop iteration if at least this number of inliers are found.</dd>
<dt>stop_score <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Stop iteration if score is greater equal than this threshold.</dd>
<dt>stop_probability <span class="classifier-delimiter">:</span> <span class="classifier">float in range [0, 1], optional</span></dt>
<dd><p class="first">RANSAC iteration stops if at least one outlier-free set of the training
data is sampled in RANSAC. This requires to generate at least N
samples (iterations):</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">N</span> <span class="o">&gt;=</span> <span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">probability</span><span class="p">)</span> <span class="o">/</span> <span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">e</span><span class="o">**</span><span class="n">m</span><span class="p">)</span>
</pre></div>
</div>
<p class="last">where the probability (confidence) is typically set to high value such
as 0.99 (the default) and e is the current fraction of inliers w.r.t.
the total number of samples.</p>
</dd>
<dt>loss <span class="classifier-delimiter">:</span> <span class="classifier">string, callable, optional, default “absolute_loss”</span></dt>
<dd><p class="first">String inputs, “absolute_loss” and “squared_loss” are supported which
find the absolute loss and squared loss per sample
respectively.</p>
<p>If <code class="docutils literal"><span class="pre">loss</span></code> is a callable, then it should be a function that takes
two arrays as inputs, the true and predicted value and returns a 1-D
array with the i-th value of the array corresponding to the loss
on <code class="docutils literal"><span class="pre">X[i]</span></code>.</p>
<p class="last">If the loss on a sample is greater than the <code class="docutils literal"><span class="pre">residual_threshold</span></code>,
then this sample is classified as an outlier.</p>
</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional, default None</span></dt>
<dd>The generator used to initialize the centers.  If int, random_state is
the seed used by the random number generator; If RandomState instance,
random_state is the random number generator; If None, the random number
generator is the RandomState instance used by <cite>np.random</cite>.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">estimator_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">object</span></dt>
<dd>Best fitted model (copy of the <cite>base_estimator</cite> object).</dd>
<dt><code class="docutils literal"><span class="pre">n_trials_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of random selection trials until one of the stop criteria is
met. It is always <code class="docutils literal"><span class="pre">&lt;=</span> <span class="pre">max_trials</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">inlier_mask_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">bool array of shape [n_samples]</span></dt>
<dd>Boolean mask of inliers classified as <code class="docutils literal"><span class="pre">True</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">n_skips_no_inliers_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd><p class="first">Number of iterations skipped due to finding zero inliers.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19.</span></p>
</div>
</dd>
<dt><code class="docutils literal"><span class="pre">n_skips_invalid_data_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd><p class="first">Number of iterations skipped due to invalid data defined by
<code class="docutils literal"><span class="pre">is_data_valid</span></code>.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19.</span></p>
</div>
</dd>
<dt><code class="docutils literal"><span class="pre">n_skips_invalid_model_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd><p class="first">Number of iterations skipped due to an invalid model defined by
<code class="docutils literal"><span class="pre">is_model_valid</span></code>.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19.</span></p>
</div>
</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RANSACRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">RANSACRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">0.9885...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">1</span><span class="p">,])</span>
<span class="go">array([-31.9417...])</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id43" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><a class="reference external" href="https://en.wikipedia.org/wiki/RANSAC">https://en.wikipedia.org/wiki/RANSAC</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id44" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><a class="reference external" href="https://www.sri.com/sites/default/files/publications/ransac-publication.pdf">https://www.sri.com/sites/default/files/publications/ransac-publication.pdf</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id45" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td><a class="reference external" href="http://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf">http://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf</a></td></tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#RANSACRegressorScikitsLearnNode">RANSACRegressorScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.HuberRegressorScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">HuberRegressorScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.HuberRegressorScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Linear regression model that is robust to outliers.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.huber.HuberRegressor</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
The Huber Regressor optimizes the squared loss for the samples where
<code class="docutils literal"><span class="pre">|(y</span> <span class="pre">-</span> <span class="pre">X'w)</span> <span class="pre">/</span> <span class="pre">sigma|</span> <span class="pre">&lt;</span> <span class="pre">epsilon</span></code> and the absolute loss for the samples
where <code class="docutils literal"><span class="pre">|(y</span> <span class="pre">-</span> <span class="pre">X'w)</span> <span class="pre">/</span> <span class="pre">sigma|</span> <span class="pre">&gt;</span> <span class="pre">epsilon</span></code>, where w and sigma are parameters
to be optimized. The parameter sigma makes sure that if y is scaled up
or down by a certain factor, one does not need to rescale epsilon to
achieve the same robustness. Note that this does not take into account
the fact that the different features of X may be of different scales.</p>
<p>This makes sure that the loss function is not heavily influenced by the
outliers while not completely ignoring their effect.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span></p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.18.</span></p>
</div>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>epsilon <span class="classifier-delimiter">:</span> <span class="classifier">float, greater than 1.0, default 1.35</span></dt>
<dd>The parameter epsilon controls the number of samples that should be
classified as outliers. The smaller the epsilon, the more robust it is
to outliers.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, default 100</span></dt>
<dd>Maximum number of iterations that scipy.optimize.fmin_l_bfgs_b
should run for.</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, default 0.0001</span></dt>
<dd>Regularization parameter.</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, default False</span></dt>
<dd>This is useful if the stored attributes of a previously used model
has to be reused. If set to False, then the coefficients will
be rewritten for every call to fit.
See <span class="xref std std-term">the Glossary</span>.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">bool, default True</span></dt>
<dd>Whether or not to fit the intercept. This can be set to False
if the data is already centered around the origin.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, default 1e-5</span></dt>
<dd>The iteration will stop when
<code class="docutils literal"><span class="pre">max{|proj</span> <span class="pre">g_i</span> <span class="pre">|</span> <span class="pre">i</span> <span class="pre">=</span> <span class="pre">1,</span> <span class="pre">...,</span> <span class="pre">n}</span></code> &lt;= <code class="docutils literal"><span class="pre">tol</span></code>
where pg_i is the i-th component of the projected gradient.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,)</span></dt>
<dd>Features got by optimizing the Huber loss.</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Bias.</dd>
<dt><code class="docutils literal"><span class="pre">scale_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The value by which <code class="docutils literal"><span class="pre">|y</span> <span class="pre">-</span> <span class="pre">X'w</span> <span class="pre">-</span> <span class="pre">c|</span></code> is scaled down.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd><p class="first">Number of iterations that fmin_l_bfgs_b has run for.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.20: </span>In SciPy &lt;= 1.0.0 the number of lbfgs iterations may exceed
<code class="docutils literal"><span class="pre">max_iter</span></code>. <code class="docutils literal"><span class="pre">n_iter_</span></code> will now report at most <code class="docutils literal"><span class="pre">max_iter</span></code>.</p>
</div>
</dd>
<dt><code class="docutils literal"><span class="pre">outliers_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_samples,)</span></dt>
<dd>A boolean mask which is set to True where the samples are identified
as outliers.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">HuberRegressor</span><span class="p">,</span> <span class="n">LinearRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">coef</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">coef</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">[:</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="p">[:</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">huber</span> <span class="o">=</span> <span class="n">HuberRegressor</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">huber</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">-7.284608623514573</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">huber</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">1</span><span class="p">,])</span>
<span class="go">array([806.7200...])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True coefficients:&quot;</span><span class="p">,</span> <span class="n">coef</span><span class="p">)</span>
<span class="go">True coefficients: [20.4923...  34.1698...]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Huber coefficients:&quot;</span><span class="p">,</span> <span class="n">huber</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">Huber coefficients: [17.7906... 31.0106...]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Linear Regression coefficients:&quot;</span><span class="p">,</span> <span class="n">linear</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">Linear Regression coefficients: [-1.9221...  7.0226...]</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id46" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Peter J. Huber, Elvezio M. Ronchetti, Robust Statistics
Concomitant scale estimates, pg 172</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id47" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>Art B. Owen (2006), A robust hybrid of lasso and ridge regression.
<a class="reference external" href="http://statweb.stanford.edu/~owen/reports/hhu.pdf">http://statweb.stanford.edu/~owen/reports/hhu.pdf</a></td></tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#HuberRegressorScikitsLearnNode">HuberRegressorScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.TruncatedSVDScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">TruncatedSVDScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.TruncatedSVDScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Dimensionality reduction using truncated SVD (aka LSA).
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.decomposition.truncated_svd.TruncatedSVD</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
This transformer performs linear dimensionality reduction by means of
truncated singular value decomposition (SVD). Contrary to PCA, this
estimator does not center the data before computing the singular value
decomposition. This means it can work with scipy.sparse matrices
efficiently.</p>
<p>In particular, truncated SVD works on term count/tf-idf matrices as
returned by the vectorizers in sklearn.feature_extraction.text. In that
context, it is known as latent semantic analysis (LSA).</p>
<p>This estimator supports two algorithms: a fast randomized SVD solver, and
a “naive” algorithm that uses ARPACK as an eigensolver on (X * X.T) or
(X.T * X), whichever is more efficient.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int, default = 2</span></dt>
<dd>Desired dimensionality of output data.
Must be strictly less than the number of features.
The default value is useful for visualisation. For LSA, a value of
100 is recommended.</dd>
<dt>algorithm <span class="classifier-delimiter">:</span> <span class="classifier">string, default = “randomized”</span></dt>
<dd>SVD solver to use. Either “arpack” for the ARPACK wrapper in SciPy
(scipy.sparse.linalg.svds), or “randomized” for the randomized
algorithm due to Halko (2009).</dd>
<dt>n_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default 5)</span></dt>
<dd>Number of iterations for randomized SVD solver. Not used by ARPACK.
The default is larger than the default in <cite>randomized_svd</cite> to handle
sparse matrices that may have large slowly decaying spectrum.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional, default = None</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Tolerance for ARPACK. 0 means machine precision. Ignored by randomized
SVD solver.</dd>
</dl>
<p><strong>Attributes</strong></p>
<p><code class="docutils literal"><span class="pre">components_</span></code> : array, shape (n_components, n_features)</p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">explained_variance_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_components,)</span></dt>
<dd>The variance of the training samples transformed by a projection to
each component.</dd>
<dt><code class="docutils literal"><span class="pre">explained_variance_ratio_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_components,)</span></dt>
<dd>Percentage of variance explained by each of the selected components.</dd>
<dt><code class="docutils literal"><span class="pre">singular_values_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_components,)</span></dt>
<dd>The singular values corresponding to each of the selected components.
The singular values are equal to the 2-norms of the <code class="docutils literal"><span class="pre">n_components</span></code>
variables in the lower-dimensional space.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">TruncatedSVD</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.random_projection</span> <span class="kn">import</span> <span class="n">sparse_random_matrix</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">sparse_random_matrix</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">svd</span> <span class="o">=</span> <span class="n">TruncatedSVD</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">svd</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  
<span class="go">TruncatedSVD(algorithm=&#39;randomized&#39;, n_components=5, n_iter=7,</span>
<span class="go">        random_state=42, tol=0.0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">svd</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span>  
<span class="go">[0.0606... 0.0584... 0.0497... 0.0434... 0.0372...]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">svd</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>  
<span class="go">0.249...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">svd</span><span class="o">.</span><span class="n">singular_values_</span><span class="p">)</span>  
<span class="go">[2.5841... 2.5245... 2.3201... 2.1753... 2.0443...]</span>
</pre></div>
</div>
<p>See also</p>
<p>PCA</p>
<p><strong>References</strong></p>
<p>Finding structure with randomness: Stochastic algorithms for constructing
approximate matrix decompositions
Halko, et al., 2009 (arXiv:909) https://arxiv.org/pdf/0909.4061.pdf</p>
<p><strong>Notes</strong></p>
<p>SVD suffers from a problem called “sign indeterminacy”, which means the
sign of the <code class="docutils literal"><span class="pre">components_</span></code> and the output from transform depend on the
algorithm and random state. To work around this, fit instances of this
class to data once, then keep the instance around to do transformations.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#TruncatedSVDScikitsLearnNode">TruncatedSVDScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.SelectFprScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">SelectFprScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.SelectFprScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Filter: Select the pvalues below alpha based on a FPR test.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.feature_selection.univariate_selection.SelectFpr</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
FPR test stands for False Positive Rate test. It controls the total
amount of false detections.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>score_func <span class="classifier-delimiter">:</span> <span class="classifier">callable</span></dt>
<dd>Function taking two arrays X and y, and returning a pair of arrays
(scores, pvalues).
Default is f_classif (see below “See also”). The default function only
works with classification tasks.</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The highest p-value for features to be kept.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">scores_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>Scores of features.</dd>
<dt><code class="docutils literal"><span class="pre">pvalues_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>p-values of feature scores.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectFpr</span><span class="p">,</span> <span class="n">chi2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(569, 30)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_new</span> <span class="o">=</span> <span class="n">SelectFpr</span><span class="p">(</span><span class="n">chi2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_new</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(569, 16)</span>
</pre></div>
</div>
<p>See also</p>
<p>f_classif: ANOVA F-value between label/feature for classification tasks.
chi2: Chi-squared stats of non-negative features for classification tasks.
mutual_info_classif:</p>
<p>f_regression: F-value between label/feature for regression tasks.
mutual_info_regression: Mutual information between features and the target.
SelectPercentile: Select features based on percentile of the highest scores.
SelectKBest: Select features based on the k highest scores.
SelectFdr: Select features based on an estimated false discovery rate.
SelectFwe: Select features based on family-wise error rate.
GenericUnivariateSelect: Univariate feature selector with configurable mode.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.SelectFprScikitsLearnNode-class.html">SelectFprScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.NuSVRScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">NuSVRScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.NuSVRScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Nu Support Vector Regression.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.svm.classes.NuSVR</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Similar to NuSVC, for regression, uses a parameter nu to control
the number of support vectors. However, unlike NuSVC, where nu
replaces C, here nu replaces the parameter epsilon of epsilon-SVR.</p>
<p>The implementation is based on libsvm.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>nu <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>An upper bound on the fraction of training errors and a lower bound of
the fraction of support vectors. Should be in the interval (0, 1].  By
default 0.5 will be taken.</dd>
<dt>C <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span></dt>
<dd>Penalty parameter C of the error term.</dd>
<dt>kernel <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=’rbf’)</span></dt>
<dd>Specifies the kernel type to be used in the algorithm.
It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or
a callable.
If none is given, ‘rbf’ will be used. If a callable is given it is
used to precompute the kernel matrix.</dd>
<dt>degree <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=3)</span></dt>
<dd>Degree of the polynomial kernel function (‘poly’).
Ignored by all other kernels.</dd>
<dt>gamma <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=’auto’)</span></dt>
<dd><p class="first">Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.</p>
<p class="last">Current default is ‘auto’ which uses 1 / n_features,
if <code class="docutils literal"><span class="pre">gamma='scale'</span></code> is passed then it uses 1 / (n_features * X.var())
as value of gamma. The current default of gamma, ‘auto’, will change
to ‘scale’ in version 0.22. ‘auto_deprecated’, a deprecated version of
‘auto’ is used as a default indicating that no explicit value of gamma
was passed.</p>
</dd>
<dt>coef0 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.0)</span></dt>
<dd>Independent term in kernel function.
It is only significant in ‘poly’ and ‘sigmoid’.</dd>
<dt>shrinking <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span></dt>
<dd>Whether to use the shrinking heuristic.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1e-3)</span></dt>
<dd>Tolerance for stopping criterion.</dd>
<dt>cache_size <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Specify the size of the kernel cache (in MB).</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">bool, default: False</span></dt>
<dd>Enable verbose output. Note that this setting takes advantage of a
per-process runtime setting in libsvm that, if enabled, may not work
properly in a multithreaded context.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=-1)</span></dt>
<dd>Hard limit on iterations within solver, or -1 for no limit.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">support_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_SV]</span></dt>
<dd>Indices of support vectors.</dd>
<dt><code class="docutils literal"><span class="pre">support_vectors_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [nSV, n_features]</span></dt>
<dd>Support vectors.</dd>
<dt><code class="docutils literal"><span class="pre">dual_coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1, n_SV]</span></dt>
<dd>Coefficients of the support vector in the decision function.</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1, n_features]</span></dt>
<dd><p class="first">Weights assigned to the features (coefficients in the primal
problem). This is only available in the case of a linear kernel.</p>
<p class="last"><cite>coef_</cite> is readonly property derived from <cite>dual_coef_</cite> and
<cite>support_vectors_</cite>.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1]</span></dt>
<dd>Constants in decision function.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">NuSVR</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">NuSVR</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  
<span class="go">NuSVR(C=1.0, cache_size=200, coef0=0.0, degree=3, gamma=&#39;scale&#39;,</span>
<span class="go">      kernel=&#39;rbf&#39;, max_iter=-1, nu=0.1, shrinking=True, tol=0.001,</span>
<span class="go">      verbose=False)</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>NuSVC</dt>
<dd>Support Vector Machine for classification implemented with libsvm
with a parameter to control the number of support vectors.</dd>
<dt>SVR</dt>
<dd>epsilon Support Vector Machine for regression implemented with libsvm.</dd>
</dl>
<p><strong>Notes</strong></p>
<p><strong>References:</strong>
<a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf">LIBSVM: A Library for Support Vector Machines</a></p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.NuSVRScikitsLearnNode-class.html">NuSVRScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.LogOddsEstimatorScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">LogOddsEstimatorScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.LogOddsEstimatorScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.ensemble.gradient_boosting.LogOddsEstimator</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#LogOddsEstimatorScikitsLearnNode">LogOddsEstimatorScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.RandomizedSearchCVScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">RandomizedSearchCVScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.RandomizedSearchCVScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomized search on hyper parameters.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.model_selection._search.RandomizedSearchCV</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
RandomizedSearchCV implements a “fit” and a “score” method.
It also implements “predict”, “predict_proba”, “decision_function”,
“transform” and “inverse_transform” if they are implemented in the
estimator used.</p>
<p>The parameters of the estimator used to apply these methods are optimized
by cross-validated search over parameter settings.</p>
<p>In contrast to GridSearchCV, not all parameter values are tried out, but
rather a fixed number of parameter settings is sampled from the specified
distributions. The number of parameter settings that are tried is
given by n_iter.</p>
<p>If all parameters are presented as a list,
sampling without replacement is performed. If at least one parameter
is given as a distribution, sampling with replacement is used.
It is highly recommended to use continuous distributions for continuous
parameters.</p>
<p>Note that before SciPy 0.16, the <code class="docutils literal"><span class="pre">scipy.stats.distributions</span></code> do not
accept a custom RNG instance and always use the singleton RNG from
<code class="docutils literal"><span class="pre">numpy.random</span></code>. Hence setting <code class="docutils literal"><span class="pre">random_state</span></code> will not guarantee a
deterministic iteration whenever <code class="docutils literal"><span class="pre">scipy.stats</span></code> distributions are used to
define the parameter search space.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>estimator <span class="classifier-delimiter">:</span> <span class="classifier">estimator object.</span></dt>
<dd>A object of that type is instantiated for each grid point.
This is assumed to implement the scikit-learn estimator interface.
Either estimator needs to provide a <code class="docutils literal"><span class="pre">score</span></code> function,
or <code class="docutils literal"><span class="pre">scoring</span></code> must be passed.</dd>
<dt>param_distributions <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd>Dictionary with parameters names (string) as keys and distributions
or lists of parameters to try. Distributions must provide a <code class="docutils literal"><span class="pre">rvs</span></code>
method for sampling (such as those from scipy.stats.distributions).
If a list is given, it is sampled uniformly.</dd>
<dt>n_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, default=10</span></dt>
<dd>Number of parameter settings that are sampled. n_iter trades
off runtime vs quality of the solution.</dd>
<dt>scoring <span class="classifier-delimiter">:</span> <span class="classifier">string, callable, list/tuple, dict or None, default: None</span></dt>
<dd><p class="first">A single string (see <span class="xref std std-ref">scoring_parameter</span>) or a callable
(see <span class="xref std std-ref">scoring</span>) to evaluate the predictions on the test set.</p>
<p>For evaluating multiple metrics, either give a list of (unique) strings
or a dict with names as keys and callables as values.</p>
<p>NOTE that when using custom scorers, each scorer should return a single
value. Metric functions returning a list/array of values can be wrapped
into multiple scorers that return one value each.</p>
<p>See <span class="xref std std-ref">multimetric_grid_search</span> for an example.</p>
<p class="last">If None, the estimator’s default scorer (if available) is used.</p>
</dd>
<dt>fit_params <span class="classifier-delimiter">:</span> <span class="classifier">dict, optional</span></dt>
<dd><p class="first">Parameters to pass to the fit method.</p>
<div class="last deprecated">
<p><span class="versionmodified">Deprecated since version 0.19: </span><code class="docutils literal"><span class="pre">fit_params</span></code> as a constructor argument was deprecated in version
0.19 and will be removed in version 0.21. Pass fit parameters to
the <code class="docutils literal"><span class="pre">fit</span></code> method instead.</p>
</div>
</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Number of jobs to run in parallel.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>pre_dispatch <span class="classifier-delimiter">:</span> <span class="classifier">int, or string, optional</span></dt>
<dd><p class="first">Controls the number of jobs that get dispatched during parallel
execution. Reducing this number can be useful to avoid an
explosion of memory consumption when more jobs get dispatched
than CPUs can process. This parameter can be:</p>
<blockquote class="last">
<div><ul class="simple">
<li>None, in which case all the jobs are immediately
created and spawned. Use this for lightweight and
fast-running jobs, to avoid delays due to on-demand
spawning of the jobs</li>
<li>An int, giving the exact number of total jobs that are
spawned</li>
<li>A string, giving an expression as a function of n_jobs,
as in ‘2*n_jobs’</li>
</ul>
</div></blockquote>
</dd>
<dt>iid <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=’warn’</span></dt>
<dd><p class="first">If True, return the average score across folds, weighted by the number
of samples in each test set. In this case, the data is assumed to be
identically distributed across the folds, and the loss minimized is
the total loss per sample, and not the mean loss across the folds. If
False, return the average score across folds. Default is True, but
will change to False in version 0.21, to correspond to the standard
definition of cross-validation.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.20: </span>Parameter <code class="docutils literal"><span class="pre">iid</span></code> will change from True to False by default in
version 0.22, and will be removed in 0.24.</p>
</div>
</dd>
<dt>cv <span class="classifier-delimiter">:</span> <span class="classifier">int, cross-validation generator or an iterable, optional</span></dt>
<dd><p class="first">Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul class="simple">
<li>None, to use the default 3-fold cross validation,</li>
<li>integer, to specify the number of folds in a <cite>(Stratified)KFold</cite>,</li>
<li><span class="xref std std-term">CV splitter</span>,</li>
<li>An iterable yielding (train, test) splits as arrays of indices.</li>
</ul>
<p>For integer/None inputs, if the estimator is a classifier and <code class="docutils literal"><span class="pre">y</span></code> is
either binary or multiclass, <code class="xref py py-class docutils literal"><span class="pre">StratifiedKFold</span></code> is used. In all
other cases, <code class="xref py py-class docutils literal"><span class="pre">KFold</span></code> is used.</p>
<p>Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.20: </span><code class="docutils literal"><span class="pre">cv</span></code> default value if None will change from 3-fold to 5-fold
in v0.22.</p>
</div>
</dd>
<dt>refit <span class="classifier-delimiter">:</span> <span class="classifier">boolean, or string default=True</span></dt>
<dd><p class="first">Refit an estimator using the best found parameters on the whole
dataset.</p>
<p>For multiple metric evaluation, this needs to be a string denoting the
scorer that would be used to find the best parameters for refitting
the estimator at the end.</p>
<p>The refitted estimator is made available at the <code class="docutils literal"><span class="pre">best_estimator_</span></code>
attribute and permits using <code class="docutils literal"><span class="pre">predict</span></code> directly on this
<code class="docutils literal"><span class="pre">RandomizedSearchCV</span></code> instance.</p>
<p>Also for multiple metric evaluation, the attributes <code class="docutils literal"><span class="pre">best_index_</span></code>,
<code class="docutils literal"><span class="pre">best_score_</span></code> and <code class="docutils literal"><span class="pre">best_params_</span></code> will only be available if
<code class="docutils literal"><span class="pre">refit</span></code> is set and all of them will be determined w.r.t this specific
scorer.</p>
<p class="last">See <code class="docutils literal"><span class="pre">scoring</span></code> parameter to know more about multiple metric
evaluation.</p>
</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd>Controls the verbosity: the higher, the more messages.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional, default=None</span></dt>
<dd>Pseudo random number generator state used for random uniform sampling
from lists of possible values instead of scipy.stats distributions.
If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>error_score <span class="classifier-delimiter">:</span> <span class="classifier">‘raise’ or numeric</span></dt>
<dd>Value to assign to the score if an error occurs in estimator fitting.
If set to ‘raise’, the error is raised. If a numeric value is given,
FitFailedWarning is raised. This parameter does not affect the refit
step, which will always raise the error. Default is ‘raise’ but from
version 0.22 it will change to np.nan.</dd>
<dt>return_train_score <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd><p class="first">If <code class="docutils literal"><span class="pre">False</span></code>, the <code class="docutils literal"><span class="pre">cv_results_</span></code> attribute will not include training
scores.</p>
<p class="last">Current default is <code class="docutils literal"><span class="pre">'warn'</span></code>, which behaves as <code class="docutils literal"><span class="pre">True</span></code> in addition
to raising a warning when a training score is looked up.
That default will be changed to <code class="docutils literal"><span class="pre">False</span></code> in 0.21.
Computing training scores is used to get insights on how different
parameter settings impact the overfitting/underfitting trade-off.
However computing the scores on the training set can be computationally
expensive and is not strictly required to select the parameters that
yield the best generalization performance.</p>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">cv_results_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">dict of numpy (masked) ndarrays</span></dt>
<dd><p class="first">A dict with keys as column headers and values as columns, that can be
imported into a pandas <code class="docutils literal"><span class="pre">DataFrame</span></code>.</p>
<p>For instance the below given table</p>
<table border="1" class="docutils">
<colgroup>
<col width="22%" />
<col width="20%" />
<col width="30%" />
<col width="5%" />
<col width="23%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">param_kernel</th>
<th class="head">param_gamma</th>
<th class="head">split0_test_score</th>
<th class="head">…</th>
<th class="head">rank_test_score</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>‘rbf’</td>
<td>0.1</td>
<td>0.80</td>
<td>…</td>
<td>2</td>
</tr>
<tr class="row-odd"><td>‘rbf’</td>
<td>0.2</td>
<td>0.90</td>
<td>…</td>
<td>1</td>
</tr>
<tr class="row-even"><td>‘rbf’</td>
<td>0.3</td>
<td>0.70</td>
<td>…</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>will be represented by a <code class="docutils literal"><span class="pre">cv_results_</span></code> dict of:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="s1">&#39;param_kernel&#39;</span> <span class="p">:</span> <span class="n">masked_array</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="s1">&#39;rbf&#39;</span><span class="p">],</span>
                              <span class="n">mask</span> <span class="o">=</span> <span class="kc">False</span><span class="p">),</span>
<span class="s1">&#39;param_gamma&#39;</span>  <span class="p">:</span> <span class="n">masked_array</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span> <span class="mf">0.2</span> <span class="mf">0.3</span><span class="p">],</span> <span class="n">mask</span> <span class="o">=</span> <span class="kc">False</span><span class="p">),</span>
<span class="s1">&#39;split0_test_score&#39;</span>  <span class="p">:</span> <span class="p">[</span><span class="mf">0.80</span><span class="p">,</span> <span class="mf">0.90</span><span class="p">,</span> <span class="mf">0.70</span><span class="p">],</span>
<span class="s1">&#39;split1_test_score&#39;</span>  <span class="p">:</span> <span class="p">[</span><span class="mf">0.82</span><span class="p">,</span> <span class="mf">0.50</span><span class="p">,</span> <span class="mf">0.70</span><span class="p">],</span>
<span class="s1">&#39;mean_test_score&#39;</span>    <span class="p">:</span> <span class="p">[</span><span class="mf">0.81</span><span class="p">,</span> <span class="mf">0.70</span><span class="p">,</span> <span class="mf">0.70</span><span class="p">],</span>
<span class="s1">&#39;std_test_score&#39;</span>     <span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.20</span><span class="p">,</span> <span class="mf">0.00</span><span class="p">],</span>
<span class="s1">&#39;rank_test_score&#39;</span>    <span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="s1">&#39;split0_train_score&#39;</span> <span class="p">:</span> <span class="p">[</span><span class="mf">0.80</span><span class="p">,</span> <span class="mf">0.92</span><span class="p">,</span> <span class="mf">0.70</span><span class="p">],</span>
<span class="s1">&#39;split1_train_score&#39;</span> <span class="p">:</span> <span class="p">[</span><span class="mf">0.82</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.70</span><span class="p">],</span>
<span class="s1">&#39;mean_train_score&#39;</span>   <span class="p">:</span> <span class="p">[</span><span class="mf">0.81</span><span class="p">,</span> <span class="mf">0.74</span><span class="p">,</span> <span class="mf">0.70</span><span class="p">],</span>
<span class="s1">&#39;std_train_score&#39;</span>    <span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.19</span><span class="p">,</span> <span class="mf">0.00</span><span class="p">],</span>
<span class="s1">&#39;mean_fit_time&#39;</span>      <span class="p">:</span> <span class="p">[</span><span class="mf">0.73</span><span class="p">,</span> <span class="mf">0.63</span><span class="p">,</span> <span class="mf">0.43</span><span class="p">],</span>
<span class="s1">&#39;std_fit_time&#39;</span>       <span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">],</span>
<span class="s1">&#39;mean_score_time&#39;</span>    <span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.06</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">],</span>
<span class="s1">&#39;std_score_time&#39;</span>     <span class="p">:</span> <span class="p">[</span><span class="mf">0.00</span><span class="p">,</span> <span class="mf">0.00</span><span class="p">,</span> <span class="mf">0.00</span><span class="p">],</span>
<span class="s1">&#39;params&#39;</span>             <span class="p">:</span> <span class="p">[{</span><span class="s1">&#39;kernel&#39;</span> <span class="p">:</span> <span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="s1">&#39;gamma&#39;</span> <span class="p">:</span> <span class="mf">0.1</span><span class="p">},</span> <span class="o">...</span><span class="p">],</span>
<span class="p">}</span>
</pre></div>
</div>
<p>NOTE</p>
<p>The key <code class="docutils literal"><span class="pre">'params'</span></code> is used to store a list of parameter
settings dicts for all the parameter candidates.</p>
<p>The <code class="docutils literal"><span class="pre">mean_fit_time</span></code>, <code class="docutils literal"><span class="pre">std_fit_time</span></code>, <code class="docutils literal"><span class="pre">mean_score_time</span></code> and
<code class="docutils literal"><span class="pre">std_score_time</span></code> are all in seconds.</p>
<p class="last">For multi-metric evaluation, the scores for all the scorers are
available in the <code class="docutils literal"><span class="pre">cv_results_</span></code> dict at the keys ending with that
scorer’s name (<code class="docutils literal"><span class="pre">'_&lt;scorer_name&gt;'</span></code>) instead of <code class="docutils literal"><span class="pre">'_score'</span></code> shown
above. (‘split0_test_precision’, ‘mean_train_precision’ etc.)</p>
</dd>
<dt><code class="docutils literal"><span class="pre">best_estimator_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">estimator or dict</span></dt>
<dd><p class="first">Estimator that was chosen by the search, i.e. estimator
which gave highest score (or smallest loss if specified)
on the left out data. Not available if <code class="docutils literal"><span class="pre">refit=False</span></code>.</p>
<p>For multi-metric evaluation, this attribute is present only if
<code class="docutils literal"><span class="pre">refit</span></code> is specified.</p>
<p class="last">See <code class="docutils literal"><span class="pre">refit</span></code> parameter for more information on allowed values.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">best_score_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd><p class="first">Mean cross-validated score of the best_estimator.</p>
<p class="last">For multi-metric evaluation, this is not available if <code class="docutils literal"><span class="pre">refit</span></code> is
<code class="docutils literal"><span class="pre">False</span></code>. See <code class="docutils literal"><span class="pre">refit</span></code> parameter for more information.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">best_params_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd><p class="first">Parameter setting that gave the best results on the hold out data.</p>
<p class="last">For multi-metric evaluation, this is not available if <code class="docutils literal"><span class="pre">refit</span></code> is
<code class="docutils literal"><span class="pre">False</span></code>. See <code class="docutils literal"><span class="pre">refit</span></code> parameter for more information.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">best_index_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd><p class="first">The index (of the <code class="docutils literal"><span class="pre">cv_results_</span></code> arrays) which corresponds to the best
candidate parameter setting.</p>
<p>The dict at <code class="docutils literal"><span class="pre">search.cv_results_['params'][search.best_index_]</span></code> gives
the parameter setting for the best model, that gives the highest
mean score (<code class="docutils literal"><span class="pre">search.best_score_</span></code>).</p>
<p class="last">For multi-metric evaluation, this is not available if <code class="docutils literal"><span class="pre">refit</span></code> is
<code class="docutils literal"><span class="pre">False</span></code>. See <code class="docutils literal"><span class="pre">refit</span></code> parameter for more information.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">scorer_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">function or a dict</span></dt>
<dd><p class="first">Scorer function used on the held out data to choose the best
parameters for the model.</p>
<p class="last">For multi-metric evaluation, this attribute holds the validated
<code class="docutils literal"><span class="pre">scoring</span></code> dict which maps the scorer key to the scorer callable.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">n_splits_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The number of cross-validation splits (folds/iterations).</dd>
<dt><code class="docutils literal"><span class="pre">refit_time_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd><p class="first">Seconds used for refitting the best model on the whole dataset.</p>
<p class="last">This is present only if <code class="docutils literal"><span class="pre">refit</span></code> is not False.</p>
</dd>
</dl>
<p><strong>Notes</strong></p>
<p>The parameters selected are those that maximize the score of the held-out
data, according to the scoring parameter.</p>
<p>If <cite>n_jobs</cite> was set to a value higher than one, the data is copied for each
parameter setting(and not <cite>n_jobs</cite> times). This is done for efficiency
reasons if individual jobs take very little time, but may raise errors if
the dataset is large and not enough memory is available.  A workaround in
this case is to set <cite>pre_dispatch</cite>. Then, the memory is copied only
<cite>pre_dispatch</cite> many times. A reasonable value for <cite>pre_dispatch</cite> is <cite>2 *
n_jobs</cite>.</p>
<p>See Also</p>
<p><code class="xref py py-class docutils literal"><span class="pre">GridSearchCV</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li>Does exhaustive search over a grid of parameters.</li>
</ul>
</div></blockquote>
<p><code class="xref py py-class docutils literal"><span class="pre">ParameterSampler</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li>A generator over parameter settings, constructed from</li>
<li>param_distributions.</li>
</ul>
</div></blockquote>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#RandomizedSearchCVScikitsLearnNode">RandomizedSearchCVScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.SGDClassifierScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">SGDClassifierScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.SGDClassifierScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Linear classifiers (SVM, logistic regression, a.o.) with SGD training.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.stochastic_gradient.SGDClassifier</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
This estimator implements regularized linear models with stochastic
gradient descent (SGD) learning: the gradient of the loss is estimated
each sample at a time and the model is updated along the way with a
decreasing strength schedule (aka learning rate). SGD allows minibatch
(online/out-of-core) learning, see the partial_fit method.
For best results using the default learning rate schedule, the data should
have zero mean and unit variance.</p>
<p>This implementation works with data represented as dense or sparse arrays
of floating point values for the features. The model it fits can be
controlled with the loss parameter; by default, it fits a linear support
vector machine (SVM).</p>
<p>The regularizer is a penalty added to the loss function that shrinks model
parameters towards the zero vector using either the squared euclidean norm
L2 or the absolute norm L1 or a combination of both (Elastic Net). If the
parameter update crosses the 0.0 value because of the regularizer, the
update is truncated to 0.0 to allow for learning sparse models and achieve
online feature selection.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>loss <span class="classifier-delimiter">:</span> <span class="classifier">str, default: ‘hinge’</span></dt>
<dd><p class="first">The loss function to be used. Defaults to ‘hinge’, which gives a
linear SVM.</p>
<p>The possible options are ‘hinge’, ‘log’, ‘modified_huber’,
‘squared_hinge’, ‘perceptron’, or a regression loss: ‘squared_loss’,
‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’.</p>
<p class="last">The ‘log’ loss gives logistic regression, a probabilistic classifier.
‘modified_huber’ is another smooth loss that brings tolerance to
outliers as well as probability estimates.
‘squared_hinge’ is like hinge but is quadratically penalized.
‘perceptron’ is the linear loss used by the perceptron algorithm.
The other losses are designed for regression but can be useful in
classification as well; see SGDRegressor for a description.</p>
</dd>
<dt>penalty <span class="classifier-delimiter">:</span> <span class="classifier">str, ‘none’, ‘l2’, ‘l1’, or ‘elasticnet’</span></dt>
<dd>The penalty (aka regularization term) to be used. Defaults to ‘l2’
which is the standard regularizer for linear SVM models. ‘l1’ and
‘elasticnet’ might bring sparsity to the model (feature selection)
not achievable with ‘l2’.</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Constant that multiplies the regularization term. Defaults to 0.0001
Also used to compute learning_rate when set to ‘optimal’.</dd>
<dt>l1_ratio <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The Elastic Net mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.
l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.
Defaults to 0.15.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Whether the intercept should be estimated or not. If False, the
data is assumed to be already centered. Defaults to True.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd><p class="first">The maximum number of passes over the training data (aka epochs).
It only impacts the behavior in the <code class="docutils literal"><span class="pre">fit</span></code> method, and not the
<cite>partial_fit</cite>.
Defaults to 5. Defaults to 1000 from 0.21, or if tol is not None.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19.</span></p>
</div>
</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float or None, optional</span></dt>
<dd><p class="first">The stopping criterion. If it is not None, the iterations will stop
when (loss &gt; previous_loss - tol). Defaults to None.
Defaults to 1e-3 from 0.21.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19.</span></p>
</div>
</dd>
<dt>shuffle <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd>Whether or not the training data should be shuffled after each epoch.
Defaults to True.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span></dt>
<dd>The verbosity level</dd>
<dt>epsilon <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Epsilon in the epsilon-insensitive loss functions; only if <cite>loss</cite> is
‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’.
For ‘huber’, determines the threshold at which it becomes less
important to get the prediction exactly right.
For epsilon-insensitive, any differences between the current prediction
and the correct label are ignored if they are less than this threshold.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The number of CPUs to use to do the OVA (One Versus All, for
multi-class problems) computation.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>The seed of the pseudo random number generator to use when shuffling
the data.  If int, random_state is the seed used by the random number
generator; If RandomState instance, random_state is the random number
generator; If None, the random number generator is the RandomState
instance used by <cite>np.random</cite>.</dd>
<dt>learning_rate <span class="classifier-delimiter">:</span> <span class="classifier">string, optional</span></dt>
<dd><p class="first">The learning rate schedule:</p>
<p>‘constant’:</p>
<blockquote>
<div><ul class="simple">
<li>eta = eta0</li>
</ul>
</div></blockquote>
<dl class="docutils">
<dt>‘optimal’: [default]</dt>
<dd>eta = 1.0 / (alpha * (t + t0))
where t0 is chosen by a heuristic proposed by Leon Bottou.</dd>
</dl>
<p>‘invscaling’:</p>
<blockquote>
<div><ul class="simple">
<li>eta = eta0 / pow(t, power_t)</li>
</ul>
</div></blockquote>
<p>‘adaptive’:</p>
<blockquote class="last">
<div><ul class="simple">
<li>eta = eta0, as long as the training keeps decreasing.</li>
<li>Each time n_iter_no_change consecutive epochs fail to decrease the</li>
<li>training loss by tol or fail to increase validation score by tol if</li>
<li>early_stopping is True, the current learning rate is divided by 5.</li>
</ul>
</div></blockquote>
</dd>
<dt>eta0 <span class="classifier-delimiter">:</span> <span class="classifier">double</span></dt>
<dd>The initial learning rate for the ‘constant’, ‘invscaling’ or
‘adaptive’ schedules. The default value is 0.0 as eta0 is not used by
the default schedule ‘optimal’.</dd>
<dt>power_t <span class="classifier-delimiter">:</span> <span class="classifier">double</span></dt>
<dd>The exponent for inverse scaling learning rate [default 0.5].</dd>
<dt>early_stopping <span class="classifier-delimiter">:</span> <span class="classifier">bool, default=False</span></dt>
<dd><p class="first">Whether to use early stopping to terminate training when validation
score is not improving. If set to True, it will automatically set aside
a fraction of training data as validation and terminate training when
validation score is not improving by at least tol for
n_iter_no_change consecutive epochs.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
<dt>validation_fraction <span class="classifier-delimiter">:</span> <span class="classifier">float, default=0.1</span></dt>
<dd><p class="first">The proportion of training data to set aside as validation set for
early stopping. Must be between 0 and 1.
Only used if early_stopping is True.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
<dt>n_iter_no_change <span class="classifier-delimiter">:</span> <span class="classifier">int, default=5</span></dt>
<dd><p class="first">Number of iterations with no improvement to wait before early stopping.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
<dt>class_weight <span class="classifier-delimiter">:</span> <span class="classifier">dict, {class_label: weight} or “balanced” or None, optional</span></dt>
<dd><p class="first">Preset for the class_weight fit parameter.</p>
<p>Weights associated with classes. If not given, all classes
are supposed to have weight one.</p>
<p class="last">The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd><p class="first">When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.
See <span class="xref std std-term">the Glossary</span>.</p>
<p class="last">Repeatedly calling fit or partial_fit when warm_start is True can
result in a different solution than when calling fit a single time
because of the way the data is shuffled.
If a dynamic learning rate is used, the learning rate is adapted
depending on the number of samples already seen. Calling <code class="docutils literal"><span class="pre">fit</span></code> resets
this counter, while <code class="docutils literal"><span class="pre">partial_fit</span></code> will result in increasing the
existing counter.</p>
</dd>
<dt>average <span class="classifier-delimiter">:</span> <span class="classifier">bool or int, optional</span></dt>
<dd>When set to True, computes the averaged SGD weights and stores the
result in the <code class="docutils literal"><span class="pre">coef_</span></code> attribute. If set to an int greater than 1,
averaging will begin once the total number of samples seen reaches
average. So <code class="docutils literal"><span class="pre">average=10</span></code> will begin averaging after seeing 10
samples.</dd>
<dt>n_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd><p class="first">The number of passes over the training data (aka epochs).
Defaults to None. Deprecated, will be removed in 0.21.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.19: </span>Deprecated</p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (1, n_features) if n_classes == 2 else (n_classes,            n_features)</span></dt>
<dd>Weights assigned to the features.</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (1,) if n_classes == 2 else (n_classes,)</span></dt>
<dd>Constants in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The actual number of iterations to reach the stopping criterion.
For multiclass fits, it is the maximum over every binary fit.</dd>
</dl>
<p><code class="docutils literal"><span class="pre">loss_function_</span></code> : concrete <code class="docutils literal"><span class="pre">LossFunction</span></code></p>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">SGDClassifier</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">SGDClassifier(alpha=0.0001, average=False, class_weight=None,</span>
<span class="go">       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,</span>
<span class="go">       l1_ratio=0.15, learning_rate=&#39;optimal&#39;, loss=&#39;hinge&#39;, max_iter=1000,</span>
<span class="go">       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty=&#39;l2&#39;,</span>
<span class="go">       power_t=0.5, random_state=None, shuffle=True, tol=0.001,</span>
<span class="go">       validation_fraction=0.1, verbose=0, warm_start=False)</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<p>See also</p>
<p>sklearn.svm.LinearSVC, LogisticRegression, Perceptron</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.SGDClassifierScikitsLearnNode-class.html">SGDClassifierScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.MeanShiftScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">MeanShiftScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.MeanShiftScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean shift clustering using a flat kernel.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.cluster.mean_shift_.MeanShift</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Mean shift clustering aims to discover “blobs” in a smooth density of
samples. It is a centroid-based algorithm, which works by updating
candidates for centroids to be the mean of the points within a given
region. These candidates are then filtered in a post-processing stage to
eliminate near-duplicates to form the final set of centroids.</p>
<p>Seeding is performed using a binning technique for scalability.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>bandwidth <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd><p class="first">Bandwidth used in the RBF kernel.</p>
<p class="last">If not given, the bandwidth is estimated using
sklearn.cluster.estimate_bandwidth; see the documentation for that
function for hints on scalability (see also the Notes, below).</p>
</dd>
<dt>seeds <span class="classifier-delimiter">:</span> <span class="classifier">array, shape=[n_samples, n_features], optional</span></dt>
<dd>Seeds used to initialize kernels. If not set,
the seeds are calculated by clustering.get_bin_seeds
with bandwidth as the grid size and default values for
other parameters.</dd>
<dt>bin_seeding <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>If true, initial kernel locations are not locations of all
points, but rather the location of the discretized version of
points, where points are binned onto a grid whose coarseness
corresponds to the bandwidth. Setting this option to True will speed
up the algorithm because fewer seeds will be initialized.
default value: False
Ignored if seeds argument is not None.</dd>
<dt>min_bin_freq <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>To speed up the algorithm, accept only those bins with at least
min_bin_freq points as seeds. If not defined, set to 1.</dd>
<dt>cluster_all <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default True</span></dt>
<dd>If true, then all points are clustered, even those orphans that are
not within any kernel. Orphans are assigned to the nearest kernel.
If false, then orphans are given cluster label -1.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd><p class="first">The number of jobs to use for the computation. This works by computing
each of the n_init runs in parallel.</p>
<p class="last"><code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">cluster_centers_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_clusters, n_features]</span></dt>
<dd>Coordinates of cluster centers.</dd>
</dl>
<p><code class="docutils literal"><span class="pre">labels_</span></code> :</p>
<blockquote>
<div><ul class="simple">
<li>Labels of each point.</li>
</ul>
</div></blockquote>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">MeanShift</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>              <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clustering</span> <span class="o">=</span> <span class="n">MeanShift</span><span class="p">(</span><span class="n">bandwidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clustering</span><span class="o">.</span><span class="n">labels_</span>
<span class="go">array([1, 1, 1, 0, 0, 0])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clustering</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">]])</span>
<span class="go">array([1, 0])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clustering</span> 
<span class="go">MeanShift(bandwidth=2, bin_seeding=False, cluster_all=True, min_bin_freq=1,</span>
<span class="go">     n_jobs=None, seeds=None)</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>Scalability:</p>
<p>Because this implementation uses a flat kernel and
a Ball Tree to look up members of each kernel, the complexity will tend
towards O(T*n*log(n)) in lower dimensions, with n the number of samples
and T the number of points. In higher dimensions the complexity will
tend towards O(T*n^2).</p>
<p>Scalability can be boosted by using fewer seeds, for example by using
a higher value of min_bin_freq in the get_bin_seeds function.</p>
<p>Note that the estimate_bandwidth function is much less scalable than the
mean shift algorithm and will be the bottleneck if it is used.</p>
<p><strong>References</strong></p>
<p>Dorin Comaniciu and Peter Meer, “Mean Shift: A robust approach toward
feature space analysis”. IEEE Transactions on Pattern Analysis and
Machine Intelligence. 2002. pp. 603-619.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#MeanShiftScikitsLearnNode">MeanShiftScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.PassiveAggressiveClassifierScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">PassiveAggressiveClassifierScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.PassiveAggressiveClassifierScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Passive Aggressive Classifier
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>C <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Maximum step size (regularization). Defaults to 1.0.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">bool, default=False</span></dt>
<dd>Whether the intercept should be estimated or not. If False, the
data is assumed to be already centered.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd><p class="first">The maximum number of passes over the training data (aka epochs).
It only impacts the behavior in the <code class="docutils literal"><span class="pre">fit</span></code> method, and not the
<cite>partial_fit</cite>.
Defaults to 5. Defaults to 1000 from 0.21, or if tol is not None.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19.</span></p>
</div>
</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float or None, optional</span></dt>
<dd><p class="first">The stopping criterion. If it is not None, the iterations will stop
when (loss &gt; previous_loss - tol). Defaults to None.
Defaults to 1e-3 from 0.21.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19.</span></p>
</div>
</dd>
<dt>early_stopping <span class="classifier-delimiter">:</span> <span class="classifier">bool, default=False</span></dt>
<dd><p class="first">Whether to use early stopping to terminate training when validation.
score is not improving. If set to True, it will automatically set aside
a fraction of training data as validation and terminate training when
validation score is not improving by at least tol for
n_iter_no_change consecutive epochs.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
<dt>validation_fraction <span class="classifier-delimiter">:</span> <span class="classifier">float, default=0.1</span></dt>
<dd><p class="first">The proportion of training data to set aside as validation set for
early stopping. Must be between 0 and 1.
Only used if early_stopping is True.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
<dt>n_iter_no_change <span class="classifier-delimiter">:</span> <span class="classifier">int, default=5</span></dt>
<dd><p class="first">Number of iterations with no improvement to wait before early stopping.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
<dt>shuffle <span class="classifier-delimiter">:</span> <span class="classifier">bool, default=True</span></dt>
<dd>Whether or not the training data should be shuffled after each epoch.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span></dt>
<dd>The verbosity level</dd>
<dt>loss <span class="classifier-delimiter">:</span> <span class="classifier">string, optional</span></dt>
<dd><p class="first">The loss function to be used:</p>
<ul class="last simple">
<li>hinge: equivalent to PA-I in the reference paper.</li>
<li>squared_hinge: equivalent to PA-II in the reference paper.</li>
</ul>
</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The number of CPUs to use to do the OVA (One Versus All, for
multi-class problems) computation.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional, default=None</span></dt>
<dd>The seed of the pseudo random number generator to use when shuffling
the data.  If int, random_state is the seed used by the random number
generator; If RandomState instance, random_state is the random number
generator; If None, the random number generator is the RandomState
instance used by <cite>np.random</cite>.</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd><p class="first">When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.
See <span class="xref std std-term">the Glossary</span>.</p>
<p class="last">Repeatedly calling fit or partial_fit when warm_start is True can
result in a different solution than when calling fit a single time
because of the way the data is shuffled.</p>
</dd>
<dt>class_weight <span class="classifier-delimiter">:</span> <span class="classifier">dict, {class_label: weight} or “balanced” or None, optional</span></dt>
<dd><p class="first">Preset for the class_weight fit parameter.</p>
<p>Weights associated with classes. If not given, all classes
are supposed to have weight one.</p>
<p>The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span>parameter <em>class_weight</em> to automatically weight samples.</p>
</div>
</dd>
<dt>average <span class="classifier-delimiter">:</span> <span class="classifier">bool or int, optional</span></dt>
<dd><p class="first">When set to True, computes the averaged SGD weights and stores the
result in the <code class="docutils literal"><span class="pre">coef_</span></code> attribute. If set to an int greater than 1,
averaging will begin once the total number of samples seen reaches
average. So average=10 will begin averaging after seeing 10 samples.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19: </span>parameter <em>average</em> to use weights averaging in SGD</p>
</div>
</dd>
<dt>n_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd><p class="first">The number of passes over the training data (aka epochs).
Defaults to None. Deprecated, will be removed in 0.21.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.19: </span>Deprecated</p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]</span></dt>
<dd>Weights assigned to the features.</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1] if n_classes == 2 else [n_classes]</span></dt>
<dd>Constants in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The actual number of iterations to reach the stopping criterion.
For multiclass fits, it is the maximum over every binary fit.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">PassiveAggressiveClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">PassiveAggressiveClassifier</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span><span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,</span>
<span class="go">              early_stopping=False, fit_intercept=True, loss=&#39;hinge&#39;,</span>
<span class="go">              max_iter=1000, n_iter=None, n_iter_no_change=5, n_jobs=None,</span>
<span class="go">              random_state=0, shuffle=True, tol=0.001,</span>
<span class="go">              validation_fraction=0.1, verbose=0, warm_start=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">[[-0.6543424   1.54603022  1.35361642  0.22199435]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="go">[0.63310933]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<p>See also</p>
<p>SGDClassifier
Perceptron</p>
<p><strong>References</strong></p>
<p>Online Passive-Aggressive Algorithms
&lt;<a class="reference external" href="http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf">http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf</a>&gt;
K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#PassiveAggressiveClassifierScikitsLearnNode">PassiveAggressiveClassifierScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.KernelPCAScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">KernelPCAScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.KernelPCAScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Kernel Principal component analysis (KPCA)
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.decomposition.kernel_pca.KernelPCA</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Non-linear dimensionality reduction through the use of kernels (see
<span class="xref std std-ref">metrics</span>).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int, default=None</span></dt>
<dd>Number of components. If None, all non-zero components are kept.</dd>
<dt>kernel <span class="classifier-delimiter">:</span> <span class="classifier">“linear” | “poly” | “rbf” | “sigmoid” | “cosine” | “precomputed”</span></dt>
<dd>Kernel. Default=”linear”.</dd>
<dt>gamma <span class="classifier-delimiter">:</span> <span class="classifier">float, default=1/n_features</span></dt>
<dd>Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other
kernels.</dd>
<dt>degree <span class="classifier-delimiter">:</span> <span class="classifier">int, default=3</span></dt>
<dd>Degree for poly kernels. Ignored by other kernels.</dd>
<dt>coef0 <span class="classifier-delimiter">:</span> <span class="classifier">float, default=1</span></dt>
<dd>Independent term in poly and sigmoid kernels.
Ignored by other kernels.</dd>
<dt>kernel_params <span class="classifier-delimiter">:</span> <span class="classifier">mapping of string to any, default=None</span></dt>
<dd>Parameters (keyword arguments) and values for kernel passed as
callable object. Ignored by other kernels.</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">int, default=1.0</span></dt>
<dd>Hyperparameter of the ridge regression that learns the
inverse transform (when fit_inverse_transform=True).</dd>
<dt>fit_inverse_transform <span class="classifier-delimiter">:</span> <span class="classifier">bool, default=False</span></dt>
<dd>Learn the inverse transform for non-precomputed kernels.
(i.e. learn to find the pre-image of a point)</dd>
<dt>eigen_solver <span class="classifier-delimiter">:</span> <span class="classifier">string [‘auto’|’dense’|’arpack’], default=’auto’</span></dt>
<dd>Select eigensolver to use. If n_components is much less than
the number of training samples, arpack may be more efficient
than the dense eigensolver.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, default=0</span></dt>
<dd>Convergence tolerance for arpack.
If 0, optimal value will be chosen by arpack.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, default=None</span></dt>
<dd>Maximum number of iterations for arpack.
If None, optimal value will be chosen by arpack.</dd>
<dt>remove_zero_eig <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=False</span></dt>
<dd>If True, then all components with zero eigenvalues are removed, so
that the number of components in the output may be &lt; n_components
(and sometimes even zero due to numerical instability).
When n_components is None, this parameter is ignored and components
with zero eigenvalues are removed regardless.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd><p class="first">If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>. Used when <code class="docutils literal"><span class="pre">eigen_solver</span></code> == ‘arpack’.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.18.</span></p>
</div>
</dd>
<dt>copy_X <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=True</span></dt>
<dd><p class="first">If True, input X is copied and stored by the model in the <cite>X_fit_</cite>
attribute. If no further changes will be done to X, setting
<cite>copy_X=False</cite> saves memory by storing a reference.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.18.</span></p>
</div>
</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd><p class="first">The number of parallel jobs to run.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.18.</span></p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">lambdas_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, (n_components,)</span></dt>
<dd>Eigenvalues of the centered kernel matrix in decreasing order.
If <cite>n_components</cite> and <cite>remove_zero_eig</cite> are not set,
then all values are stored.</dd>
<dt><code class="docutils literal"><span class="pre">alphas_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, (n_samples, n_components)</span></dt>
<dd>Eigenvectors of the centered kernel matrix. If <cite>n_components</cite> and
<cite>remove_zero_eig</cite> are not set, then all components are stored.</dd>
<dt><code class="docutils literal"><span class="pre">dual_coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, (n_samples, n_features)</span></dt>
<dd>Inverse transform matrix. Only available when
<code class="docutils literal"><span class="pre">fit_inverse_transform</span></code> is True.</dd>
<dt><code class="docutils literal"><span class="pre">X_transformed_fit_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, (n_samples, n_components)</span></dt>
<dd>Projection of the fitted data on the kernel principal components.
Only available when <code class="docutils literal"><span class="pre">fit_inverse_transform</span></code> is True.</dd>
<dt><code class="docutils literal"><span class="pre">X_fit_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">(n_samples, n_features)</span></dt>
<dd>The data used to fit the model. If <cite>copy_X=False</cite>, then <cite>X_fit_</cite> is
a reference. This attribute is used for the calls to transform.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">KernelPCA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span> <span class="o">=</span> <span class="n">KernelPCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_transformed</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_transformed</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(1797, 7)</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>Kernel PCA was introduced in:</p>
<blockquote>
<div><ul class="simple">
<li>Bernhard Schoelkopf, Alexander J. Smola,</li>
<li>and Klaus-Robert Mueller. 1999. Kernel principal</li>
<li>component analysis. In Advances in kernel methods,</li>
<li>MIT Press, Cambridge, MA, USA 327-352.</li>
</ul>
</div></blockquote>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.KernelPCAScikitsLearnNode-class.html">KernelPCAScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.AffinityPropagationScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">AffinityPropagationScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.AffinityPropagationScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform Affinity Propagation Clustering of data.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.cluster.affinity_propagation_.AffinityPropagation</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>damping <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default: 0.5</span></dt>
<dd>Damping factor (between 0.5 and 1) is the extent to
which the current value is maintained relative to
incoming values (weighted 1 - damping). This in order
to avoid numerical oscillations when updating these
values (messages).</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default: 200</span></dt>
<dd>Maximum number of iterations.</dd>
<dt>convergence_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default: 15</span></dt>
<dd>Number of iterations with no change in the number
of estimated clusters that stops the convergence.</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default: True</span></dt>
<dd>Make a copy of input data.</dd>
<dt>preference <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_samples,) or float, optional</span></dt>
<dd>Preferences for each point - points with larger values of
preferences are more likely to be chosen as exemplars. The number
of exemplars, ie of clusters, is influenced by the input
preferences value. If the preferences are not passed as arguments,
they will be set to the median of the input similarities.</dd>
<dt>affinity <span class="classifier-delimiter">:</span> <span class="classifier">string, optional, default=``euclidean``</span></dt>
<dd>Which affinity to use. At the moment <code class="docutils literal"><span class="pre">precomputed</span></code> and
<code class="docutils literal"><span class="pre">euclidean</span></code> are supported. <code class="docutils literal"><span class="pre">euclidean</span></code> uses the
negative squared euclidean distance between points.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default: False</span></dt>
<dd>Whether to be verbose.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">cluster_centers_indices_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_clusters,)</span></dt>
<dd>Indices of cluster centers</dd>
<dt><code class="docutils literal"><span class="pre">cluster_centers_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_clusters, n_features)</span></dt>
<dd>Cluster centers (if affinity != <code class="docutils literal"><span class="pre">precomputed</span></code>).</dd>
<dt><code class="docutils literal"><span class="pre">labels_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_samples,)</span></dt>
<dd>Labels of each point</dd>
<dt><code class="docutils literal"><span class="pre">affinity_matrix_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_samples, n_samples)</span></dt>
<dd>Stores the affinity matrix used in <code class="docutils literal"><span class="pre">fit</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of iterations taken to converge.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">AffinityPropagation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>              <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clustering</span> <span class="o">=</span> <span class="n">AffinityPropagation</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clustering</span> 
<span class="go">AffinityPropagation(affinity=&#39;euclidean&#39;, convergence_iter=15, copy=True,</span>
<span class="go">          damping=0.5, max_iter=200, preference=None, verbose=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clustering</span><span class="o">.</span><span class="n">labels_</span>
<span class="go">array([0, 0, 0, 1, 1, 1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clustering</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="go">array([0, 1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clustering</span><span class="o">.</span><span class="n">cluster_centers_</span>
<span class="go">array([[1, 2],</span>
<span class="go">       [4, 2]])</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>For an example, see <span class="xref std std-ref">examples/cluster/plot_affinity_propagation.py</span>.</p>
<p>The algorithmic complexity of affinity propagation is quadratic
in the number of points.</p>
<p>When <code class="docutils literal"><span class="pre">fit</span></code> does not converge, <code class="docutils literal"><span class="pre">cluster_centers_</span></code> becomes an empty
array and all training samples will be labelled as <code class="docutils literal"><span class="pre">-1</span></code>. In addition,
<code class="docutils literal"><span class="pre">predict</span></code> will then label every sample as <code class="docutils literal"><span class="pre">-1</span></code>.</p>
<p>When all training samples have equal similarities and equal preferences,
the assignment of cluster centers and labels depends on the preference.
If the preference is smaller than the similarities, <code class="docutils literal"><span class="pre">fit</span></code> will result in
a single cluster center and label <code class="docutils literal"><span class="pre">0</span></code> for every sample. Otherwise, every
training sample becomes its own cluster center and is assigned a unique
label.</p>
<p><strong>References</strong></p>
<p>Brendan J. Frey and Delbert Dueck, “Clustering by Passing Messages
Between Data Points”, Science Feb. 2007</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#AffinityPropagationScikitsLearnNode">AffinityPropagationScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.OneVsOneClassifierScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">OneVsOneClassifierScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.OneVsOneClassifierScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>One-vs-one multiclass strategy
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.multiclass.OneVsOneClassifier</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
This strategy consists in fitting one classifier per class pair.
At prediction time, the class which received the most votes is selected.
Since it requires to fit <cite>n_classes * (n_classes - 1) / 2</cite> classifiers,
this method is usually slower than one-vs-the-rest, due to its
O(n_classes^2) complexity. However, this method may be advantageous for
algorithms such as kernel algorithms which don’t scale well with
<cite>n_samples</cite>. This is because each individual learning problem only involves
a small subset of the data whereas, with one-vs-the-rest, the complete
dataset is used <cite>n_classes</cite> times.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>estimator <span class="classifier-delimiter">:</span> <span class="classifier">estimator object</span></dt>
<dd>An estimator object implementing <cite>fit</cite> and one of <cite>decision_function</cite>
or <cite>predict_proba</cite>.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The number of jobs to use for the computation.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">estimators_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list of <cite>n_classes * (n_classes - 1) / 2</cite> estimators</span></dt>
<dd>Estimators used for predictions.</dd>
<dt><code class="docutils literal"><span class="pre">classes_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">numpy array of shape [n_classes]</span></dt>
<dd>Array containing labels.</dd>
</dl>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#OneVsOneClassifierScikitsLearnNode">OneVsOneClassifierScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.CCAScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">CCAScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.CCAScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>CCA Canonical Correlation Analysis.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.cross_decomposition.cca_.CCA</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
CCA inherits from PLS with mode=”B” and deflation_mode=”canonical”.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int, (default 2).</span></dt>
<dd>number of components to keep.</dd>
<dt>scale <span class="classifier-delimiter">:</span> <span class="classifier">boolean, (default True)</span></dt>
<dd>whether to scale the data?</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">an integer, (default 500)</span></dt>
<dd>the maximum number of iterations of the NIPALS inner loop</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">non-negative real, default 1e-06.</span></dt>
<dd>the tolerance used in the iterative algorithm</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>Whether the deflation be done on a copy. Let the default value
to True unless you don’t care about side effects</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">x_weights_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [p, n_components]</span></dt>
<dd>X block weights vectors.</dd>
<dt><code class="docutils literal"><span class="pre">y_weights_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [q, n_components]</span></dt>
<dd>Y block weights vectors.</dd>
<dt><code class="docutils literal"><span class="pre">x_loadings_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [p, n_components]</span></dt>
<dd>X block loadings vectors.</dd>
<dt><code class="docutils literal"><span class="pre">y_loadings_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [q, n_components]</span></dt>
<dd>Y block loadings vectors.</dd>
<dt><code class="docutils literal"><span class="pre">x_scores_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_samples, n_components]</span></dt>
<dd>X scores.</dd>
<dt><code class="docutils literal"><span class="pre">y_scores_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_samples, n_components]</span></dt>
<dd>Y scores.</dd>
<dt><code class="docutils literal"><span class="pre">x_rotations_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [p, n_components]</span></dt>
<dd>X block to latents rotations.</dd>
<dt><code class="docutils literal"><span class="pre">y_rotations_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [q, n_components]</span></dt>
<dd>Y block to latents rotations.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like</span></dt>
<dd>Number of iterations of the NIPALS inner loop for each
component.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>For each component k, find the weights u, v that maximizes
max corr(Xk u, Yk v), such that <code class="docutils literal"><span class="pre">|u|</span> <span class="pre">=</span> <span class="pre">|v|</span> <span class="pre">=</span> <span class="pre">1</span></code></p>
<p>Note that it maximizes only the correlations between the scores.</p>
<p>The residual matrix of X (Xk+1) block is obtained by the deflation on the
current X score: x_score.</p>
<p>The residual matrix of Y (Yk+1) block is obtained by deflation on the
current Y score.</p>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cross_decomposition</span> <span class="kn">import</span> <span class="n">CCA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.</span><span class="p">,</span><span class="mf">5.</span><span class="p">,</span><span class="mf">4.</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.2</span><span class="p">,</span> <span class="mf">5.9</span><span class="p">],</span> <span class="p">[</span><span class="mf">11.9</span><span class="p">,</span> <span class="mf">12.3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cca</span> <span class="o">=</span> <span class="n">CCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">CCA(copy=True, max_iter=500, n_components=1, scale=True, tol=1e-06)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_c</span><span class="p">,</span> <span class="n">Y_c</span> <span class="o">=</span> <span class="n">cca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with
emphasis on the two-block case. Technical Report 371, Department of
Statistics, University of Washington, Seattle, 2000.</p>
<p>In french but still a reference:</p>
<p>Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris:</p>
<p>Editions Technic.</p>
<p>See also</p>
<p>PLSCanonical
PLSSVD</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.CCAScikitsLearnNode-class.html">CCAScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.KernelCentererScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">KernelCentererScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.KernelCentererScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Center a kernel matrix
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.preprocessing.data.KernelCenterer</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Let K(x, z) be a kernel defined by phi(x)^T phi(z), where phi is a
function mapping x to a Hilbert space. KernelCenterer centers (i.e.,
normalize to have zero mean) the data without explicitly computing phi(x).
It is equivalent to centering phi(x) with
sklearn.preprocessing.StandardScaler(with_std=False).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">KernelCenterer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">pairwise_kernels</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span> <span class="o">-</span><span class="mf">2.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span> <span class="mf">4.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">K</span> <span class="o">=</span> <span class="n">pairwise_kernels</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">K</span>
<span class="go">array([[  9.,   2.,  -2.],</span>
<span class="go">       [  2.,  14., -13.],</span>
<span class="go">       [ -2., -13.,  21.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span> <span class="o">=</span> <span class="n">KernelCenterer</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span>
<span class="go">KernelCenterer()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
<span class="go">array([[  5.,   0.,  -5.],</span>
<span class="go">       [  0.,  14., -14.],</span>
<span class="go">       [ -5., -14.,  19.]])</span>
</pre></div>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.KernelCentererScikitsLearnNode-class.html">KernelCentererScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.PolynomialFeaturesScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">PolynomialFeaturesScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.PolynomialFeaturesScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate polynomial and interaction features.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.preprocessing.data.PolynomialFeatures</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Generate a new feature matrix consisting of all polynomial combinations
of the features with degree less than or equal to the specified degree.
For example, if an input sample is two dimensional and of the form
[a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>degree <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd>The degree of the polynomial features. Default = 2.</dd>
<dt>interaction_only <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default = False</span></dt>
<dd>If true, only interaction features are produced: features that are
products of at most <code class="docutils literal"><span class="pre">degree</span></code> <em>distinct</em> input features (so not
<code class="docutils literal"><span class="pre">x[1]</span> <span class="pre">**</span> <span class="pre">2</span></code>, <code class="docutils literal"><span class="pre">x[0]</span> <span class="pre">*</span> <span class="pre">x[2]</span> <span class="pre">**</span> <span class="pre">3</span></code>, etc.).</dd>
<dt>include_bias <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>If True (default), then include a bias column, the feature in which
all polynomial powers are zero (i.e. a column of ones - acts as an
intercept term in a linear model).</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span>
<span class="go">array([[0, 1],</span>
<span class="go">       [2, 3],</span>
<span class="go">       [4, 5]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([[ 1.,  0.,  1.,  0.,  0.,  1.],</span>
<span class="go">       [ 1.,  2.,  3.,  4.,  6.,  9.],</span>
<span class="go">       [ 1.,  4.,  5., 16., 20., 25.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">interaction_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([[ 1.,  0.,  1.,  0.],</span>
<span class="go">       [ 1.,  2.,  3.,  6.],</span>
<span class="go">       [ 1.,  4.,  5., 20.]])</span>
</pre></div>
</div>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">powers_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_output_features, n_input_features)</span></dt>
<dd>powers_[i, j] is the exponent of the jth input in the ith output.</dd>
<dt><code class="docutils literal"><span class="pre">n_input_features_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The total number of input features.</dd>
<dt><code class="docutils literal"><span class="pre">n_output_features_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The total number of polynomial output features. The number of output
features is computed by iterating over all suitably sized combinations
of input features.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>Be aware that the number of features in the output array scales
polynomially in the number of features of the input array, and
exponentially in the degree. High degrees can cause overfitting.</p>
<p>See <span class="xref std std-ref">examples/linear_model/plot_polynomial_interpolation.py</span></p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#PolynomialFeaturesScikitsLearnNode">PolynomialFeaturesScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.AdaBoostRegressorScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">AdaBoostRegressorScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.AdaBoostRegressorScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>An AdaBoost regressor.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.ensemble.weight_boosting.AdaBoostRegressor</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
An AdaBoost [1] regressor is a meta-estimator that begins by fitting a
regressor on the original dataset and then fits additional copies of the
regressor on the same dataset but where the weights of instances are
adjusted according to the error of the current prediction. As such,
subsequent regressors focus more on difficult cases.</p>
<p>This class implements the algorithm known as AdaBoost.R2 [2].</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>base_estimator <span class="classifier-delimiter">:</span> <span class="classifier">object, optional (default=None)</span></dt>
<dd>The base estimator from which the boosted ensemble is built.
Support for sample weighting is required. If <code class="docutils literal"><span class="pre">None</span></code>, then
the base estimator is <code class="docutils literal"><span class="pre">DecisionTreeRegressor(max_depth=3)</span></code></dd>
<dt>n_estimators <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=50)</span></dt>
<dd>The maximum number of estimators at which boosting is terminated.
In case of perfect fit, the learning procedure is stopped early.</dd>
<dt>learning_rate <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.)</span></dt>
<dd>Learning rate shrinks the contribution of each regressor by
<code class="docutils literal"><span class="pre">learning_rate</span></code>. There is a trade-off between <code class="docutils literal"><span class="pre">learning_rate</span></code> and
<code class="docutils literal"><span class="pre">n_estimators</span></code>.</dd>
<dt>loss <span class="classifier-delimiter">:</span> <span class="classifier">{‘linear’, ‘square’, ‘exponential’}, optional (default=’linear’)</span></dt>
<dd>The loss function to use when updating the weights after each
boosting iteration.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">estimators_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list of classifiers</span></dt>
<dd>The collection of fitted sub-estimators.</dd>
<dt><code class="docutils literal"><span class="pre">estimator_weights_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of floats</span></dt>
<dd>Weights for each estimator in the boosted ensemble.</dd>
<dt><code class="docutils literal"><span class="pre">estimator_errors_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of floats</span></dt>
<dd>Regression error for each estimator in the boosted ensemble.</dd>
<dt><code class="docutils literal"><span class="pre">feature_importances_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span></dt>
<dd>The feature importances if supported by the <code class="docutils literal"><span class="pre">base_estimator</span></code>.</dd>
</dl>
<p>See also</p>
<p>AdaBoostClassifier, GradientBoostingRegressor,
sklearn.tree.DecisionTreeRegressor</p>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id48" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Y. Freund, R. Schapire, “A Decision-Theoretic Generalization of
on-Line Learning and an Application to Boosting”, 1995.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id49" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><ol class="first last upperalpha simple" start="8">
<li>Drucker, “Improving Regressors using Boosting Techniques”, 1997.</li>
</ol>
</td></tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#AdaBoostRegressorScikitsLearnNode">AdaBoostRegressorScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.SelectFdrScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">SelectFdrScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.SelectFdrScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Filter: Select the p-values for an estimated false discovery rate
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.feature_selection.univariate_selection.SelectFdr</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
This uses the Benjamini-Hochberg procedure. <code class="docutils literal"><span class="pre">alpha</span></code> is an upper bound
on the expected false discovery rate.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>score_func <span class="classifier-delimiter">:</span> <span class="classifier">callable</span></dt>
<dd>Function taking two arrays X and y, and returning a pair of arrays
(scores, pvalues).
Default is f_classif (see below “See also”). The default function only
works with classification tasks.</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The highest uncorrected p-value for features to keep.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectFdr</span><span class="p">,</span> <span class="n">chi2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(569, 30)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_new</span> <span class="o">=</span> <span class="n">SelectFdr</span><span class="p">(</span><span class="n">chi2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_new</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(569, 16)</span>
</pre></div>
</div>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">scores_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>Scores of features.</dd>
<dt><code class="docutils literal"><span class="pre">pvalues_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>p-values of feature scores.</dd>
</dl>
<p><strong>References</strong></p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/False_discovery_rate">https://en.wikipedia.org/wiki/False_discovery_rate</a></p>
<p>See also</p>
<p>f_classif: ANOVA F-value between label/feature for classification tasks.
mutual_info_classif: Mutual information for a discrete target.
chi2: Chi-squared stats of non-negative features for classification tasks.
f_regression: F-value between label/feature for regression tasks.
mutual_info_regression: Mutual information for a contnuous target.
SelectPercentile: Select features based on percentile of the highest scores.
SelectKBest: Select features based on the k highest scores.
SelectFpr: Select features based on a false positive rate test.
SelectFwe: Select features based on family-wise error rate.
GenericUnivariateSelect: Univariate feature selector with configurable mode.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.SelectFdrScikitsLearnNode-class.html">SelectFdrScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.IsolationForestScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">IsolationForestScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.IsolationForestScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Isolation Forest Algorithm
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.ensemble.iforest.IsolationForest</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Return the anomaly score of each sample using the IsolationForest algorithm</p>
<p>The IsolationForest ‘isolates’ observations by randomly selecting a feature
and then randomly selecting a split value between the maximum and minimum
values of the selected feature.</p>
<p>Since recursive partitioning can be represented by a tree structure, the
number of splittings required to isolate a sample is equivalent to the path
length from the root node to the terminating node.</p>
<p>This path length, averaged over a forest of such random trees, is a
measure of normality and our decision function.</p>
<p>Random partitioning produces noticeably shorter paths for anomalies.
Hence, when a forest of random trees collectively produce shorter path
lengths for particular samples, they are highly likely to be anomalies.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.18.</span></p>
</div>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_estimators <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=100)</span></dt>
<dd>The number of base estimators in the ensemble.</dd>
<dt>max_samples <span class="classifier-delimiter">:</span> <span class="classifier">int or float, optional (default=”auto”)</span></dt>
<dd><dl class="first docutils">
<dt>The number of samples to draw from X to train each base estimator.</dt>
<dd><ul class="first last simple">
<li>If int, then draw <cite>max_samples</cite> samples.</li>
<li>If float, then draw <cite>max_samples * X.shape[0]</cite> samples.</li>
<li>If “auto”, then <cite>max_samples=min(256, n_samples)</cite>.</li>
</ul>
</dd>
</dl>
<p class="last">If max_samples is larger than the number of samples provided,
all samples will be used for all trees (no sampling).</p>
</dd>
<dt>contamination <span class="classifier-delimiter">:</span> <span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt>
<dd><p class="first">The amount of contamination of the data set, i.e. the proportion
of outliers in the data set. Used when fitting to define the threshold
on the decision function. If ‘auto’, the decision function threshold is
determined as in the original paper.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.20: </span>The default value of <code class="docutils literal"><span class="pre">contamination</span></code> will change from 0.1 in 0.20
to <code class="docutils literal"><span class="pre">'auto'</span></code> in 0.22.</p>
</div>
</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int or float, optional (default=1.0)</span></dt>
<dd><p class="first">The number of features to draw from X to train each base estimator.</p>
<blockquote class="last">
<div><ul class="simple">
<li>If int, then draw <cite>max_features</cite> features.</li>
<li>If float, then draw <cite>max_features * X.shape[1]</cite> features.</li>
</ul>
</div></blockquote>
</dd>
<dt>bootstrap <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=False)</span></dt>
<dd>If True, individual trees are fit on random subsets of the training
data sampled with replacement. If False, sampling without replacement
is performed.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The number of jobs to run in parallel for both <cite>fit</cite> and <cite>predict</cite>.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>behaviour <span class="classifier-delimiter">:</span> <span class="classifier">str, default=’old’</span></dt>
<dd><p class="first">Behaviour of the <code class="docutils literal"><span class="pre">decision_function</span></code> which can be either ‘old’ or
‘new’. Passing <code class="docutils literal"><span class="pre">behaviour='new'</span></code> makes the <code class="docutils literal"><span class="pre">decision_function</span></code>
change to match other anomaly detection algorithm API which will be
the default behaviour in the future. As explained in details in the
<code class="docutils literal"><span class="pre">offset_</span></code> attribute documentation, the <code class="docutils literal"><span class="pre">decision_function</span></code> becomes
dependent on the contamination parameter, in such a way that 0 becomes
its natural threshold to detect outliers.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.20: </span><code class="docutils literal"><span class="pre">behaviour</span></code> is added in 0.20 for back-compatibility purpose.</p>
</div>
<div class="deprecated">
<p><span class="versionmodified">Deprecated since version 0.20: </span><code class="docutils literal"><span class="pre">behaviour='old'</span></code> is deprecated in 0.20 and will not be possible
in 0.22.</p>
</div>
<div class="last deprecated">
<p><span class="versionmodified">Deprecated since version 0.22: </span><code class="docutils literal"><span class="pre">behaviour</span></code> parameter will be deprecated in 0.22 and removed in
0.24.</p>
</div>
</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=0)</span></dt>
<dd>Controls the verbosity of the tree building process.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">estimators_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list of DecisionTreeClassifier</span></dt>
<dd>The collection of fitted sub-estimators.</dd>
<dt><code class="docutils literal"><span class="pre">estimators_samples_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list of arrays</span></dt>
<dd>The subset of drawn samples (i.e., the in-bag samples) for each base
estimator.</dd>
<dt><code class="docutils literal"><span class="pre">max_samples_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd>The actual number of samples</dd>
<dt><code class="docutils literal"><span class="pre">offset_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Offset used to define the decision function from the raw scores.
We have the relation: <code class="docutils literal"><span class="pre">decision_function</span> <span class="pre">=</span> <span class="pre">score_samples</span> <span class="pre">-</span> <span class="pre">offset_</span></code>.
Assuming behaviour == ‘new’, <code class="docutils literal"><span class="pre">offset_</span></code> is defined as follows.
When the contamination parameter is set to “auto”, the offset is equal
to -0.5 as the scores of inliers are close to 0 and the scores of
outliers are close to -1. When a contamination parameter different
than “auto” is provided, the offset is defined in such a way we obtain
the expected number of outliers (samples with decision function &lt; 0)
in training.
Assuming the behaviour parameter is set to ‘old’, we always have
<code class="docutils literal"><span class="pre">offset_</span> <span class="pre">=</span> <span class="pre">-0.5</span></code>, making the decision function independent from the
contamination parameter.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id50" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. “Isolation forest.”
Data Mining, 2008. ICDM’08. Eighth IEEE International Conference on.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id51" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. “Isolation-based
anomaly detection.” ACM Transactions on Knowledge Discovery from
Data (TKDD) 6.1 (2012): 3.</td></tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#IsolationForestScikitsLearnNode">IsolationForestScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.CalibratedClassifierCVScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">CalibratedClassifierCVScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.CalibratedClassifierCVScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Probability calibration with isotonic regression or sigmoid.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.calibration.CalibratedClassifierCV</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
See glossary entry for <span class="xref std std-term">cross-validation estimator</span>.</p>
<p>With this class, the base_estimator is fit on the train set of the
cross-validation generator and the test set is used for calibration.
The probabilities for each of the folds are then averaged
for prediction. In case that cv=”prefit” is passed to <code class="docutils literal"><span class="pre">__init__</span></code>,
it is assumed that base_estimator has been fitted already and all
data is used for calibration. Note that data for fitting the
classifier and for calibrating it must be disjoint.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>base_estimator <span class="classifier-delimiter">:</span> <span class="classifier">instance BaseEstimator</span></dt>
<dd>The classifier whose output decision function needs to be calibrated
to offer more accurate predict_proba outputs. If cv=prefit, the
classifier must have been fit already on data.</dd>
<dt>method <span class="classifier-delimiter">:</span> <span class="classifier">‘sigmoid’ or ‘isotonic’</span></dt>
<dd>The method to use for calibration. Can be ‘sigmoid’ which
corresponds to Platt’s method or ‘isotonic’ which is a
non-parametric approach. It is not advised to use isotonic calibration
with too few calibration samples <code class="docutils literal"><span class="pre">(&lt;&lt;1000)</span></code> since it tends to
overfit.
Use sigmoids (Platt’s calibration) in this case.</dd>
<dt>cv <span class="classifier-delimiter">:</span> <span class="classifier">integer, cross-validation generator, iterable or “prefit”, optional</span></dt>
<dd><p class="first">Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul class="simple">
<li>None, to use the default 3-fold cross-validation,</li>
<li>integer, to specify the number of folds.</li>
<li><span class="xref std std-term">CV splitter</span>,</li>
<li>An iterable yielding (train, test) splits as arrays of indices.</li>
</ul>
<p>For integer/None inputs, if <code class="docutils literal"><span class="pre">y</span></code> is binary or multiclass,
<code class="xref py py-class docutils literal"><span class="pre">sklearn.model_selection.StratifiedKFold</span></code> is used. If <code class="docutils literal"><span class="pre">y</span></code> is
neither binary nor multiclass, <code class="xref py py-class docutils literal"><span class="pre">sklearn.model_selection.KFold</span></code>
is used.</p>
<p>Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
<p>If “prefit” is passed, it is assumed that base_estimator has been
fitted already and all data is used for calibration.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.20: </span><code class="docutils literal"><span class="pre">cv</span></code> default value if None will change from 3-fold to 5-fold
in v0.22.</p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">classes_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes)</span></dt>
<dd>The class labels.</dd>
<dt><code class="docutils literal"><span class="pre">calibrated_classifiers_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list (len() equal to cv or 1 if cv == “prefit”)</span></dt>
<dd>The list of calibrated classifiers, one for each crossvalidation fold,
which has been fitted on all but the validation fold and calibrated
on the validation fold.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id52" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Obtaining calibrated probability estimates from decision trees
and naive Bayesian classifiers, B. Zadrozny &amp; C. Elkan, ICML 2001</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id53" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>Transforming Classifier Scores into Accurate Multiclass
Probability Estimates, B. Zadrozny &amp; C. Elkan, (KDD 2002)</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id54" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td>Probabilistic Outputs for Support Vector Machines and Comparisons to
Regularized Likelihood Methods, J. Platt, (1999)</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id55" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td>Predicting Good Probabilities with Supervised Learning,
A. Niculescu-Mizil &amp; R. Caruana, ICML 2005</td></tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#CalibratedClassifierCVScikitsLearnNode">CalibratedClassifierCVScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.ExtraTreeClassifierScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">ExtraTreeClassifierScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.ExtraTreeClassifierScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>An extremely randomized tree classifier.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.tree.tree.ExtraTreeClassifier</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Extra-trees differ from classic decision trees in the way they are built.
When looking for the best split to separate the samples of a node into two
groups, random splits are drawn for each of the <cite>max_features</cite> randomly
selected features and the best split among those is chosen. When
<cite>max_features</cite> is set 1, this amounts to building a totally random
decision tree.</p>
<p>Warning: Extra-trees should only be used within ensemble methods.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>criterion <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=”gini”)</span></dt>
<dd>The function to measure the quality of a split. Supported criteria are
“gini” for the Gini impurity and “entropy” for the information gain.</dd>
<dt>splitter <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=”random”)</span></dt>
<dd>The strategy used to choose the split at each node. Supported
strategies are “best” to choose the best split and “random” to choose
the best random split.</dd>
<dt>max_depth <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</dd>
<dt>min_samples_split <span class="classifier-delimiter">:</span> <span class="classifier">int, float, optional (default=2)</span></dt>
<dd><p class="first">The minimum number of samples required to split an internal node:</p>
<ul class="simple">
<li>If int, then consider <cite>min_samples_split</cite> as the minimum number.</li>
<li>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</li>
</ul>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</dd>
<dt>min_samples_leaf <span class="classifier-delimiter">:</span> <span class="classifier">int, float, optional (default=1)</span></dt>
<dd><p class="first">The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul class="simple">
<li>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</li>
<li>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</li>
</ul>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</dd>
<dt>min_weight_fraction_leaf <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span></dt>
<dd>The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int, float, string or None, optional (default=”auto”)</span></dt>
<dd><p class="first">The number of features to consider when looking for the best split:</p>
<blockquote>
<div><ul class="simple">
<li>If int, then consider <cite>max_features</cite> features at each split.</li>
<li>If float, then <cite>max_features</cite> is a fraction and
<cite>int(max_features * n_features)</cite> features are considered at each
split.</li>
<li>If “auto”, then <cite>max_features=sqrt(n_features)</cite>.</li>
<li>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite>.</li>
<li>If “log2”, then <cite>max_features=log2(n_features)</cite>.</li>
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
</div></blockquote>
<p class="last">Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal"><span class="pre">max_features</span></code> features.</p>
</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>max_leaf_nodes <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Grow a tree with <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</dd>
<dt>min_impurity_decrease <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span></dt>
<dd><p class="first">A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal"><span class="pre">N</span></code>, <code class="docutils literal"><span class="pre">N_t</span></code>, <code class="docutils literal"><span class="pre">N_t_R</span></code> and <code class="docutils literal"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19.</span></p>
</div>
</dd>
<dt>min_impurity_split <span class="classifier-delimiter">:</span> <span class="classifier">float, (default=1e-7)</span></dt>
<dd><p class="first">Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="last deprecated">
<p><span class="versionmodified">Deprecated since version 0.19: </span><code class="docutils literal"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal"><span class="pre">min_impurity_split</span></code> will change from 1e-7 to 0 in 0.23 and it
will be removed in 0.25. Use <code class="docutils literal"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</dd>
<dt>class_weight <span class="classifier-delimiter">:</span> <span class="classifier">dict, list of dicts, “balanced” or None, default=None</span></dt>
<dd><p class="first">Weights associated with classes in the form <code class="docutils literal"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
If not given, all classes are supposed to have weight one. For
multi-output problems, a list of dicts can be provided in the same
order as the columns of y.</p>
<p>Note that for multioutput (including multilabel) weights should be
defined for each class of every column in its own dict. For example,
for four-class multilabel classification weights should be
[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
[{1:1}, {2:5}, {3:1}, {4:1}].</p>
<p>The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
<p>For multi-output, the weights of each column of y will be multiplied.</p>
<p class="last">Note that these weights will be multiplied with sample_weight (passed
through the fit method) if sample_weight is specified.</p>
</dd>
</dl>
<p>See also</p>
<p>ExtraTreeRegressor, sklearn.ensemble.ExtraTreesClassifier,
sklearn.ensemble.ExtraTreesRegressor</p>
<p><strong>Notes</strong></p>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code class="docutils literal"><span class="pre">max_depth</span></code>, <code class="docutils literal"><span class="pre">min_samples_leaf</span></code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id56" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized trees”,
Machine Learning, 63(1), 3-42, 2006.</td></tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#ExtraTreeClassifierScikitsLearnNode">ExtraTreeClassifierScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.SelectKBestScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">SelectKBestScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.SelectKBestScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Select features according to the k highest scores.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.feature_selection.univariate_selection.SelectKBest</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>score_func <span class="classifier-delimiter">:</span> <span class="classifier">callable</span></dt>
<dd>Function taking two arrays X and y, and returning a pair of arrays
(scores, pvalues) or a single array with scores.
Default is f_classif (see below “See also”). The default function only
works with classification tasks.</dd>
<dt>k <span class="classifier-delimiter">:</span> <span class="classifier">int or “all”, optional, default=10</span></dt>
<dd>Number of top features to select.
The “all” option bypasses selection, for use in a parameter search.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">scores_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>Scores of features.</dd>
<dt><code class="docutils literal"><span class="pre">pvalues_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>p-values of feature scores, None if <cite>score_func</cite> returned only scores.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectKBest</span><span class="p">,</span> <span class="n">chi2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(1797, 64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_new</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">chi2</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_new</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(1797, 20)</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>Ties between features with equal scores will be broken in an unspecified
way.</p>
<p>See also</p>
<p>f_classif: ANOVA F-value between label/feature for classification tasks.
mutual_info_classif: Mutual information for a discrete target.
chi2: Chi-squared stats of non-negative features for classification tasks.
f_regression: F-value between label/feature for regression tasks.
mutual_info_regression: Mutual information for a continuous target.
SelectPercentile: Select features based on percentile of the highest scores.
SelectFpr: Select features based on a false positive rate test.
SelectFdr: Select features based on an estimated false discovery rate.
SelectFwe: Select features based on family-wise error rate.
GenericUnivariateSelect: Univariate feature selector with configurable mode.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.SelectKBestScikitsLearnNode-class.html">SelectKBestScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.NormalizerScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">NormalizerScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.NormalizerScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Normalize samples individually to unit norm.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.preprocessing.data.Normalizer</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Each sample (i.e. each row of the data matrix) with at least one
non zero component is rescaled independently of other samples so
that its norm (l1 or l2) equals one.</p>
<p>This transformer is able to work both with dense numpy arrays and
scipy.sparse matrix (use CSR format if you want to avoid the burden of
a copy / conversion).</p>
<p>Scaling inputs to unit norms is a common operation for text
classification or clustering for instance. For instance the dot
product of two l2-normalized TF-IDF vectors is the cosine similarity
of the vectors and is the base similarity metric for the Vector
Space Model commonly used by the Information Retrieval community.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>norm <span class="classifier-delimiter">:</span> <span class="classifier">‘l1’, ‘l2’, or ‘max’, optional (‘l2’ by default)</span></dt>
<dd>The norm to use to normalize each non zero sample.</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>set to False to perform inplace row normalization and avoid a
copy (if the input is already a numpy array or a scipy.sparse
CSR matrix).</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">Normalizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span> <span class="o">=</span> <span class="n">Normalizer</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1"># fit does nothing.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span>
<span class="go">Normalizer(copy=True, norm=&#39;l2&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([[0.8, 0.2, 0.4, 0.4],</span>
<span class="go">       [0.1, 0.3, 0.9, 0.3],</span>
<span class="go">       [0.5, 0.7, 0.5, 0.1]])</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>This estimator is stateless (besides constructor parameters), the
fit method does nothing but is useful when used in a pipeline.</p>
<p>For a comparison of the different scalers, transformers, and normalizers,
see <span class="xref std std-ref">examples/preprocessing/plot_all_scaling.py</span>.</p>
<p>See also</p>
<p>normalize: Equivalent function without the estimator API.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.NormalizerScikitsLearnNode-class.html">NormalizerScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.TfidfTransformerScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">TfidfTransformerScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.TfidfTransformerScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform a count matrix to a normalized tf or tf-idf representation
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.feature_extraction.text.TfidfTransformer</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Tf means term-frequency while tf-idf means term-frequency times inverse
document-frequency. This is a common term weighting scheme in information
retrieval, that has also found good use in document classification.</p>
<p>The goal of using tf-idf instead of the raw frequencies of occurrence of a
token in a given document is to scale down the impact of tokens that occur
very frequently in a given corpus and that are hence empirically less
informative than features that occur in a small fraction of the training
corpus.</p>
<p>The formula that is used to compute the tf-idf for a term t of a document d
in a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is
computed as idf(t) = log [ n / df(t) ] + 1 (if <code class="docutils literal"><span class="pre">smooth_idf=False</span></code>), where
n is the total number of documents in the document set and df(t) is the
document frequency of t; the document frequency is the number of documents
in the document set that contain the term t. The effect of adding “1” to
the idf in the equation above is that terms with zero idf, i.e., terms
that occur in all documents in a training set, will not be entirely
ignored.
(Note that the idf formula above differs from the standard textbook
notation that defines the idf as
idf(t) = log [ n / (df(t) + 1) ]).</p>
<p>If <code class="docutils literal"><span class="pre">smooth_idf=True</span></code> (the default), the constant “1” is added to the
numerator and denominator of the idf as if an extra document was seen
containing every term in the collection exactly once, which prevents
zero divisions: idf(d, t) = log [ (1 + n) / (1 + df(d, t)) ] + 1.</p>
<p>Furthermore, the formulas used to compute tf and idf depend
on parameter settings that correspond to the SMART notation used in IR
as follows:</p>
<p>Tf is “n” (natural) by default, “l” (logarithmic) when
<code class="docutils literal"><span class="pre">sublinear_tf=True</span></code>.
Idf is “t” when use_idf is given, “n” (none) otherwise.
Normalization is “c” (cosine) when <code class="docutils literal"><span class="pre">norm='l2'</span></code>, “n” (none)
when <code class="docutils literal"><span class="pre">norm=None</span></code>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>norm <span class="classifier-delimiter">:</span> <span class="classifier">‘l1’, ‘l2’ or None, optional (default=’l2’)</span></dt>
<dd><p class="first">Each output row will have unit norm, either:</p>
<ul class="last simple">
<li><ul class="first">
<li>‘l2’: Sum of squares of vector elements is 1. The cosine</li>
</ul>
</li>
<li>similarity between two vectors is their dot product when l2 norm has</li>
<li>been applied.</li>
<li><ul class="first">
<li>‘l1’: Sum of absolute values of vector elements is 1.</li>
</ul>
</li>
<li>See <code class="xref py py-func docutils literal"><span class="pre">preprocessing.normalize()</span></code></li>
</ul>
</dd>
<dt>use_idf <span class="classifier-delimiter">:</span> <span class="classifier">boolean (default=True)</span></dt>
<dd>Enable inverse-document-frequency reweighting.</dd>
<dt>smooth_idf <span class="classifier-delimiter">:</span> <span class="classifier">boolean (default=True)</span></dt>
<dd>Smooth idf weights by adding one to document frequencies, as if an
extra document was seen containing every term in the collection
exactly once. Prevents zero divisions.</dd>
<dt>sublinear_tf <span class="classifier-delimiter">:</span> <span class="classifier">boolean (default=False)</span></dt>
<dd>Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">idf_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features)</span></dt>
<dd>The inverse document frequency (IDF) vector; only defined
if  <code class="docutils literal"><span class="pre">use_idf</span></code> is True.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils citation" frame="void" id="yates2011" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[Yates2011]</td><td><cite>R. Baeza-Yates and B. Ribeiro-Neto (2011). Modern
Information Retrieval. Addison Wesley, pp. 68-74.</cite></td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="mrs2008" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[MRS2008]</td><td><cite>C.D. Manning, P. Raghavan and H. Schütze  (2008).
Introduction to Information Retrieval. Cambridge University
Press, pp. 118-120.</cite></td></tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.TfidfTransformerScikitsLearnNode-class.html">TfidfTransformerScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.GradientBoostingClassifierScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">GradientBoostingClassifierScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.GradientBoostingClassifierScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Gradient Boosting for classification.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.ensemble.gradient_boosting.GradientBoostingClassifier</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
GB builds an additive model in a
forward stage-wise fashion; it allows for the optimization of
arbitrary differentiable loss functions. In each stage <code class="docutils literal"><span class="pre">n_classes_</span></code>
regression trees are fit on the negative gradient of the
binomial or multinomial deviance loss function. Binary classification
is a special case where only a single regression tree is induced.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>loss <span class="classifier-delimiter">:</span> <span class="classifier">{‘deviance’, ‘exponential’}, optional (default=’deviance’)</span></dt>
<dd>loss function to be optimized. ‘deviance’ refers to
deviance (= logistic regression) for classification
with probabilistic outputs. For loss ‘exponential’ gradient
boosting recovers the AdaBoost algorithm.</dd>
<dt>learning_rate <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.1)</span></dt>
<dd>learning rate shrinks the contribution of each tree by <cite>learning_rate</cite>.
There is a trade-off between learning_rate and n_estimators.</dd>
<dt>n_estimators <span class="classifier-delimiter">:</span> <span class="classifier">int (default=100)</span></dt>
<dd>The number of boosting stages to perform. Gradient boosting
is fairly robust to over-fitting so a large number usually
results in better performance.</dd>
<dt>subsample <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span></dt>
<dd>The fraction of samples to be used for fitting the individual base
learners. If smaller than 1.0 this results in Stochastic Gradient
Boosting. <cite>subsample</cite> interacts with the parameter <cite>n_estimators</cite>.
Choosing <cite>subsample &lt; 1.0</cite> leads to a reduction of variance
and an increase in bias.</dd>
<dt>criterion <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=”friedman_mse”)</span></dt>
<dd><p class="first">The function to measure the quality of a split. Supported criteria
are “friedman_mse” for the mean squared error with improvement
score by Friedman, “mse” for mean squared error, and “mae” for
the mean absolute error. The default value of “friedman_mse” is
generally the best as it can provide a better approximation in
some cases.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.18.</span></p>
</div>
</dd>
<dt>min_samples_split <span class="classifier-delimiter">:</span> <span class="classifier">int, float, optional (default=2)</span></dt>
<dd><p class="first">The minimum number of samples required to split an internal node:</p>
<ul class="simple">
<li>If int, then consider <cite>min_samples_split</cite> as the minimum number.</li>
<li>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</li>
</ul>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</dd>
<dt>min_samples_leaf <span class="classifier-delimiter">:</span> <span class="classifier">int, float, optional (default=1)</span></dt>
<dd><p class="first">The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul class="simple">
<li>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</li>
<li>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</li>
</ul>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</dd>
<dt>min_weight_fraction_leaf <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span></dt>
<dd>The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</dd>
<dt>max_depth <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=3)</span></dt>
<dd>maximum depth of the individual regression estimators. The maximum
depth limits the number of nodes in the tree. Tune this parameter
for best performance; the best value depends on the interaction
of the input variables.</dd>
<dt>min_impurity_decrease <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span></dt>
<dd><p class="first">A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal"><span class="pre">N</span></code>, <code class="docutils literal"><span class="pre">N_t</span></code>, <code class="docutils literal"><span class="pre">N_t_R</span></code> and <code class="docutils literal"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19.</span></p>
</div>
</dd>
<dt>min_impurity_split <span class="classifier-delimiter">:</span> <span class="classifier">float, (default=1e-7)</span></dt>
<dd><p class="first">Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="last deprecated">
<p><span class="versionmodified">Deprecated since version 0.19: </span><code class="docutils literal"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal"><span class="pre">min_impurity_split</span></code> will change from 1e-7 to 0 in 0.23 and it
will be removed in 0.25. Use <code class="docutils literal"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</dd>
<dt>init <span class="classifier-delimiter">:</span> <span class="classifier">estimator, optional</span></dt>
<dd>An estimator object that is used to compute the initial
predictions. <code class="docutils literal"><span class="pre">init</span></code> has to provide <code class="docutils literal"><span class="pre">fit</span></code> and <code class="docutils literal"><span class="pre">predict</span></code>.
If None it uses <code class="docutils literal"><span class="pre">loss.init_estimator</span></code>.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int, float, string or None, optional (default=None)</span></dt>
<dd><p class="first">The number of features to consider when looking for the best split:</p>
<ul class="simple">
<li>If int, then consider <cite>max_features</cite> features at each split.</li>
<li>If float, then <cite>max_features</cite> is a fraction and
<cite>int(max_features * n_features)</cite> features are considered at each
split.</li>
<li>If “auto”, then <cite>max_features=sqrt(n_features)</cite>.</li>
<li>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite>.</li>
<li>If “log2”, then <cite>max_features=log2(n_features)</cite>.</li>
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
<p>Choosing <cite>max_features &lt; n_features</cite> leads to a reduction of variance
and an increase in bias.</p>
<p class="last">Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal"><span class="pre">max_features</span></code> features.</p>
</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, default: 0</span></dt>
<dd>Enable verbose output. If 1 then it prints progress and performance
once in a while (the more trees the lower the frequency). If greater
than 1 then it prints progress and performance for every tree.</dd>
<dt>max_leaf_nodes <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Grow trees with <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, default: False</span></dt>
<dd>When set to <code class="docutils literal"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just erase the
previous solution. See <span class="xref std std-term">the Glossary</span>.</dd>
<dt>presort <span class="classifier-delimiter">:</span> <span class="classifier">bool or ‘auto’, optional (default=’auto’)</span></dt>
<dd><p class="first">Whether to presort the data to speed up the finding of best splits in
fitting. Auto mode by default will use presorting on dense data and
default to normal sorting on sparse data. Setting presort to true on
sparse data will raise an error.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>presort</em> parameter.</p>
</div>
</dd>
<dt>validation_fraction <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default 0.1</span></dt>
<dd><p class="first">The proportion of training data to set aside as validation set for
early stopping. Must be between 0 and 1.
Only used if <code class="docutils literal"><span class="pre">n_iter_no_change</span></code> is set to an integer.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
<dt>n_iter_no_change <span class="classifier-delimiter">:</span> <span class="classifier">int, default None</span></dt>
<dd><p class="first"><code class="docutils literal"><span class="pre">n_iter_no_change</span></code> is used to decide if early stopping will be used
to terminate training when validation score is not improving. By
default it is set to None to disable early stopping. If set to a
number, it will set aside <code class="docutils literal"><span class="pre">validation_fraction</span></code> size of the training
data as validation and terminate training when validation score is not
improving in all of the previous <code class="docutils literal"><span class="pre">n_iter_no_change</span></code> numbers of
iterations.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default 1e-4</span></dt>
<dd><p class="first">Tolerance for the early stopping. When the loss is not improving
by at least tol for <code class="docutils literal"><span class="pre">n_iter_no_change</span></code> iterations (if set to a
number), the training stops.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">n_estimators_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd><p class="first">The number of estimators as selected by early stopping (if
<code class="docutils literal"><span class="pre">n_iter_no_change</span></code> is specified). Otherwise it is set to
<code class="docutils literal"><span class="pre">n_estimators</span></code>.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
<dt><code class="docutils literal"><span class="pre">feature_importances_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,)</span></dt>
<dd>The feature importances (the higher, the more important the feature).</dd>
<dt><code class="docutils literal"><span class="pre">oob_improvement_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_estimators,)</span></dt>
<dd>The improvement in loss (= deviance) on the out-of-bag samples
relative to the previous iteration.
<code class="docutils literal"><span class="pre">oob_improvement_[0]</span></code> is the improvement in
loss of the first stage over the <code class="docutils literal"><span class="pre">init</span></code> estimator.</dd>
<dt><code class="docutils literal"><span class="pre">train_score_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_estimators,)</span></dt>
<dd>The i-th score <code class="docutils literal"><span class="pre">train_score_[i]</span></code> is the deviance (= loss) of the
model at iteration <code class="docutils literal"><span class="pre">i</span></code> on the in-bag sample.
If <code class="docutils literal"><span class="pre">subsample</span> <span class="pre">==</span> <span class="pre">1</span></code> this is the deviance on the training data.</dd>
<dt><code class="docutils literal"><span class="pre">loss_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">LossFunction</span></dt>
<dd>The concrete <code class="docutils literal"><span class="pre">LossFunction</span></code> object.</dd>
<dt><code class="docutils literal"><span class="pre">init_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">estimator</span></dt>
<dd>The estimator that provides the initial predictions.
Set via the <code class="docutils literal"><span class="pre">init</span></code> argument or <code class="docutils literal"><span class="pre">loss.init_estimator</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">estimators_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">ndarray of DecisionTreeRegressor,shape (n_estimators, <code class="docutils literal"><span class="pre">loss_.K</span></code>)</span></dt>
<dd>The collection of fitted sub-estimators. <code class="docutils literal"><span class="pre">loss_.K</span></code> is 1 for binary
classification, otherwise n_classes.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data and
<code class="docutils literal"><span class="pre">max_features=n_features</span></code>, if the improvement of the criterion is
identical for several splits enumerated during the search of the best
split. To obtain a deterministic behaviour during fitting,
<code class="docutils literal"><span class="pre">random_state</span></code> has to be fixed.</p>
<p>See also</p>
<p>sklearn.tree.DecisionTreeClassifier, RandomForestClassifier
AdaBoostClassifier</p>
<p><strong>References</strong></p>
<p>J. Friedman, Greedy Function Approximation: A Gradient Boosting
Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.</p>
<ol class="upperalpha simple" start="10">
<li>Friedman, Stochastic Gradient Boosting, 1999</li>
</ol>
<p>T. Hastie, R. Tibshirani and J. Friedman.
Elements of Statistical Learning Ed. 2, Springer, 2009.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#GradientBoostingClassifierScikitsLearnNode">GradientBoostingClassifierScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.BayesianGaussianMixtureScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">BayesianGaussianMixtureScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.BayesianGaussianMixtureScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Variational Bayesian estimation of a Gaussian mixture.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.mixture.bayesian_mixture.BayesianGaussianMixture</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
This class allows to infer an approximate posterior distribution over the
parameters of a Gaussian mixture distribution. The effective number of
components can be inferred from the data.</p>
<p>This class implements two types of prior for the weights distribution: a
finite mixture model with Dirichlet distribution and an infinite mixture
model with the Dirichlet Process. In practice Dirichlet Process inference
algorithm is approximated and uses a truncated distribution with a fixed
maximum number of components (called the Stick-breaking representation).
The number of components actually used almost always depends on the data.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.18.</span></p>
</div>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int, defaults to 1.</span></dt>
<dd>The number of mixture components. Depending on the data and the value
of the <cite>weight_concentration_prior</cite> the model can decide to not use
all the components by setting some component <cite>weights_</cite> to values very
close to zero. The number of effective components is therefore smaller
than n_components.</dd>
<dt>covariance_type <span class="classifier-delimiter">:</span> <span class="classifier">{‘full’, ‘tied’, ‘diag’, ‘spherical’}, defaults to ‘full’</span></dt>
<dd><p class="first">String describing the type of covariance parameters to use.
Must be one of:</p>
<div class="last highlight-default"><div class="highlight"><pre><span></span><span class="s1">&#39;full&#39;</span> <span class="p">(</span><span class="n">each</span> <span class="n">component</span> <span class="n">has</span> <span class="n">its</span> <span class="n">own</span> <span class="n">general</span> <span class="n">covariance</span> <span class="n">matrix</span><span class="p">),</span>
<span class="s1">&#39;tied&#39;</span> <span class="p">(</span><span class="nb">all</span> <span class="n">components</span> <span class="n">share</span> <span class="n">the</span> <span class="n">same</span> <span class="n">general</span> <span class="n">covariance</span> <span class="n">matrix</span><span class="p">),</span>
<span class="s1">&#39;diag&#39;</span> <span class="p">(</span><span class="n">each</span> <span class="n">component</span> <span class="n">has</span> <span class="n">its</span> <span class="n">own</span> <span class="n">diagonal</span> <span class="n">covariance</span> <span class="n">matrix</span><span class="p">),</span>
<span class="s1">&#39;spherical&#39;</span> <span class="p">(</span><span class="n">each</span> <span class="n">component</span> <span class="n">has</span> <span class="n">its</span> <span class="n">own</span> <span class="n">single</span> <span class="n">variance</span><span class="p">)</span><span class="o">.</span>
</pre></div>
</div>
</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, defaults to 1e-3.</span></dt>
<dd>The convergence threshold. EM iterations will stop when the
lower bound average gain on the likelihood (of the training data with
respect to the model) is below this threshold.</dd>
<dt>reg_covar <span class="classifier-delimiter">:</span> <span class="classifier">float, defaults to 1e-6.</span></dt>
<dd>Non-negative regularization added to the diagonal of covariance.
Allows to assure that the covariance matrices are all positive.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, defaults to 100.</span></dt>
<dd>The number of EM iterations to perform.</dd>
<dt>n_init <span class="classifier-delimiter">:</span> <span class="classifier">int, defaults to 1.</span></dt>
<dd>The number of initializations to perform. The result with the highest
lower bound value on the likelihood is kept.</dd>
<dt>init_params <span class="classifier-delimiter">:</span> <span class="classifier">{‘kmeans’, ‘random’}, defaults to ‘kmeans’.</span></dt>
<dd><p class="first">The method used to initialize the weights, the means and the
covariances.
Must be one of:</p>
<div class="last highlight-default"><div class="highlight"><pre><span></span><span class="s1">&#39;kmeans&#39;</span> <span class="p">:</span> <span class="n">responsibilities</span> <span class="n">are</span> <span class="n">initialized</span> <span class="n">using</span> <span class="n">kmeans</span><span class="o">.</span>
<span class="s1">&#39;random&#39;</span> <span class="p">:</span> <span class="n">responsibilities</span> <span class="n">are</span> <span class="n">initialized</span> <span class="n">randomly</span><span class="o">.</span>
</pre></div>
</div>
</dd>
<dt>weight_concentration_prior_type <span class="classifier-delimiter">:</span> <span class="classifier">str, defaults to ‘dirichlet_process’.</span></dt>
<dd><p class="first">String describing the type of the weight concentration prior.
Must be one of:</p>
<div class="last highlight-default"><div class="highlight"><pre><span></span><span class="s1">&#39;dirichlet_process&#39;</span> <span class="p">(</span><span class="n">using</span> <span class="n">the</span> <span class="n">Stick</span><span class="o">-</span><span class="n">breaking</span> <span class="n">representation</span><span class="p">),</span>
<span class="s1">&#39;dirichlet_distribution&#39;</span> <span class="p">(</span><span class="n">can</span> <span class="n">favor</span> <span class="n">more</span> <span class="n">uniform</span> <span class="n">weights</span><span class="p">)</span><span class="o">.</span>
</pre></div>
</div>
</dd>
<dt>weight_concentration_prior <span class="classifier-delimiter">:</span> <span class="classifier">float | None, optional.</span></dt>
<dd>The dirichlet concentration of each component on the weight
distribution (Dirichlet). This is commonly called gamma in the
literature. The higher concentration puts more mass in
the center and will lead to more components being active, while a lower
concentration parameter will lead to more mass at the edge of the
mixture weights simplex. The value of the parameter must be greater
than 0. If it is None, it’s set to <code class="docutils literal"><span class="pre">1.</span> <span class="pre">/</span> <span class="pre">n_components</span></code>.</dd>
<dt>mean_precision_prior <span class="classifier-delimiter">:</span> <span class="classifier">float | None, optional.</span></dt>
<dd>The precision prior on the mean distribution (Gaussian).
Controls the extend to where means can be placed. Smaller
values concentrate the means of each clusters around <cite>mean_prior</cite>.
The value of the parameter must be greater than 0.
If it is None, it’s set to 1.</dd>
<dt>mean_prior <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_features,), optional</span></dt>
<dd>The prior on the mean distribution (Gaussian).
If it is None, it’s set to the mean of X.</dd>
<dt>degrees_of_freedom_prior <span class="classifier-delimiter">:</span> <span class="classifier">float | None, optional.</span></dt>
<dd>The prior of the number of degrees of freedom on the covariance
distributions (Wishart). If it is None, it’s set to <cite>n_features</cite>.</dd>
<dt>covariance_prior <span class="classifier-delimiter">:</span> <span class="classifier">float or array-like, optional</span></dt>
<dd><p class="first">The prior on the covariance distribution (Wishart).
If it is None, the emiprical covariance prior is initialized using the
covariance of X. The shape depends on <cite>covariance_type</cite>:</p>
<div class="last highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;full&#39;</span><span class="p">,</span>
<span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;tied&#39;</span><span class="p">,</span>
<span class="p">(</span><span class="n">n_features</span><span class="p">)</span>             <span class="k">if</span> <span class="s1">&#39;diag&#39;</span><span class="p">,</span>
<span class="nb">float</span>                    <span class="k">if</span> <span class="s1">&#39;spherical&#39;</span>
</pre></div>
</div>
</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, default to False.</span></dt>
<dd>If ‘warm_start’ is True, the solution of the last fitting is used as
initialization for the next call of fit(). This can speed up
convergence when fit is called several times on similar problems.
See <span class="xref std std-term">the Glossary</span>.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, default to 0.</span></dt>
<dd>Enable verbose output. If 1 then it prints the current
initialization and each iteration step. If greater than 1 then
it prints also the log probability and the time needed
for each step.</dd>
<dt>verbose_interval <span class="classifier-delimiter">:</span> <span class="classifier">int, default to 10.</span></dt>
<dd>Number of iteration done before the next print.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">weights_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_components,)</span></dt>
<dd>The weights of each mixture components.</dd>
<dt><code class="docutils literal"><span class="pre">means_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_components, n_features)</span></dt>
<dd>The mean of each mixture component.</dd>
<dt><code class="docutils literal"><span class="pre">covariances_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like</span></dt>
<dd><p class="first">The covariance of each mixture component.
The shape depends on <cite>covariance_type</cite>:</p>
<div class="last highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">n_components</span><span class="p">,)</span>                        <span class="k">if</span> <span class="s1">&#39;spherical&#39;</span><span class="p">,</span>
<span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>               <span class="k">if</span> <span class="s1">&#39;tied&#39;</span><span class="p">,</span>
<span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>             <span class="k">if</span> <span class="s1">&#39;diag&#39;</span><span class="p">,</span>
<span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;full&#39;</span>
</pre></div>
</div>
</dd>
<dt><code class="docutils literal"><span class="pre">precisions_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like</span></dt>
<dd><p class="first">The precision matrices for each component in the mixture. A precision
matrix is the inverse of a covariance matrix. A covariance matrix is
symmetric positive definite so the mixture of Gaussian can be
equivalently parameterized by the precision matrices. Storing the
precision matrices instead of the covariance matrices makes it more
efficient to compute the log-likelihood of new samples at test time.
The shape depends on <code class="docutils literal"><span class="pre">covariance_type</span></code>:</p>
<div class="last highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">n_components</span><span class="p">,)</span>                        <span class="k">if</span> <span class="s1">&#39;spherical&#39;</span><span class="p">,</span>
<span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>               <span class="k">if</span> <span class="s1">&#39;tied&#39;</span><span class="p">,</span>
<span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>             <span class="k">if</span> <span class="s1">&#39;diag&#39;</span><span class="p">,</span>
<span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;full&#39;</span>
</pre></div>
</div>
</dd>
<dt><code class="docutils literal"><span class="pre">precisions_cholesky_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like</span></dt>
<dd><p class="first">The cholesky decomposition of the precision matrices of each mixture
component. A precision matrix is the inverse of a covariance matrix.
A covariance matrix is symmetric positive definite so the mixture of
Gaussian can be equivalently parameterized by the precision matrices.
Storing the precision matrices instead of the covariance matrices makes
it more efficient to compute the log-likelihood of new samples at test
time. The shape depends on <code class="docutils literal"><span class="pre">covariance_type</span></code>:</p>
<div class="last highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">n_components</span><span class="p">,)</span>                        <span class="k">if</span> <span class="s1">&#39;spherical&#39;</span><span class="p">,</span>
<span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>               <span class="k">if</span> <span class="s1">&#39;tied&#39;</span><span class="p">,</span>
<span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>             <span class="k">if</span> <span class="s1">&#39;diag&#39;</span><span class="p">,</span>
<span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;full&#39;</span>
</pre></div>
</div>
</dd>
<dt><code class="docutils literal"><span class="pre">converged_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>True when convergence was reached in fit(), False otherwise.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of step used by the best fit of inference to reach the
convergence.</dd>
<dt><code class="docutils literal"><span class="pre">lower_bound_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Lower bound value on the likelihood (of the training data with
respect to the model) of the best fit of inference.</dd>
<dt><code class="docutils literal"><span class="pre">weight_concentration_prior_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">tuple or float</span></dt>
<dd><p class="first">The dirichlet concentration of each component on the weight
distribution (Dirichlet). The type depends on
<code class="docutils literal"><span class="pre">weight_concentration_prior_type</span></code>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;dirichlet_process&#39;</span> <span class="p">(</span><span class="n">Beta</span> <span class="n">parameters</span><span class="p">),</span>
<span class="nb">float</span>          <span class="k">if</span> <span class="s1">&#39;dirichlet_distribution&#39;</span> <span class="p">(</span><span class="n">Dirichlet</span> <span class="n">parameters</span><span class="p">)</span><span class="o">.</span>
</pre></div>
</div>
<p class="last">The higher concentration puts more mass in
the center and will lead to more components being active, while a lower
concentration parameter will lead to more mass at the edge of the
simplex.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">weight_concentration_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_components,)</span></dt>
<dd>The dirichlet concentration of each component on the weight
distribution (Dirichlet).</dd>
<dt>mean_precision_prior <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The precision prior on the mean distribution (Gaussian).
Controls the extend to where means can be placed.
Smaller values concentrate the means of each clusters around
<cite>mean_prior</cite>.</dd>
<dt><code class="docutils literal"><span class="pre">mean_precision_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_components,)</span></dt>
<dd>The precision of each components on the mean distribution (Gaussian).</dd>
<dt><code class="docutils literal"><span class="pre">mean_prior_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_features,)</span></dt>
<dd>The prior on the mean distribution (Gaussian).</dd>
<dt><code class="docutils literal"><span class="pre">degrees_of_freedom_prior_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The prior of the number of degrees of freedom on the covariance
distributions (Wishart).</dd>
<dt><code class="docutils literal"><span class="pre">degrees_of_freedom_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_components,)</span></dt>
<dd>The number of degrees of freedom of each components in the model.</dd>
<dt><code class="docutils literal"><span class="pre">covariance_prior_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float or array-like</span></dt>
<dd><p class="first">The prior on the covariance distribution (Wishart).
The shape depends on <cite>covariance_type</cite>:</p>
<div class="last highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;full&#39;</span><span class="p">,</span>
<span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;tied&#39;</span><span class="p">,</span>
<span class="p">(</span><span class="n">n_features</span><span class="p">)</span>             <span class="k">if</span> <span class="s1">&#39;diag&#39;</span><span class="p">,</span>
<span class="nb">float</span>                    <span class="k">if</span> <span class="s1">&#39;spherical&#39;</span>
</pre></div>
</div>
</dd>
</dl>
<p>See Also</p>
<p>GaussianMixture : Finite Gaussian mixture fit with EM.</p>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id57" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><a class="reference external" href="https://www.springer.com/kr/book/9780387310732">Bishop, Christopher M. (2006). “Pattern recognition and machine
learning”. Vol. 4 No. 4. New York: Springer.</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id58" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.36.2841&amp;rep=rep1&amp;type=pdf">Hagai Attias. (2000). “A Variational Bayesian Framework for
Graphical Models”. In Advances in Neural Information Processing
Systems 12.</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id59" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td><a class="reference external" href="https://www.cs.princeton.edu/courses/archive/fall11/cos597C/reading/BleiJordan2005.pdf">Blei, David M. and Michael I. Jordan. (2006). “Variational
inference for Dirichlet process mixtures”. Bayesian analysis 1.1</a></td></tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#BayesianGaussianMixtureScikitsLearnNode">BayesianGaussianMixtureScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.NystroemScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">NystroemScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.NystroemScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Approximate a kernel map using a subset of the training data.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.kernel_approximation.Nystroem</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Constructs an approximate feature map for an arbitrary kernel
using a subset of the data as basis.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>kernel <span class="classifier-delimiter">:</span> <span class="classifier">string or callable, default=”rbf”</span></dt>
<dd>Kernel map to be approximated. A callable should accept two arguments
and the keyword arguments passed to this object as kernel_params, and
should return a floating point number.</dd>
<dt>gamma <span class="classifier-delimiter">:</span> <span class="classifier">float, default=None</span></dt>
<dd>Gamma parameter for the RBF, laplacian, polynomial, exponential chi2
and sigmoid kernels. Interpretation of the default value is left to
the kernel; see the documentation for sklearn.metrics.pairwise.
Ignored by other kernels.</dd>
<dt>coef0 <span class="classifier-delimiter">:</span> <span class="classifier">float, default=None</span></dt>
<dd>Zero coefficient for polynomial and sigmoid kernels.
Ignored by other kernels.</dd>
<dt>degree <span class="classifier-delimiter">:</span> <span class="classifier">float, default=None</span></dt>
<dd>Degree of the polynomial kernel. Ignored by other kernels.</dd>
<dt>kernel_params <span class="classifier-delimiter">:</span> <span class="classifier">mapping of string to any, optional</span></dt>
<dd>Additional parameters (keyword arguments) for kernel function passed
as callable object.</dd>
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of features to construct.
How many data points will be used to construct the mapping.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">components_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_components, n_features)</span></dt>
<dd>Subset of training points used to construct the feature map.</dd>
<dt><code class="docutils literal"><span class="pre">component_indices_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_components)</span></dt>
<dd>Indices of <code class="docutils literal"><span class="pre">components_</span></code> in the training set.</dd>
<dt><code class="docutils literal"><span class="pre">normalization_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_components, n_components)</span></dt>
<dd>Normalization matrix needed for embedding.
Square root of the kernel matrix on <code class="docutils literal"><span class="pre">components_</span></code>.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">svm</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.kernel_approximation</span> <span class="kn">import</span> <span class="n">Nystroem</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">digits</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_digits</span><span class="p">(</span><span class="n">n_class</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">data</span> <span class="o">/</span> <span class="mf">16.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">LinearSVC</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">feature_map_nystroem</span> <span class="o">=</span> <span class="n">Nystroem</span><span class="p">(</span><span class="n">gamma</span><span class="o">=.</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>                                <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>                                <span class="n">n_components</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_transformed</span> <span class="o">=</span> <span class="n">feature_map_nystroem</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_transformed</span><span class="p">,</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,</span>
<span class="go">     intercept_scaling=1, loss=&#39;squared_hinge&#39;, max_iter=1000,</span>
<span class="go">     multi_class=&#39;ovr&#39;, penalty=&#39;l2&#39;, random_state=None, tol=0.0001,</span>
<span class="go">     verbose=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">data_transformed</span><span class="p">,</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">)</span> 
<span class="go">0.9987...</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<ul class="simple">
<li>Williams, C.K.I. and Seeger, M.
“Using the Nystroem method to speed up kernel machines”,
Advances in neural information processing systems 2001</li>
<li>T. Yang, Y. Li, M. Mahdavi, R. Jin and Z. Zhou
“Nystroem Method vs Random Fourier Features: A Theoretical and Empirical
Comparison”,
Advances in Neural Information Processing Systems 2012</li>
</ul>
<p>See also</p>
<dl class="docutils">
<dt>RBFSampler <span class="classifier-delimiter">:</span> <span class="classifier">An approximation to the RBF kernel using random Fourier</span></dt>
<dd>features.</dd>
</dl>
<p>sklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#NystroemScikitsLearnNode">NystroemScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.GaussianRandomProjectionHashScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">GaussianRandomProjectionHashScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.GaussianRandomProjectionHashScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Use GaussianRandomProjection to produce a cosine LSH fingerprint
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.neighbors.approximate.GaussianRandomProjectionHash</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
<strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int or ‘auto’, optional (default = 32)</span></dt>
<dd><p class="first">Dimensionality of the target projection space.</p>
<p>n_components can be automatically adjusted according to the
number of samples in the dataset and the bound given by the
Johnson-Lindenstrauss lemma. In that case the quality of the
embedding is controlled by the <code class="docutils literal"><span class="pre">eps</span></code> parameter.</p>
<p class="last">It should be noted that Johnson-Lindenstrauss lemma can yield
very conservative estimated of the required number of components
as it makes no assumption on the structure of the dataset.</p>
</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
</dl>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#GaussianRandomProjectionHashScikitsLearnNode">GaussianRandomProjectionHashScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.VotingClassifierScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">VotingClassifierScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.VotingClassifierScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Soft Voting/Majority Rule classifier for unfitted estimators.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.ensemble.voting_classifier.VotingClassifier</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
.. versionadded:: 0.17</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>estimators <span class="classifier-delimiter">:</span> <span class="classifier">list of (string, estimator) tuples</span></dt>
<dd>Invoking the <code class="docutils literal"><span class="pre">fit</span></code> method on the <code class="docutils literal"><span class="pre">VotingClassifier</span></code> will fit clones
of those original estimators that will be stored in the class attribute
<code class="docutils literal"><span class="pre">self.estimators_</span></code>. An estimator can be set to <cite>None</cite> using
<code class="docutils literal"><span class="pre">set_params</span></code>.</dd>
<dt>voting <span class="classifier-delimiter">:</span> <span class="classifier">str, {‘hard’, ‘soft’} (default=’hard’)</span></dt>
<dd>If ‘hard’, uses predicted class labels for majority rule voting.
Else if ‘soft’, predicts the class label based on the argmax of
the sums of the predicted probabilities, which is recommended for
an ensemble of well-calibrated classifiers.</dd>
<dt>weights <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_classifiers], optional (default=`None`)</span></dt>
<dd>Sequence of weights (<cite>float</cite> or <cite>int</cite>) to weight the occurrences of
predicted class labels (<cite>hard</cite> voting) or class probabilities
before averaging (<cite>soft</cite> voting). Uses uniform weights if <cite>None</cite>.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The number of jobs to run in parallel for <code class="docutils literal"><span class="pre">fit</span></code>.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>flatten_transform <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default=None)</span></dt>
<dd>Affects shape of transform output only when voting=’soft’
If voting=’soft’ and flatten_transform=True, transform method returns
matrix with shape (n_samples, n_classifiers * n_classes). If
flatten_transform=False, it returns
(n_classifiers, n_samples, n_classes).</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">estimators_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list of classifiers</span></dt>
<dd>The collection of fitted sub-estimators as defined in <code class="docutils literal"><span class="pre">estimators</span></code>
that are not <cite>None</cite>.</dd>
<dt><code class="docutils literal"><span class="pre">named_estimators_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">Bunch object, a dictionary with attribute access</span></dt>
<dd><p class="first">Attribute to access any fitted sub-estimators by name.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
<dt><code class="docutils literal"><span class="pre">classes_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_predictions]</span></dt>
<dd>The classes labels.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span><span class="p">,</span> <span class="n">VotingClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf1</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;multinomial&#39;</span><span class="p">,</span>
<span class="gp">... </span>                          <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf2</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf3</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf1</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="p">[</span>
<span class="gp">... </span>        <span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;gnb&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span> <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;hard&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf1</span> <span class="o">=</span> <span class="n">eclf1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">eclf1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="go">[1 1 1 2 2 2]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">array_equal</span><span class="p">(</span><span class="n">eclf1</span><span class="o">.</span><span class="n">named_estimators_</span><span class="o">.</span><span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span>
<span class="gp">... </span>               <span class="n">eclf1</span><span class="o">.</span><span class="n">named_estimators_</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf2</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="p">[</span>
<span class="gp">... </span>        <span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;gnb&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span>
<span class="gp">... </span>        <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;soft&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf2</span> <span class="o">=</span> <span class="n">eclf2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">eclf2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="go">[1 1 1 2 2 2]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf3</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="p">[</span>
<span class="gp">... </span>       <span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;gnb&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span>
<span class="gp">... </span>       <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;soft&#39;</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>       <span class="n">flatten_transform</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf3</span> <span class="o">=</span> <span class="n">eclf3</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">eclf3</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="go">[1 1 1 2 2 2]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">eclf3</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(6, 6)</span>
</pre></div>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#VotingClassifierScikitsLearnNode">VotingClassifierScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.LassoLarsCVScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">LassoLarsCVScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.LassoLarsCVScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Cross-validated Lasso, using the LARS algorithm.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.least_angle.LassoLarsCV</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
See glossary entry for <span class="xref std std-term">cross-validation estimator</span>.</p>
<p>The optimization objective for Lasso is:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2_2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
</pre></div>
</div>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">boolean or integer, optional</span></dt>
<dd>Sets the verbosity amount</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span></dt>
<dd>Maximum number of iterations to perform.</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>This parameter is ignored when <code class="docutils literal"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<code class="xref py py-class docutils literal"><span class="pre">sklearn.preprocessing.StandardScaler</span></code> before calling <code class="docutils literal"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal"><span class="pre">normalize=False</span></code>.</dd>
<dt>precompute <span class="classifier-delimiter">:</span> <span class="classifier">True | False | ‘auto’</span></dt>
<dd>Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal"><span class="pre">'auto'</span></code> let us decide. The Gram matrix
cannot be passed as argument since we will use only subsets of X.</dd>
<dt>cv <span class="classifier-delimiter">:</span> <span class="classifier">int, cross-validation generator or an iterable, optional</span></dt>
<dd><p class="first">Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul class="simple">
<li>None, to use the default 3-fold cross-validation,</li>
<li>integer, to specify the number of folds.</li>
<li><span class="xref std std-term">CV splitter</span>,</li>
<li>An iterable yielding (train, test) splits as arrays of indices.</li>
</ul>
<p>For integer/None inputs, <code class="xref py py-class docutils literal"><span class="pre">KFold</span></code> is used.</p>
<p>Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.20: </span><code class="docutils literal"><span class="pre">cv</span></code> default value if None will change from 3-fold to 5-fold
in v0.22.</p>
</div>
</dd>
<dt>max_n_alphas <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span></dt>
<dd>The maximum number of points on the path used to compute the
residuals in the cross-validation</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Number of CPUs to use during the cross validation.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>eps <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The machine-precision regularization in the computation of the
Cholesky diagonal factors. Increase this for very ill-conditioned
systems.</dd>
<dt>copy_X <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>If True, X will be copied; else, it may be overwritten.</dd>
<dt>positive <span class="classifier-delimiter">:</span> <span class="classifier">boolean (default=False)</span></dt>
<dd>Restrict coefficients to be &gt;= 0. Be aware that you might want to
remove fit_intercept which is set True by default.
Under the positive restriction the model coefficients do not converge
to the ordinary-least-squares solution for small values of alpha.
Only coefficients up to the smallest alpha value (<code class="docutils literal"><span class="pre">alphas_[alphas_</span> <span class="pre">&gt;</span>
<span class="pre">0.].min()</span></code> when fit_path=True) reached by the stepwise Lars-Lasso
algorithm are typically in congruence with the solution of the
coordinate descent Lasso estimator.
As a consequence using LassoLarsCV only makes sense for problems where
a sparse solution is expected and/or reached.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,)</span></dt>
<dd>parameter vector (w in the formulation formula)</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>independent term in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">coef_path_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features, n_alphas)</span></dt>
<dd>the varying values of the coefficients along the path</dd>
<dt><code class="docutils literal"><span class="pre">alpha_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>the estimated regularization parameter alpha</dd>
<dt><code class="docutils literal"><span class="pre">alphas_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_alphas,)</span></dt>
<dd>the different values of alpha along the path</dd>
<dt><code class="docutils literal"><span class="pre">cv_alphas_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_cv_alphas,)</span></dt>
<dd>all the values of alpha along the path for the different folds</dd>
<dt><code class="docutils literal"><span class="pre">mse_path_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_folds, n_cv_alphas)</span></dt>
<dd>the mean square error on left-out for each fold along the path
(alpha values given by <code class="docutils literal"><span class="pre">cv_alphas</span></code>)</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like or int</span></dt>
<dd>the number of iterations run by Lars with the optimal alpha.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LassoLarsCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">noise</span><span class="o">=</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">LassoLarsCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">0.9992...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">alpha_</span>
<span class="go">0.0484...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">1</span><span class="p">,])</span>
<span class="go">array([-77.8723...])</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>The object solves the same problem as the LassoCV object. However,
unlike the LassoCV, it find the relevant alphas values by itself.
In general, because of this property, it will be more stable.
However, it is more fragile to heavily multicollinear datasets.</p>
<p>It is more efficient than the LassoCV if only a small number of
features are selected compared to the total number, for instance if
there are very few samples compared to the number of features.</p>
<p>See also</p>
<p>lars_path, LassoLars, LarsCV, LassoCV</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#LassoLarsCVScikitsLearnNode">LassoLarsCVScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.BernoulliRBMScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">BernoulliRBMScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.BernoulliRBMScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bernoulli Restricted Boltzmann Machine (RBM).
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.neural_network.rbm.BernoulliRBM</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
A Restricted Boltzmann Machine with binary visible units and
binary hidden units. Parameters are estimated using Stochastic Maximum
Likelihood (SML), also known as Persistent Contrastive Divergence (PCD)
[2].</p>
<p>The time complexity of this implementation is <code class="docutils literal"><span class="pre">O(d</span> <span class="pre">**</span> <span class="pre">2)</span></code> assuming
d ~ n_features ~ n_components.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Number of binary hidden units.</dd>
<dt>learning_rate <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The learning rate for weight updates. It is <em>highly</em> recommended
to tune this hyper-parameter. Reasonable values are in the
10**[0., -3.] range.</dd>
<dt>batch_size <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Number of examples per minibatch.</dd>
<dt>n_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Number of iterations/sweeps over the training dataset to perform
during training.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>The verbosity level. The default, zero, means silent mode.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">integer or RandomState, optional</span></dt>
<dd>A random number generator instance to define the state of the
random permutations generator. If an integer is given, it fixes the
seed. Defaults to the global numpy random number generator.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">intercept_hidden_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_components,)</span></dt>
<dd>Biases of the hidden units.</dd>
<dt><code class="docutils literal"><span class="pre">intercept_visible_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_features,)</span></dt>
<dd>Biases of the visible units.</dd>
<dt><code class="docutils literal"><span class="pre">components_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_components, n_features)</span></dt>
<dd>Weight matrix, where n_features in the number of
visible units and n_components is the number of hidden units.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">BernoulliRBM</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">BernoulliRBM</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">BernoulliRBM(batch_size=10, learning_rate=0.1, n_components=2, n_iter=10,</span>
<span class="go">       random_state=None, verbose=0)</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<dl class="docutils">
<dt>[1] Hinton, G. E., Osindero, S. and Teh, Y. A fast learning algorithm for</dt>
<dd>deep belief nets. Neural Computation 18, pp 1527-1554.
<a class="reference external" href="https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf">https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf</a></dd>
<dt>[2] Tieleman, T. Training Restricted Boltzmann Machines using</dt>
<dd>Approximations to the Likelihood Gradient. International Conference
on Machine Learning (ICML) 2008</dd>
</dl>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#BernoulliRBMScikitsLearnNode">BernoulliRBMScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.RobustScalerScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">RobustScalerScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.RobustScalerScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Scale features using statistics that are robust to outliers.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.preprocessing.data.RobustScaler</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
This Scaler removes the median and scales the data according to
the quantile range (defaults to IQR: Interquartile Range).
The IQR is the range between the 1st quartile (25th quantile)
and the 3rd quartile (75th quantile).</p>
<p>Centering and scaling happen independently on each feature by
computing the relevant statistics on the samples in the training
set. Median and interquartile range are then stored to be used on
later data using the <code class="docutils literal"><span class="pre">transform</span></code> method.</p>
<p>Standardization of a dataset is a common requirement for many
machine learning estimators. Typically this is done by removing the mean
and scaling to unit variance. However, outliers can often influence the
sample mean / variance in a negative way. In such cases, the median and
the interquartile range often give better results.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17.</span></p>
</div>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>with_centering <span class="classifier-delimiter">:</span> <span class="classifier">boolean, True by default</span></dt>
<dd>If True, center the data before scaling.
This will cause <code class="docutils literal"><span class="pre">transform</span></code> to raise an exception when attempted on
sparse matrices, because centering them entails building a dense
matrix which in common use cases is likely to be too large to fit in
memory.</dd>
<dt>with_scaling <span class="classifier-delimiter">:</span> <span class="classifier">boolean, True by default</span></dt>
<dd>If True, scale the data to interquartile range.</dd>
<dt>quantile_range <span class="classifier-delimiter">:</span> <span class="classifier">tuple (q_min, q_max), 0.0 &lt; q_min &lt; q_max &lt; 100.0</span></dt>
<dd><p class="first">Default: (25.0, 75.0) = (1st quantile, 3rd quantile) = IQR
Quantile range used to calculate <code class="docutils literal"><span class="pre">scale_</span></code>.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.18.</span></p>
</div>
</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default is True</span></dt>
<dd>If False, try to avoid a copy and do inplace scaling instead.
This is not guaranteed to always work inplace; e.g. if the data is
not a NumPy array or scipy.sparse CSR matrix, a copy may still be
returned.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">center_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of floats</span></dt>
<dd>The median value for each feature in the training set.</dd>
<dt><code class="docutils literal"><span class="pre">scale_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of floats</span></dt>
<dd><p class="first">The (scaled) interquartile range for each feature in the training set.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>scale_</em> attribute.</p>
</div>
</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">RobustScaler</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span> <span class="o">-</span><span class="mf">2.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span> <span class="mf">4.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span> <span class="o">=</span> <span class="n">RobustScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span>
<span class="go">RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,</span>
<span class="go">       with_scaling=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([[ 0. , -2. ,  0. ],</span>
<span class="go">       [-1. ,  0. ,  0.4],</span>
<span class="go">       [ 1. ,  0. , -1.6]])</span>
</pre></div>
</div>
<p>See also</p>
<p>robust_scale: Equivalent function without the estimator API.</p>
<dl class="docutils">
<dt><code class="xref py py-class docutils literal"><span class="pre">sklearn.decomposition.PCA</span></code></dt>
<dd>Further removes the linear correlation across features with
‘whiten=True’.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>For a comparison of the different scalers, transformers, and normalizers,
see <span class="xref std std-ref">examples/preprocessing/plot_all_scaling.py</span>.</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Median">https://en.wikipedia.org/wiki/Median</a>
<a class="reference external" href="https://en.wikipedia.org/wiki/Interquartile_range">https://en.wikipedia.org/wiki/Interquartile_range</a></p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#RobustScalerScikitsLearnNode">RobustScalerScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.BaggingClassifierScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">BaggingClassifierScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.BaggingClassifierScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>A Bagging classifier.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.ensemble.bagging.BaggingClassifier</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
A Bagging classifier is an ensemble meta-estimator that fits base
classifiers each on random subsets of the original dataset and then
aggregate their individual predictions (either by voting or by averaging)
to form a final prediction. Such a meta-estimator can typically be used as
a way to reduce the variance of a black-box estimator (e.g., a decision
tree), by introducing randomization into its construction procedure and
then making an ensemble out of it.</p>
<p>This algorithm encompasses several works from the literature. When random
subsets of the dataset are drawn as random subsets of the samples, then
this algorithm is known as Pasting <a href="#id92"><span class="problematic" id="id60">[1]_</span></a>. If samples are drawn with
replacement, then the method is known as Bagging <a href="#id93"><span class="problematic" id="id61">[2]_</span></a>. When random subsets
of the dataset are drawn as random subsets of the features, then the method
is known as Random Subspaces <a href="#id94"><span class="problematic" id="id62">[3]_</span></a>. Finally, when base estimators are built
on subsets of both samples and features, then the method is known as
Random Patches <a href="#id95"><span class="problematic" id="id63">[4]_</span></a>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>base_estimator <span class="classifier-delimiter">:</span> <span class="classifier">object or None, optional (default=None)</span></dt>
<dd>The base estimator to fit on random subsets of the dataset.
If None, then the base estimator is a decision tree.</dd>
<dt>n_estimators <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=10)</span></dt>
<dd>The number of base estimators in the ensemble.</dd>
<dt>max_samples <span class="classifier-delimiter">:</span> <span class="classifier">int or float, optional (default=1.0)</span></dt>
<dd><p class="first">The number of samples to draw from X to train each base estimator.</p>
<ul class="last simple">
<li>If int, then draw <cite>max_samples</cite> samples.</li>
<li>If float, then draw <cite>max_samples * X.shape[0]</cite> samples.</li>
</ul>
</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int or float, optional (default=1.0)</span></dt>
<dd><p class="first">The number of features to draw from X to train each base estimator.</p>
<ul class="last simple">
<li>If int, then draw <cite>max_features</cite> features.</li>
<li>If float, then draw <cite>max_features * X.shape[1]</cite> features.</li>
</ul>
</dd>
<dt>bootstrap <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span></dt>
<dd>Whether samples are drawn with replacement. If False, sampling
without replacement is performed.</dd>
<dt>bootstrap_features <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=False)</span></dt>
<dd>Whether features are drawn with replacement.</dd>
<dt>oob_score <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default=False)</span></dt>
<dd>Whether to use out-of-bag samples to estimate
the generalization error.</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default=False)</span></dt>
<dd><p class="first">When set to True, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit
a whole new ensemble. See <span class="xref std std-term">the Glossary</span>.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>warm_start</em> constructor parameter.</p>
</div>
</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The number of jobs to run in parallel for both <cite>fit</cite> and <cite>predict</cite>.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=0)</span></dt>
<dd>Controls the verbosity when fitting and predicting.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">base_estimator_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">estimator</span></dt>
<dd>The base estimator from which the ensemble is grown.</dd>
<dt><code class="docutils literal"><span class="pre">estimators_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list of estimators</span></dt>
<dd>The collection of fitted base estimators.</dd>
<dt><code class="docutils literal"><span class="pre">estimators_samples_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list of arrays</span></dt>
<dd>The subset of drawn samples (i.e., the in-bag samples) for each base
estimator. Each subset is defined by an array of the indices selected.</dd>
<dt><code class="docutils literal"><span class="pre">estimators_features_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list of arrays</span></dt>
<dd>The subset of drawn features for each base estimator.</dd>
<dt><code class="docutils literal"><span class="pre">classes_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_classes]</span></dt>
<dd>The classes labels.</dd>
<dt><code class="docutils literal"><span class="pre">n_classes_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int or list</span></dt>
<dd>The number of classes.</dd>
<dt><code class="docutils literal"><span class="pre">oob_score_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Score of the training dataset obtained using an out-of-bag estimate.</dd>
<dt><code class="docutils literal"><span class="pre">oob_decision_function_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_samples, n_classes]</span></dt>
<dd>Decision function computed with out-of-bag estimate on the training
set. If n_estimators is small it might be possible that a data point
was never left out during the bootstrap. In this case,
<cite>oob_decision_function_</cite> might contain NaN.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id64" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>L. Breiman, “Pasting small votes for classification in large
databases and on-line”, Machine Learning, 36(1), 85-103, 1999.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id65" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>L. Breiman, “Bagging predictors”, Machine Learning, 24(2), 123-140,
1996.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id66" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td>T. Ho, “The random subspace method for constructing decision
forests”, Pattern Analysis and Machine Intelligence, 20(8), 832-844,
1998.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id67" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td>G. Louppe and P. Geurts, “Ensembles on Random Patches”, Machine
Learning and Knowledge Discovery in Databases, 346-361, 2012.</td></tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#BaggingClassifierScikitsLearnNode">BaggingClassifierScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.DecisionTreeRegressorScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">DecisionTreeRegressorScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.DecisionTreeRegressorScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>A decision tree regressor.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.tree.tree.DecisionTreeRegressor</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>criterion <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=”mse”)</span></dt>
<dd><p class="first">The function to measure the quality of a split. Supported criteria
are “mse” for the mean squared error, which is equal to variance
reduction as feature selection criterion and minimizes the L2 loss
using the mean of each terminal node, “friedman_mse”, which uses mean
squared error with Friedman’s improvement score for potential splits,
and “mae” for the mean absolute error, which minimizes the L1 loss
using the median of each terminal node.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.18: </span>Mean Absolute Error (MAE) criterion.</p>
</div>
</dd>
<dt>splitter <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=”best”)</span></dt>
<dd>The strategy used to choose the split at each node. Supported
strategies are “best” to choose the best split and “random” to choose
the best random split.</dd>
<dt>max_depth <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</dd>
<dt>min_samples_split <span class="classifier-delimiter">:</span> <span class="classifier">int, float, optional (default=2)</span></dt>
<dd><p class="first">The minimum number of samples required to split an internal node:</p>
<ul class="simple">
<li>If int, then consider <cite>min_samples_split</cite> as the minimum number.</li>
<li>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</li>
</ul>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</dd>
<dt>min_samples_leaf <span class="classifier-delimiter">:</span> <span class="classifier">int, float, optional (default=1)</span></dt>
<dd><p class="first">The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul class="simple">
<li>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</li>
<li>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</li>
</ul>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</dd>
<dt>min_weight_fraction_leaf <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span></dt>
<dd>The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int, float, string or None, optional (default=None)</span></dt>
<dd><p class="first">The number of features to consider when looking for the best split:</p>
<ul class="simple">
<li>If int, then consider <cite>max_features</cite> features at each split.</li>
<li>If float, then <cite>max_features</cite> is a fraction and
<cite>int(max_features * n_features)</cite> features are considered at each
split.</li>
<li>If “auto”, then <cite>max_features=n_features</cite>.</li>
<li>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite>.</li>
<li>If “log2”, then <cite>max_features=log2(n_features)</cite>.</li>
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
<p class="last">Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal"><span class="pre">max_features</span></code> features.</p>
</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>max_leaf_nodes <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Grow a tree with <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</dd>
<dt>min_impurity_decrease <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span></dt>
<dd><p class="first">A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal"><span class="pre">N</span></code>, <code class="docutils literal"><span class="pre">N_t</span></code>, <code class="docutils literal"><span class="pre">N_t_R</span></code> and <code class="docutils literal"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.19.</span></p>
</div>
</dd>
<dt>min_impurity_split <span class="classifier-delimiter">:</span> <span class="classifier">float, (default=1e-7)</span></dt>
<dd><p class="first">Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="last deprecated">
<p><span class="versionmodified">Deprecated since version 0.19: </span><code class="docutils literal"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal"><span class="pre">min_impurity_split</span></code> will change from 1e-7 to 0 in 0.23 and it
will be removed in 0.25. Use <code class="docutils literal"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</dd>
<dt>presort <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default=False)</span></dt>
<dd>Whether to presort the data to speed up the finding of best splits in
fitting. For the default settings of a decision tree on large
datasets, setting this to true may slow down the training process.
When using either a smaller dataset or a restricted depth, this may
speed up the training.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">feature_importances_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span></dt>
<dd>The feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the
(normalized) total reduction of the criterion brought
by that feature. It is also known as the Gini importance <a href="#id96"><span class="problematic" id="id68">[4]_</span></a>.</dd>
<dt><code class="docutils literal"><span class="pre">max_features_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>The inferred value of max_features.</dd>
<dt><code class="docutils literal"><span class="pre">n_features_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The number of features when <code class="docutils literal"><span class="pre">fit</span></code> is performed.</dd>
<dt><code class="docutils literal"><span class="pre">n_outputs_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The number of outputs when <code class="docutils literal"><span class="pre">fit</span></code> is performed.</dd>
<dt><code class="docutils literal"><span class="pre">tree_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">Tree object</span></dt>
<dd>The underlying Tree object. Please refer to
<code class="docutils literal"><span class="pre">help(sklearn.tree._tree.Tree)</span></code> for attributes of Tree object and
<span class="xref std std-ref">sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py</span>
for basic usage of these attributes.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code class="docutils literal"><span class="pre">max_depth</span></code>, <code class="docutils literal"><span class="pre">min_samples_leaf</span></code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<p>The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data and
<code class="docutils literal"><span class="pre">max_features=n_features</span></code>, if the improvement of the criterion is
identical for several splits enumerated during the search of the best
split. To obtain a deterministic behaviour during fitting,
<code class="docutils literal"><span class="pre">random_state</span></code> has to be fixed.</p>
<p>See also</p>
<p>DecisionTreeClassifier</p>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id69" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><a class="reference external" href="https://en.wikipedia.org/wiki/Decision_tree_learning">https://en.wikipedia.org/wiki/Decision_tree_learning</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id70" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>L. Breiman, J. Friedman, R. Olshen, and C. Stone, “Classification
and Regression Trees”, Wadsworth, Belmont, CA, 1984.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id71" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td>T. Hastie, R. Tibshirani and J. Friedman. “Elements of Statistical
Learning”, Springer, 2009.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id72" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td>L. Breiman, and A. Cutler, “Random Forests”,
<a class="reference external" href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm">https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm</a></td></tr>
</tbody>
</table>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_boston</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">boston</span> <span class="o">=</span> <span class="n">load_boston</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regressor</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">regressor</span><span class="p">,</span> <span class="n">boston</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">boston</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">... </span>                   
<span class="gp">...</span>
<span class="go">array([ 0.61..., 0.57..., -0.34..., 0.41..., 0.75...,</span>
<span class="go">        0.07..., 0.29..., 0.33..., -1.42..., -1.77...])</span>
</pre></div>
</div>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#DecisionTreeRegressorScikitsLearnNode">DecisionTreeRegressorScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.LarsScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">LarsScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.LarsScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Least Angle Regression model a.k.a. LAR
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.least_angle.Lars</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">boolean or integer, optional</span></dt>
<dd>Sets the verbosity amount</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>This parameter is ignored when <code class="docutils literal"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<code class="xref py py-class docutils literal"><span class="pre">sklearn.preprocessing.StandardScaler</span></code> before calling <code class="docutils literal"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal"><span class="pre">normalize=False</span></code>.</dd>
<dt>precompute <span class="classifier-delimiter">:</span> <span class="classifier">True | False | ‘auto’ | array-like</span></dt>
<dd>Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal"><span class="pre">'auto'</span></code> let us decide. The Gram
matrix can also be passed as argument.</dd>
<dt>n_nonzero_coefs <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Target number of non-zero coefficients. Use <code class="docutils literal"><span class="pre">np.inf</span></code> for no limit.</dd>
<dt>eps <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The machine-precision regularization in the computation of the
Cholesky diagonal factors. Increase this for very ill-conditioned
systems. Unlike the <code class="docutils literal"><span class="pre">tol</span></code> parameter in some iterative
optimization-based algorithms, this parameter does not control
the tolerance of the optimization.</dd>
<dt>copy_X <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>If <code class="docutils literal"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</dd>
<dt>fit_path <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>If True the full path is stored in the <code class="docutils literal"><span class="pre">coef_path_</span></code> attribute.
If you compute the solution for a large problem or many targets,
setting <code class="docutils literal"><span class="pre">fit_path</span></code> to <code class="docutils literal"><span class="pre">False</span></code> will lead to a speedup, especially
with a small alpha.</dd>
<dt>positive <span class="classifier-delimiter">:</span> <span class="classifier">boolean (default=False)</span></dt>
<dd><p class="first">Restrict coefficients to be &gt;= 0. Be aware that you might want to
remove fit_intercept which is set True by default.</p>
<div class="last deprecated">
<p><span class="versionmodified">Deprecated since version 0.20: </span>The option is broken and deprecated. It will be removed in v0.22.</p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">alphas_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_alphas + 1,) | list of n_targets such arrays</span></dt>
<dd>Maximum of covariances (in absolute value) at each iteration.         <code class="docutils literal"><span class="pre">n_alphas</span></code> is either <code class="docutils literal"><span class="pre">n_nonzero_coefs</span></code> or <code class="docutils literal"><span class="pre">n_features</span></code>,         whichever is smaller.</dd>
<dt><code class="docutils literal"><span class="pre">active_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list, length = n_alphas | list of n_targets such lists</span></dt>
<dd>Indices of active variables at the end of the path.</dd>
<dt><code class="docutils literal"><span class="pre">coef_path_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features, n_alphas + 1)         | list of n_targets such arrays</span></dt>
<dd>The varying values of the coefficients along the path. It is not
present if the <code class="docutils literal"><span class="pre">fit_path</span></code> parameter is <code class="docutils literal"><span class="pre">False</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,) or (n_targets, n_features)</span></dt>
<dd>Parameter vector (w in the formulation formula).</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float | array, shape (n_targets,)</span></dt>
<dd>Independent term in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like or int</span></dt>
<dd>The number of iterations taken by lars_path to find the
grid of alphas for each target.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">Lars</span><span class="p">(</span><span class="n">n_nonzero_coefs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.1111</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1111</span><span class="p">])</span>
<span class="gp">... </span>
<span class="go">Lars(copy_X=True, eps=..., fit_intercept=True, fit_path=True,</span>
<span class="go">   n_nonzero_coefs=1, normalize=True, positive=False, precompute=&#39;auto&#39;,</span>
<span class="go">   verbose=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span> 
<span class="go">[ 0. -1.11...]</span>
</pre></div>
</div>
<p>See also</p>
<p>lars_path, LarsCV
sklearn.decomposition.sparse_encode</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#LarsScikitsLearnNode">LarsScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.BaggingRegressorScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">BaggingRegressorScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.BaggingRegressorScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>A Bagging regressor.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.ensemble.bagging.BaggingRegressor</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
A Bagging regressor is an ensemble meta-estimator that fits base
regressors each on random subsets of the original dataset and then
aggregate their individual predictions (either by voting or by averaging)
to form a final prediction. Such a meta-estimator can typically be used as
a way to reduce the variance of a black-box estimator (e.g., a decision
tree), by introducing randomization into its construction procedure and
then making an ensemble out of it.</p>
<p>This algorithm encompasses several works from the literature. When random
subsets of the dataset are drawn as random subsets of the samples, then
this algorithm is known as Pasting <a href="#id97"><span class="problematic" id="id73">[1]_</span></a>. If samples are drawn with
replacement, then the method is known as Bagging <a href="#id98"><span class="problematic" id="id74">[2]_</span></a>. When random subsets
of the dataset are drawn as random subsets of the features, then the method
is known as Random Subspaces <a href="#id99"><span class="problematic" id="id75">[3]_</span></a>. Finally, when base estimators are built
on subsets of both samples and features, then the method is known as
Random Patches <a href="#id100"><span class="problematic" id="id76">[4]_</span></a>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>base_estimator <span class="classifier-delimiter">:</span> <span class="classifier">object or None, optional (default=None)</span></dt>
<dd>The base estimator to fit on random subsets of the dataset.
If None, then the base estimator is a decision tree.</dd>
<dt>n_estimators <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=10)</span></dt>
<dd>The number of base estimators in the ensemble.</dd>
<dt>max_samples <span class="classifier-delimiter">:</span> <span class="classifier">int or float, optional (default=1.0)</span></dt>
<dd><p class="first">The number of samples to draw from X to train each base estimator.</p>
<ul class="last simple">
<li>If int, then draw <cite>max_samples</cite> samples.</li>
<li>If float, then draw <cite>max_samples * X.shape[0]</cite> samples.</li>
</ul>
</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int or float, optional (default=1.0)</span></dt>
<dd><p class="first">The number of features to draw from X to train each base estimator.</p>
<ul class="last simple">
<li>If int, then draw <cite>max_features</cite> features.</li>
<li>If float, then draw <cite>max_features * X.shape[1]</cite> features.</li>
</ul>
</dd>
<dt>bootstrap <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span></dt>
<dd>Whether samples are drawn with replacement. If False, sampling
without replacement is performed.</dd>
<dt>bootstrap_features <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=False)</span></dt>
<dd>Whether features are drawn with replacement.</dd>
<dt>oob_score <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Whether to use out-of-bag samples to estimate
the generalization error.</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default=False)</span></dt>
<dd>When set to True, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit
a whole new ensemble. See <span class="xref std std-term">the Glossary</span>.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The number of jobs to run in parallel for both <cite>fit</cite> and <cite>predict</cite>.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=0)</span></dt>
<dd>Controls the verbosity when fitting and predicting.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">estimators_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list of estimators</span></dt>
<dd>The collection of fitted sub-estimators.</dd>
<dt><code class="docutils literal"><span class="pre">estimators_samples_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list of arrays</span></dt>
<dd>The subset of drawn samples (i.e., the in-bag samples) for each base
estimator. Each subset is defined by an array of the indices selected.</dd>
<dt><code class="docutils literal"><span class="pre">estimators_features_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list of arrays</span></dt>
<dd>The subset of drawn features for each base estimator.</dd>
<dt><code class="docutils literal"><span class="pre">oob_score_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Score of the training dataset obtained using an out-of-bag estimate.</dd>
<dt><code class="docutils literal"><span class="pre">oob_prediction_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_samples]</span></dt>
<dd>Prediction computed with out-of-bag estimate on the training
set. If n_estimators is small it might be possible that a data point
was never left out during the bootstrap. In this case,
<cite>oob_prediction_</cite> might contain NaN.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id77" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>L. Breiman, “Pasting small votes for classification in large
databases and on-line”, Machine Learning, 36(1), 85-103, 1999.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id78" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>L. Breiman, “Bagging predictors”, Machine Learning, 24(2), 123-140,
1996.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id79" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td>T. Ho, “The random subspace method for constructing decision
forests”, Pattern Analysis and Machine Intelligence, 20(8), 832-844,
1998.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id80" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td>G. Louppe and P. Geurts, “Ensembles on Random Patches”, Machine
Learning and Knowledge Discovery in Databases, 346-361, 2012.</td></tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#BaggingRegressorScikitsLearnNode">BaggingRegressorScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.SVRScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">SVRScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.SVRScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Epsilon-Support Vector Regression.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.svm.classes.SVR</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
The free parameters in the model are C and epsilon.</p>
<p>The implementation is based on libsvm.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>kernel <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=’rbf’)</span></dt>
<dd>Specifies the kernel type to be used in the algorithm.
It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or
a callable.
If none is given, ‘rbf’ will be used. If a callable is given it is
used to precompute the kernel matrix.</dd>
<dt>degree <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=3)</span></dt>
<dd>Degree of the polynomial kernel function (‘poly’).
Ignored by all other kernels.</dd>
<dt>gamma <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=’auto’)</span></dt>
<dd><p class="first">Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.</p>
<p class="last">Current default is ‘auto’ which uses 1 / n_features,
if <code class="docutils literal"><span class="pre">gamma='scale'</span></code> is passed then it uses 1 / (n_features * X.var())
as value of gamma. The current default of gamma, ‘auto’, will change
to ‘scale’ in version 0.22. ‘auto_deprecated’, a deprecated version of
‘auto’ is used as a default indicating that no explicit value of gamma
was passed.</p>
</dd>
<dt>coef0 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.0)</span></dt>
<dd>Independent term in kernel function.
It is only significant in ‘poly’ and ‘sigmoid’.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1e-3)</span></dt>
<dd>Tolerance for stopping criterion.</dd>
<dt>C <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span></dt>
<dd>Penalty parameter C of the error term.</dd>
<dt>epsilon <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.1)</span></dt>
<dd>Epsilon in the epsilon-SVR model. It specifies the epsilon-tube
within which no penalty is associated in the training loss function
with points predicted within a distance epsilon from the actual
value.</dd>
<dt>shrinking <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span></dt>
<dd>Whether to use the shrinking heuristic.</dd>
<dt>cache_size <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Specify the size of the kernel cache (in MB).</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">bool, default: False</span></dt>
<dd>Enable verbose output. Note that this setting takes advantage of a
per-process runtime setting in libsvm that, if enabled, may not work
properly in a multithreaded context.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=-1)</span></dt>
<dd>Hard limit on iterations within solver, or -1 for no limit.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">support_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_SV]</span></dt>
<dd>Indices of support vectors.</dd>
<dt><code class="docutils literal"><span class="pre">support_vectors_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [nSV, n_features]</span></dt>
<dd>Support vectors.</dd>
<dt><code class="docutils literal"><span class="pre">dual_coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1, n_SV]</span></dt>
<dd>Coefficients of the support vector in the decision function.</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1, n_features]</span></dt>
<dd><p class="first">Weights assigned to the features (coefficients in the primal
problem). This is only available in the case of a linear kernel.</p>
<p class="last"><cite>coef_</cite> is readonly property derived from <cite>dual_coef_</cite> and
<cite>support_vectors_</cite>.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1]</span></dt>
<dd>Constants in decision function.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVR</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">SVR</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.2, gamma=&#39;scale&#39;,</span>
<span class="go">    kernel=&#39;rbf&#39;, max_iter=-1, shrinking=True, tol=0.001, verbose=False)</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>NuSVR</dt>
<dd>Support Vector Machine for regression implemented using libsvm
using a parameter to control the number of support vectors.</dd>
<dt>LinearSVR</dt>
<dd>Scalable Linear Support Vector Machine for regression
implemented using liblinear.</dd>
</dl>
<p><strong>Notes</strong></p>
<p><strong>References:</strong>
<a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf">LIBSVM: A Library for Support Vector Machines</a></p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.SVRScikitsLearnNode-class.html">SVRScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.RFECVScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">RFECVScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.RFECVScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Feature ranking with recursive feature elimination and cross-validated
selection of the best number of features.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.feature_selection.rfe.RFECV</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
See glossary entry for <span class="xref std std-term">cross-validation estimator</span>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>estimator <span class="classifier-delimiter">:</span> <span class="classifier">object</span></dt>
<dd>A supervised learning estimator with a <code class="docutils literal"><span class="pre">fit</span></code> method that provides
information about feature importance either through a <code class="docutils literal"><span class="pre">coef_</span></code>
attribute or through a <code class="docutils literal"><span class="pre">feature_importances_</span></code> attribute.</dd>
<dt>step <span class="classifier-delimiter">:</span> <span class="classifier">int or float, optional (default=1)</span></dt>
<dd>If greater than or equal to 1, then <code class="docutils literal"><span class="pre">step</span></code> corresponds to the
(integer) number of features to remove at each iteration.
If within (0.0, 1.0), then <code class="docutils literal"><span class="pre">step</span></code> corresponds to the percentage
(rounded down) of features to remove at each iteration.
Note that the last iteration may remove fewer than <code class="docutils literal"><span class="pre">step</span></code> features in
order to reach <code class="docutils literal"><span class="pre">min_features_to_select</span></code>.</dd>
<dt>min_features_to_select <span class="classifier-delimiter">:</span> <span class="classifier">int, (default=1)</span></dt>
<dd>The minimum number of features to be selected. This number of features
will always be scored, even if the difference between the original
feature count and <code class="docutils literal"><span class="pre">min_features_to_select</span></code> isn’t divisible by
<code class="docutils literal"><span class="pre">step</span></code>.</dd>
<dt>cv <span class="classifier-delimiter">:</span> <span class="classifier">int, cross-validation generator or an iterable, optional</span></dt>
<dd><p class="first">Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul class="simple">
<li>None, to use the default 3-fold cross-validation,</li>
<li>integer, to specify the number of folds.</li>
<li><span class="xref std std-term">CV splitter</span>,</li>
<li>An iterable yielding (train, test) splits as arrays of indices.</li>
</ul>
<p>For integer/None inputs, if <code class="docutils literal"><span class="pre">y</span></code> is binary or multiclass,
<code class="xref py py-class docutils literal"><span class="pre">sklearn.model_selection.StratifiedKFold</span></code> is used. If the
estimator is a classifier or if <code class="docutils literal"><span class="pre">y</span></code> is neither binary nor multiclass,
<code class="xref py py-class docutils literal"><span class="pre">sklearn.model_selection.KFold</span></code> is used.</p>
<p>Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.20: </span><code class="docutils literal"><span class="pre">cv</span></code> default value of None will change from 3-fold to 5-fold
in v0.22.</p>
</div>
</dd>
<dt>scoring <span class="classifier-delimiter">:</span> <span class="classifier">string, callable or None, optional, (default=None)</span></dt>
<dd>A string (see model evaluation documentation) or
a scorer callable object / function with signature
<code class="docutils literal"><span class="pre">scorer(estimator,</span> <span class="pre">X,</span> <span class="pre">y)</span></code>.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, (default=0)</span></dt>
<dd>Controls verbosity of output.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Number of cores to run in parallel while fitting across folds.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">n_features_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The number of selected features with cross-validation.</dd>
<dt><code class="docutils literal"><span class="pre">support_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape [n_features]</span></dt>
<dd>The mask of selected features.</dd>
<dt><code class="docutils literal"><span class="pre">ranking_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape [n_features]</span></dt>
<dd>The feature ranking, such that <cite>ranking_[i]</cite>
corresponds to the ranking
position of the i-th feature.
Selected (i.e., estimated best)
features are assigned rank 1.</dd>
<dt><code class="docutils literal"><span class="pre">grid_scores_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape [n_subsets_of_features]</span></dt>
<dd>The cross-validation scores such that
<code class="docutils literal"><span class="pre">grid_scores_[i]</span></code> corresponds to
the CV score of the i-th subset of features.</dd>
<dt><code class="docutils literal"><span class="pre">estimator_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">object</span></dt>
<dd>The external estimator fit on the reduced dataset.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>The size of <code class="docutils literal"><span class="pre">grid_scores_</span></code> is equal to
<code class="docutils literal"><span class="pre">ceil((n_features</span> <span class="pre">-</span> <span class="pre">min_features_to_select)</span> <span class="pre">/</span> <span class="pre">step)</span> <span class="pre">+</span> <span class="pre">1</span></code>,
where step is the number of features removed at each iteration.</p>
<p><strong>Examples</strong></p>
<p>The following example shows how to retrieve the a-priori not known 5
informative features in the Friedman #1 dataset.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_friedman1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">RFECV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVR</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_friedman1</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">estimator</span> <span class="o">=</span> <span class="n">SVR</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span> <span class="o">=</span> <span class="n">RFECV</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span> <span class="o">=</span> <span class="n">selector</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span><span class="o">.</span><span class="n">support_</span> 
<span class="go">array([ True,  True,  True,  True,  True, False, False, False, False,</span>
<span class="go">       False])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span><span class="o">.</span><span class="n">ranking_</span>
<span class="go">array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])</span>
</pre></div>
</div>
<p>See also</p>
<p>RFE : Recursive feature elimination</p>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id81" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Guyon, I., Weston, J., Barnhill, S., &amp; Vapnik, V., “Gene selection
for cancer classification using support vector machines”,
Mach. Learn., 46(1-3), 389–422, 2002.</td></tr>
</tbody>
</table>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.RFECVScikitsLearnNode-class.html">RFECVScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.BayesianRidgeScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">BayesianRidgeScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.BayesianRidgeScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bayesian ridge regression
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.bayes.BayesianRidge</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Fit a Bayesian ridge model and optimize the regularization parameters
lambda (precision of the weights) and alpha (precision of the noise).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Maximum number of iterations.  Default is 300.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Stop the algorithm if w has converged. Default is 1.e-3.</dd>
<dt>alpha_1 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Hyper-parameter : shape parameter for the Gamma distribution prior
over the alpha parameter. Default is 1.e-6</dd>
<dt>alpha_2 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Hyper-parameter : inverse scale parameter (rate parameter) for the
Gamma distribution prior over the alpha parameter.
Default is 1.e-6.</dd>
<dt>lambda_1 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Hyper-parameter : shape parameter for the Gamma distribution prior
over the lambda parameter. Default is 1.e-6.</dd>
<dt>lambda_2 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Hyper-parameter : inverse scale parameter (rate parameter) for the
Gamma distribution prior over the lambda parameter.
Default is 1.e-6</dd>
<dt>compute_score <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>If True, compute the objective function at each step of the model.
Default is False</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).
Default is True.</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span></dt>
<dd>This parameter is ignored when <code class="docutils literal"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<code class="xref py py-class docutils literal"><span class="pre">sklearn.preprocessing.StandardScaler</span></code> before calling <code class="docutils literal"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal"><span class="pre">normalize=False</span></code>.</dd>
<dt>copy_X <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>If True, X will be copied; else, it may be overwritten.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span></dt>
<dd>Verbose mode when fitting the model.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = (n_features)</span></dt>
<dd>Coefficients of the regression model (mean of distribution)</dd>
<dt><code class="docutils literal"><span class="pre">alpha_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>estimated precision of the noise.</dd>
<dt><code class="docutils literal"><span class="pre">lambda_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>estimated precision of the weights.</dd>
<dt><code class="docutils literal"><span class="pre">sigma_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = (n_features, n_features)</span></dt>
<dd>estimated variance-covariance matrix of the weights</dd>
<dt><code class="docutils literal"><span class="pre">scores_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>if computed, value of the objective function (to be maximized)</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">BayesianRidge</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">... </span>
<span class="go">BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False,</span>
<span class="go">        copy_X=True, fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06,</span>
<span class="go">        n_iter=300, normalize=False, tol=0.001, verbose=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="go">array([1.])</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>For an example, see <span class="xref std std-ref">examples/linear_model/plot_bayesian_ridge.py</span>.</p>
<p><strong>References</strong></p>
<p>D. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems,
Vol. 4, No. 3, 1992.</p>
<p>R. Salakhutdinov, Lecture notes on Statistical Machine Learning,
<a class="reference external" href="http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15">http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15</a>
Their beta is our <code class="docutils literal"><span class="pre">self.alpha_</span></code>
Their alpha is our <code class="docutils literal"><span class="pre">self.lambda_</span></code></p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.BayesianRidgeScikitsLearnNode-class.html">BayesianRidgeScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.MultiTaskElasticNetCVScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">MultiTaskElasticNetCVScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.MultiTaskElasticNetCVScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Multi-task L1/L2 ElasticNet with built-in cross-validation.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.coordinate_descent.MultiTaskElasticNetCV</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
See glossary entry for <span class="xref std std-term">cross-validation estimator</span>.</p>
<p>The optimization objective for MultiTaskElasticNet is:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">Y</span> <span class="o">-</span> <span class="n">XW</span><span class="o">||^</span><span class="n">Fro_2</span>
<span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span>
<span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span>
</pre></div>
</div>
<p>Where:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span> <span class="o">=</span> \<span class="n">sum_i</span> \<span class="n">sqrt</span><span class="p">{</span>\<span class="n">sum_j</span> <span class="n">w_</span><span class="p">{</span><span class="n">ij</span><span class="p">}</span><span class="o">^</span><span class="mi">2</span><span class="p">}</span>
</pre></div>
</div>
<p>i.e. the sum of norm of each row.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>l1_ratio <span class="classifier-delimiter">:</span> <span class="classifier">float or array of floats</span></dt>
<dd>The ElasticNet mixing parameter, with 0 &lt; l1_ratio &lt;= 1.
For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it
is an L2 penalty.
For <code class="docutils literal"><span class="pre">0</span> <span class="pre">&lt;</span> <span class="pre">l1_ratio</span> <span class="pre">&lt;</span> <span class="pre">1</span></code>, the penalty is a combination of L1/L2 and L2.
This parameter can be a list, in which case the different
values are tested by cross-validation and the one giving the best
prediction score is used. Note that a good choice of list of
values for l1_ratio is often to put more values close to 1
(i.e. Lasso) and less close to 0 (i.e. Ridge), as in <code class="docutils literal"><span class="pre">[.1,</span> <span class="pre">.5,</span> <span class="pre">.7,</span>
<span class="pre">.9,</span> <span class="pre">.95,</span> <span class="pre">.99,</span> <span class="pre">1]</span></code></dd>
<dt>eps <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Length of the path. <code class="docutils literal"><span class="pre">eps=1e-3</span></code> means that
<code class="docutils literal"><span class="pre">alpha_min</span> <span class="pre">/</span> <span class="pre">alpha_max</span> <span class="pre">=</span> <span class="pre">1e-3</span></code>.</dd>
<dt>n_alphas <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Number of alphas along the regularization path</dd>
<dt>alphas <span class="classifier-delimiter">:</span> <span class="classifier">array-like, optional</span></dt>
<dd>List of alphas where to compute the models.
If not provided, set automatically.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span></dt>
<dd>This parameter is ignored when <code class="docutils literal"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<code class="xref py py-class docutils literal"><span class="pre">sklearn.preprocessing.StandardScaler</span></code> before calling <code class="docutils literal"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal"><span class="pre">normalize=False</span></code>.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>The maximum number of iterations</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The tolerance for the optimization: if the updates are
smaller than <code class="docutils literal"><span class="pre">tol</span></code>, the optimization code checks the
dual gap for optimality and continues until it is smaller
than <code class="docutils literal"><span class="pre">tol</span></code>.</dd>
<dt>cv <span class="classifier-delimiter">:</span> <span class="classifier">int, cross-validation generator or an iterable, optional</span></dt>
<dd><p class="first">Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul class="simple">
<li>None, to use the default 3-fold cross-validation,</li>
<li>integer, to specify the number of folds.</li>
<li><span class="xref std std-term">CV splitter</span>,</li>
<li>An iterable yielding (train, test) splits as arrays of indices.</li>
</ul>
<p>For integer/None inputs, <code class="xref py py-class docutils literal"><span class="pre">KFold</span></code> is used.</p>
<p>Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.20: </span><code class="docutils literal"><span class="pre">cv</span></code> default value if None will change from 3-fold to 5-fold
in v0.22.</p>
</div>
</dd>
<dt>copy_X <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>If <code class="docutils literal"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">bool or integer</span></dt>
<dd>Amount of verbosity.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>Number of CPUs to use during the cross validation. Note that this is
used only if multiple values for l1_ratio are given.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional, default None</span></dt>
<dd>The seed of the pseudo random number generator that selects a random
feature to update.  If int, random_state is the seed used by the random
number generator; If RandomState instance, random_state is the random
number generator; If None, the random number generator is the
RandomState instance used by <cite>np.random</cite>. Used when <code class="docutils literal"><span class="pre">selection</span></code> ==
‘random’.</dd>
<dt>selection <span class="classifier-delimiter">:</span> <span class="classifier">str, default ‘cyclic’</span></dt>
<dd>If set to ‘random’, a random coefficient is updated every iteration
rather than looping over features sequentially by default. This
(setting to ‘random’) often leads to significantly faster convergence
especially when tol is higher than 1e-4.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_tasks,)</span></dt>
<dd>Independent term in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_tasks, n_features)</span></dt>
<dd>Parameter vector (W in the cost function formula).
Note that <code class="docutils literal"><span class="pre">coef_</span></code> stores the transpose of <code class="docutils literal"><span class="pre">W</span></code>, <code class="docutils literal"><span class="pre">W.T</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">alpha_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The amount of penalization chosen by cross validation</dd>
<dt><code class="docutils literal"><span class="pre">mse_path_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_alphas, n_folds) or                 (n_l1_ratio, n_alphas, n_folds)</span></dt>
<dd>mean square error for the test set on each fold, varying alpha</dd>
<dt><code class="docutils literal"><span class="pre">alphas_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">numpy array, shape (n_alphas,) or (n_l1_ratio, n_alphas)</span></dt>
<dd>The grid of alphas used for fitting, for each l1_ratio</dd>
<dt><code class="docutils literal"><span class="pre">l1_ratio_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>best l1_ratio obtained by cross-validation.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>number of iterations run by the coordinate descent solver to reach
the specified tolerance for the optimal alpha.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">MultiTaskElasticNetCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span>
<span class="gp">... </span>        <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">... </span>
<span class="go">MultiTaskElasticNetCV(alphas=None, copy_X=True, cv=3, eps=0.001,</span>
<span class="go">       fit_intercept=True, l1_ratio=0.5, max_iter=1000, n_alphas=100,</span>
<span class="go">       n_jobs=None, normalize=False, random_state=None, selection=&#39;cyclic&#39;,</span>
<span class="go">       tol=0.0001, verbose=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">[[0.52875032 0.46958558]</span>
<span class="go"> [0.52875032 0.46958558]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="go">[0.00166409 0.00166409]</span>
</pre></div>
</div>
<p>See also</p>
<p>MultiTaskElasticNet
ElasticNetCV
MultiTaskLassoCV</p>
<p><strong>Notes</strong></p>
<p>The algorithm used to fit the model is coordinate descent.</p>
<p>To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#MultiTaskElasticNetCVScikitsLearnNode">MultiTaskElasticNetCVScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.PLSRegressionScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">PLSRegressionScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.PLSRegressionScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>PLS regression
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.cross_decomposition.pls_.PLSRegression</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
PLSRegression implements the PLS 2 blocks regression known as PLS2 or PLS1
in case of one dimensional response.
This class inherits from _PLS with mode=”A”, deflation_mode=”regression”,
norm_y_weights=False and algorithm=”nipals”.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int, (default 2)</span></dt>
<dd>Number of components to keep.</dd>
<dt>scale <span class="classifier-delimiter">:</span> <span class="classifier">boolean, (default True)</span></dt>
<dd>whether to scale the data</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">an integer, (default 500)</span></dt>
<dd>the maximum number of iterations of the NIPALS inner loop (used
only if algorithm=”nipals”)</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">non-negative real</span></dt>
<dd>Tolerance used in the iterative algorithm default 1e-06.</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default True</span></dt>
<dd>Whether the deflation should be done on a copy. Let the default
value to True unless you don’t care about side effect</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">x_weights_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [p, n_components]</span></dt>
<dd>X block weights vectors.</dd>
<dt><code class="docutils literal"><span class="pre">y_weights_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [q, n_components]</span></dt>
<dd>Y block weights vectors.</dd>
<dt><code class="docutils literal"><span class="pre">x_loadings_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [p, n_components]</span></dt>
<dd>X block loadings vectors.</dd>
<dt><code class="docutils literal"><span class="pre">y_loadings_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [q, n_components]</span></dt>
<dd>Y block loadings vectors.</dd>
<dt><code class="docutils literal"><span class="pre">x_scores_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_samples, n_components]</span></dt>
<dd>X scores.</dd>
<dt><code class="docutils literal"><span class="pre">y_scores_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_samples, n_components]</span></dt>
<dd>Y scores.</dd>
<dt><code class="docutils literal"><span class="pre">x_rotations_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [p, n_components]</span></dt>
<dd>X block to latents rotations.</dd>
<dt><code class="docutils literal"><span class="pre">y_rotations_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [q, n_components]</span></dt>
<dd>Y block to latents rotations.</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, [p, q]</span></dt>
<dd>The coefficients of the linear model: <code class="docutils literal"><span class="pre">Y</span> <span class="pre">=</span> <span class="pre">X</span> <span class="pre">``coef_</span></code> + Err``</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array-like</span></dt>
<dd>Number of iterations of the NIPALS inner loop for each
component.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>Matrices:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>T: ``x_scores_``
U: ``y_scores_``
W: ``x_weights_``
C: ``y_weights_``
P: ``x_loadings_``
Q: ``y_loadings__``
</pre></div>
</div>
<p>Are computed such that:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>X = T P.T + Err and Y = U Q.T + Err
T[:, k] = Xk W[:, k] for k in range(n_components)
U[:, k] = Yk C[:, k] for k in range(n_components)
``x_rotations_`` = W (P.T W)^(-1)
``y_rotations_`` = C (Q.T C)^(-1)
</pre></div>
</div>
<p>where Xk and Yk are residual matrices at iteration k.</p>
<p><a class="reference external" href="http://www.eigenvector.com/Docs/Wise_pls_properties.pdf">Slides explaining
PLS</a></p>
<p>For each component k, find weights u, v that optimizes:</p>
<p><code class="docutils literal"><span class="pre">max</span> <span class="pre">corr(Xk</span> <span class="pre">u,</span> <span class="pre">Yk</span> <span class="pre">v)</span> <span class="pre">*</span> <span class="pre">std(Xk</span> <span class="pre">u)</span> <span class="pre">std(Yk</span> <span class="pre">u)</span></code>, such that <code class="docutils literal"><span class="pre">|u|</span> <span class="pre">=</span> <span class="pre">1</span></code></p>
<p>Note that it maximizes both the correlations between the scores and the
intra-block variances.</p>
<p>The residual matrix of X (Xk+1) block is obtained by the deflation on
the current X score: x_score.</p>
<p>The residual matrix of Y (Yk+1) block is obtained by deflation on the
current X score. This performs the PLS regression known as PLS2. This
mode is prediction oriented.</p>
<p>This implementation provides the same results that 3 PLS packages
provided in the R language (R-project):</p>
<blockquote>
<div><ul class="simple">
<li>“mixOmics” with function pls(X, Y, mode = “regression”)</li>
<li>“plspm ” with function plsreg2(X, Y)</li>
<li>“pls” with function oscorespls.fit(X, Y)</li>
</ul>
</div></blockquote>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cross_decomposition</span> <span class="kn">import</span> <span class="n">PLSRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.</span><span class="p">,</span><span class="mf">5.</span><span class="p">,</span><span class="mf">4.</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.2</span><span class="p">,</span> <span class="mf">5.9</span><span class="p">],</span> <span class="p">[</span><span class="mf">11.9</span><span class="p">,</span> <span class="mf">12.3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pls2</span> <span class="o">=</span> <span class="n">PLSRegression</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pls2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">PLSRegression(copy=True, max_iter=500, n_components=2, scale=True,</span>
<span class="go">        tol=1e-06)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y_pred</span> <span class="o">=</span> <span class="n">pls2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with
emphasis on the two-block case. Technical Report 371, Department of
Statistics, University of Washington, Seattle, 2000.</p>
<p>In french but still a reference:</p>
<p>Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris:</p>
<p>Editions Technic.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.PLSRegressionScikitsLearnNode-class.html">PLSRegressionScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.FeatureHasherScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">FeatureHasherScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.FeatureHasherScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements feature hashing, aka the hashing trick.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.feature_extraction.hashing.FeatureHasher</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
This class turns sequences of symbolic feature names (strings) into
scipy.sparse matrices, using a hash function to compute the matrix column
corresponding to a name. The hash function employed is the signed 32-bit
version of Murmurhash3.</p>
<p>Feature names of type byte string are used as-is. Unicode strings are
converted to UTF-8 first, but no Unicode normalization is done.
Feature values must be (finite) numbers.</p>
<p>This class is a low-memory alternative to DictVectorizer and
CountVectorizer, intended for large-scale (online) learning and situations
where memory is tight, e.g. when running prediction code on embedded
devices.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_features <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span></dt>
<dd>The number of features (columns) in the output matrices. Small numbers
of features are likely to cause hash collisions, but large numbers
will cause larger coefficient dimensions in linear learners.</dd>
<dt>input_type <span class="classifier-delimiter">:</span> <span class="classifier">string, optional, default “dict”</span></dt>
<dd>Either “dict” (the default) to accept dictionaries over
(feature_name, value); “pair” to accept pairs of (feature_name, value);
or “string” to accept single strings.
feature_name should be a string, while value should be a number.
In the case of “string”, a value of 1 is implied.
The feature_name is hashed to find the appropriate column for the
feature. The value’s sign might be flipped in the output (but see
non_negative, below).</dd>
<dt>dtype <span class="classifier-delimiter">:</span> <span class="classifier">numpy type, optional, default np.float64</span></dt>
<dd>The type of feature values. Passed to scipy.sparse matrix constructors
as the dtype argument. Do not set this to bool, np.boolean or any
unsigned integer type.</dd>
<dt>alternate_sign <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>When True, an alternating sign is added to the features as to
approximately conserve the inner product in the hashed space even for
small n_features. This approach is similar to sparse random projection.</dd>
<dt>non_negative <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span></dt>
<dd><p class="first">When True, an absolute value is applied to the features matrix prior to
returning it. When used in conjunction with alternate_sign=True, this
significantly reduces the inner product preservation property.</p>
<div class="last deprecated">
<p><span class="versionmodified">Deprecated since version 0.19: </span>This option will be removed in 0.21.</p>
</div>
</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction</span> <span class="kn">import</span> <span class="n">FeatureHasher</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h</span> <span class="o">=</span> <span class="n">FeatureHasher</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">D</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;dog&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;cat&#39;</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;elephant&#39;</span><span class="p">:</span><span class="mi">4</span><span class="p">},{</span><span class="s1">&#39;dog&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;run&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">}]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="go">array([[ 0.,  0., -4., -1.,  0.,  0.,  0.,  0.,  0.,  2.],</span>
<span class="go">       [ 0.,  0.,  0., -2., -5.,  0.,  0.,  0.,  0.,  0.]])</span>
</pre></div>
</div>
<p>See also</p>
<p>DictVectorizer : vectorizes string-valued features using a hash table.
sklearn.preprocessing.OneHotEncoder : handles nominal/categorical features.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#FeatureHasherScikitsLearnNode">FeatureHasherScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.MLPRegressorScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">MLPRegressorScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.MLPRegressorScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Multi-layer Perceptron regressor.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.neural_network.multilayer_perceptron.MLPRegressor</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
This model optimizes the squared-loss using LBFGS or stochastic gradient
descent.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.18.</span></p>
</div>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>hidden_layer_sizes <span class="classifier-delimiter">:</span> <span class="classifier">tuple, length = n_layers - 2, default (100,)</span></dt>
<dd>The ith element represents the number of neurons in the ith
hidden layer.</dd>
<dt>activation <span class="classifier-delimiter">:</span> <span class="classifier">{‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default ‘relu’</span></dt>
<dd><p class="first">Activation function for the hidden layer.</p>
<ul class="last simple">
<li>‘identity’, no-op activation, useful to implement linear bottleneck,
returns f(x) = x</li>
<li>‘logistic’, the logistic sigmoid function,
returns f(x) = 1 / (1 + exp(-x)).</li>
<li>‘tanh’, the hyperbolic tan function,
returns f(x) = tanh(x).</li>
<li>‘relu’, the rectified linear unit function,
returns f(x) = max(0, x)</li>
</ul>
</dd>
<dt>solver <span class="classifier-delimiter">:</span> <span class="classifier">{‘lbfgs’, ‘sgd’, ‘adam’}, default ‘adam’</span></dt>
<dd><p class="first">The solver for weight optimization.</p>
<ul class="simple">
<li>‘lbfgs’ is an optimizer in the family of quasi-Newton methods.</li>
<li>‘sgd’ refers to stochastic gradient descent.</li>
<li>‘adam’ refers to a stochastic gradient-based optimizer proposed by
Kingma, Diederik, and Jimmy Ba</li>
</ul>
<p class="last">Note: The default solver ‘adam’ works pretty well on relatively
large datasets (with thousands of training samples or more) in terms of
both training time and validation score.
For small datasets, however, ‘lbfgs’ can converge faster and perform
better.</p>
</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default 0.0001</span></dt>
<dd>L2 penalty (regularization term) parameter.</dd>
<dt>batch_size <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default ‘auto’</span></dt>
<dd>Size of minibatches for stochastic optimizers.
If the solver is ‘lbfgs’, the classifier will not use minibatch.
When set to “auto”, <cite>batch_size=min(200, n_samples)</cite></dd>
<dt>learning_rate <span class="classifier-delimiter">:</span> <span class="classifier">{‘constant’, ‘invscaling’, ‘adaptive’}, default ‘constant’</span></dt>
<dd><p class="first">Learning rate schedule for weight updates.</p>
<ul class="simple">
<li>‘constant’ is a constant learning rate given by
‘learning_rate_init’.</li>
<li>‘invscaling’ gradually decreases the learning rate <code class="docutils literal"><span class="pre">learning_rate_</span></code>
at each time step ‘t’ using an inverse scaling exponent of ‘power_t’.
effective_learning_rate = learning_rate_init / pow(t, power_t)</li>
<li>‘adaptive’ keeps the learning rate constant to
‘learning_rate_init’ as long as training loss keeps decreasing.
Each time two consecutive epochs fail to decrease training loss by at
least tol, or fail to increase validation score by at least tol if
‘early_stopping’ is on, the current learning rate is divided by 5.</li>
</ul>
<p class="last">Only used when solver=’sgd’.</p>
</dd>
<dt>learning_rate_init <span class="classifier-delimiter">:</span> <span class="classifier">double, optional, default 0.001</span></dt>
<dd>The initial learning rate used. It controls the step-size
in updating the weights. Only used when solver=’sgd’ or ‘adam’.</dd>
<dt>power_t <span class="classifier-delimiter">:</span> <span class="classifier">double, optional, default 0.5</span></dt>
<dd>The exponent for inverse scaling learning rate.
It is used in updating effective learning rate when the learning_rate
is set to ‘invscaling’. Only used when solver=’sgd’.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default 200</span></dt>
<dd>Maximum number of iterations. The solver iterates until convergence
(determined by ‘tol’) or this number of iterations. For stochastic
solvers (‘sgd’, ‘adam’), note that this determines the number of epochs
(how many times each data point will be used), not the number of
gradient steps.</dd>
<dt>shuffle <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional, default True</span></dt>
<dd>Whether to shuffle samples in each iteration. Only used when
solver=’sgd’ or ‘adam’.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional, default None</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default 1e-4</span></dt>
<dd>Tolerance for the optimization. When the loss or score is not improving
by at least <code class="docutils literal"><span class="pre">tol</span></code> for <code class="docutils literal"><span class="pre">n_iter_no_change</span></code> consecutive iterations,
unless <code class="docutils literal"><span class="pre">learning_rate</span></code> is set to ‘adaptive’, convergence is
considered to be reached and training stops.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional, default False</span></dt>
<dd>Whether to print progress messages to stdout.</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional, default False</span></dt>
<dd>When set to True, reuse the solution of the previous
call to fit as initialization, otherwise, just erase the
previous solution. See <span class="xref std std-term">the Glossary</span>.</dd>
<dt>momentum <span class="classifier-delimiter">:</span> <span class="classifier">float, default 0.9</span></dt>
<dd>Momentum for gradient descent update.  Should be between 0 and 1. Only
used when solver=’sgd’.</dd>
<dt>nesterovs_momentum <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default True</span></dt>
<dd>Whether to use Nesterov’s momentum. Only used when solver=’sgd’ and
momentum &gt; 0.</dd>
<dt>early_stopping <span class="classifier-delimiter">:</span> <span class="classifier">bool, default False</span></dt>
<dd>Whether to use early stopping to terminate training when validation
score is not improving. If set to true, it will automatically set
aside 10% of training data as validation and terminate training when
validation score is not improving by at least <code class="docutils literal"><span class="pre">tol</span></code> for
<code class="docutils literal"><span class="pre">n_iter_no_change</span></code> consecutive epochs.
Only effective when solver=’sgd’ or ‘adam’</dd>
<dt>validation_fraction <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default 0.1</span></dt>
<dd>The proportion of training data to set aside as validation set for
early stopping. Must be between 0 and 1.
Only used if early_stopping is True</dd>
<dt>beta_1 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default 0.9</span></dt>
<dd>Exponential decay rate for estimates of first moment vector in adam,
should be in [0, 1). Only used when solver=’adam’</dd>
<dt>beta_2 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default 0.999</span></dt>
<dd>Exponential decay rate for estimates of second moment vector in adam,
should be in [0, 1). Only used when solver=’adam’</dd>
<dt>epsilon <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default 1e-8</span></dt>
<dd>Value for numerical stability in adam. Only used when solver=’adam’</dd>
<dt>n_iter_no_change <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default 10</span></dt>
<dd><p class="first">Maximum number of epochs to not meet <code class="docutils literal"><span class="pre">tol</span></code> improvement.
Only effective when solver=’sgd’ or ‘adam’</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.20.</span></p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">loss_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The current loss computed with the loss function.</dd>
<dt><code class="docutils literal"><span class="pre">coefs_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list, length n_layers - 1</span></dt>
<dd>The ith element in the list represents the weight matrix corresponding
to layer i.</dd>
<dt><code class="docutils literal"><span class="pre">intercepts_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list, length n_layers - 1</span></dt>
<dd>The ith element in the list represents the bias vector corresponding to
layer i + 1.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>The number of iterations the solver has ran.</dd>
<dt><code class="docutils literal"><span class="pre">n_layers_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of layers.</dd>
<dt><code class="docutils literal"><span class="pre">n_outputs_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of outputs.</dd>
<dt><code class="docutils literal"><span class="pre">out_activation_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">string</span></dt>
<dd>Name of the output activation function.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>MLPRegressor trains iteratively since at each time step
the partial derivatives of the loss function with respect to the model
parameters are computed to update the parameters.</p>
<p>It can also have a regularization term added to the loss function
that shrinks model parameters to prevent overfitting.</p>
<p>This implementation works with data represented as dense and sparse numpy
arrays of floating point values.</p>
<p><strong>References</strong></p>
<dl class="docutils">
<dt>Hinton, Geoffrey E.</dt>
<dd>“Connectionist learning procedures.” Artificial intelligence 40.1
(1989): 185-234.</dd>
<dt>Glorot, Xavier, and Yoshua Bengio. “Understanding the difficulty of</dt>
<dd>training deep feedforward neural networks.” International Conference
on Artificial Intelligence and Statistics. 2010.</dd>
<dt>He, Kaiming, et al. “Delving deep into rectifiers: Surpassing human-level</dt>
<dd>performance on imagenet classification.” arXiv preprint
arXiv:1502.01852 (2015).</dd>
<dt>Kingma, Diederik, and Jimmy Ba. “Adam: A method for stochastic</dt>
<dd>optimization.” arXiv preprint arXiv:1412.6980 (2014).</dd>
</dl>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#MLPRegressorScikitsLearnNode">MLPRegressorScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.LinearRegressionScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">LinearRegressionScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.LinearRegressionScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Ordinary least squares Linear Regression.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.base.LinearRegression</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
<strong>Parameters</strong></p>
<dl class="docutils">
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>whether to calculate the intercept for this model. If set
to False, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span></dt>
<dd>This parameter is ignored when <code class="docutils literal"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<code class="xref py py-class docutils literal"><span class="pre">sklearn.preprocessing.StandardScaler</span></code> before calling <code class="docutils literal"><span class="pre">fit</span></code> on
an estimator with <code class="docutils literal"><span class="pre">normalize=False</span></code>.</dd>
<dt>copy_X <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>If True, X will be copied; else, it may be overwritten.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The number of jobs to use for the computation. This will only provide
speedup for n_targets &gt; 1 and sufficient large problems.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features, ) or (n_targets, n_features)</span></dt>
<dd>Estimated coefficients for the linear regression problem.
If multiple targets are passed during the fit (y 2D), this
is a 2D array of shape (n_targets, n_features), while if only
one target is passed, this is a 1D array of length n_features.</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array</span></dt>
<dd>Independent term in the linear model.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># y = 1 * x_0 + 2 * x_1 + 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]))</span> <span class="o">+</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">1.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">coef_</span>
<span class="go">array([1., 2.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">intercept_</span> 
<span class="go">3.0000...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">]]))</span>
<span class="go">array([16.])</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>From the implementation point of view, this is just plain Ordinary
Least Squares (scipy.linalg.lstsq) wrapped as a predictor object.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.LinearRegressionScikitsLearnNode-class.html">LinearRegressionScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.LabelBinarizerScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">LabelBinarizerScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.LabelBinarizerScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Binarize labels in a one-vs-all fashion
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.preprocessing.label.LabelBinarizer</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Several regression and binary classification algorithms are
available in scikit-learn. A simple way to extend these algorithms
to the multi-class classification case is to use the so-called
one-vs-all scheme.</p>
<p>At learning time, this simply consists in learning one regressor
or binary classifier per class. In doing so, one needs to convert
multi-class labels to binary labels (belong or does not belong
to the class). LabelBinarizer makes this process easy with the
transform method.</p>
<p>At prediction time, one assigns the class for which the corresponding
model gave the greatest confidence. LabelBinarizer makes this easy
with the inverse_transform method.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>neg_label <span class="classifier-delimiter">:</span> <span class="classifier">int (default: 0)</span></dt>
<dd>Value with which negative labels must be encoded.</dd>
<dt>pos_label <span class="classifier-delimiter">:</span> <span class="classifier">int (default: 1)</span></dt>
<dd>Value with which positive labels must be encoded.</dd>
<dt>sparse_output <span class="classifier-delimiter">:</span> <span class="classifier">boolean (default: False)</span></dt>
<dd>True if the returned array from transform is desired to be in sparse
CSR format.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">classes_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape [n_class]</span></dt>
<dd>Holds the label for each class.</dd>
<dt><code class="docutils literal"><span class="pre">y_type_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">str,</span></dt>
<dd>Represents the type of the target data as evaluated by
utils.multiclass.type_of_target. Possible type are ‘continuous’,
‘continuous-multioutput’, ‘binary’, ‘multiclass’,
‘multiclass-multioutput’, ‘multilabel-indicator’, and ‘unknown’.</dd>
<dt><code class="docutils literal"><span class="pre">sparse_input_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">boolean,</span></dt>
<dd>True if the input data to transform is given as a sparse matrix, False
otherwise.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">LabelBinarizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="go">LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span><span class="o">.</span><span class="n">classes_</span>
<span class="go">array([1, 2, 4, 6])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="go">array([[1, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 1]])</span>
</pre></div>
</div>
<p>Binary targets transform to a column vector</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">LabelBinarizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">([</span><span class="s1">&#39;yes&#39;</span><span class="p">,</span> <span class="s1">&#39;no&#39;</span><span class="p">,</span> <span class="s1">&#39;no&#39;</span><span class="p">,</span> <span class="s1">&#39;yes&#39;</span><span class="p">])</span>
<span class="go">array([[1],</span>
<span class="go">       [0],</span>
<span class="go">       [0],</span>
<span class="go">       [1]])</span>
</pre></div>
</div>
<p>Passing a 2D matrix for multilabel classification</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>
<span class="go">LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span><span class="o">.</span><span class="n">classes_</span>
<span class="go">array([0, 1, 2])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">array([[1, 0, 0],</span>
<span class="go">       [0, 1, 0],</span>
<span class="go">       [0, 0, 1],</span>
<span class="go">       [0, 1, 0]])</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>label_binarize <span class="classifier-delimiter">:</span> <span class="classifier">function to perform the transform operation of</span></dt>
<dd>LabelBinarizer with fixed classes.</dd>
<dt>sklearn.preprocessing.OneHotEncoder <span class="classifier-delimiter">:</span> <span class="classifier">encode categorical features</span></dt>
<dd>using a one-hot aka one-of-K scheme.</dd>
</dl>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.LabelBinarizerScikitsLearnNode-class.html">LabelBinarizerScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.OneVsRestClassifierScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">OneVsRestClassifierScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.OneVsRestClassifierScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>One-vs-the-rest (OvR) multiclass/multilabel strategy
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.multiclass.OneVsRestClassifier</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Also known as one-vs-all, this strategy consists in fitting one classifier
per class. For each classifier, the class is fitted against all the other
classes. In addition to its computational efficiency (only <cite>n_classes</cite>
classifiers are needed), one advantage of this approach is its
interpretability. Since each class is represented by one and one classifier
only, it is possible to gain knowledge about the class by inspecting its
corresponding classifier. This is the most commonly used strategy for
multiclass classification and is a fair default choice.</p>
<p>This strategy can also be used for multilabel learning, where a classifier
is used to predict multiple labels for instance, by fitting on a 2-d matrix
in which cell [i, j] is 1 if sample i has label j and 0 otherwise.</p>
<p>In the multilabel learning literature, OvR is also known as the binary
relevance method.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>estimator <span class="classifier-delimiter">:</span> <span class="classifier">estimator object</span></dt>
<dd>An estimator object implementing <cite>fit</cite> and one of <cite>decision_function</cite>
or <cite>predict_proba</cite>.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The number of jobs to use for the computation.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">estimators_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">list of <cite>n_classes</cite> estimators</span></dt>
<dd>Estimators used for predictions.</dd>
<dt><code class="docutils literal"><span class="pre">classes_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [<cite>n_classes</cite>]</span></dt>
<dd>Class labels.</dd>
<dt><code class="docutils literal"><span class="pre">label_binarizer_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">LabelBinarizer object</span></dt>
<dd>Object used to transform multiclass labels to binary labels and
vice-versa.</dd>
<dt><code class="docutils literal"><span class="pre">multilabel_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>Whether a OneVsRestClassifier is a multilabel classifier.</dd>
</dl>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#OneVsRestClassifierScikitsLearnNode">OneVsRestClassifierScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.RadiusNeighborsClassifierScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">RadiusNeighborsClassifierScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.RadiusNeighborsClassifierScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Classifier implementing a vote among neighbors within a given radius
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.neighbors.classification.RadiusNeighborsClassifier</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>radius <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default = 1.0)</span></dt>
<dd>Range of parameter space to use by default for <code class="xref py py-meth docutils literal"><span class="pre">radius_neighbors()</span></code>
queries.</dd>
<dt>weights <span class="classifier-delimiter">:</span> <span class="classifier">str or callable</span></dt>
<dd><p class="first">weight function used in prediction.  Possible values:</p>
<ul class="simple">
<li>‘uniform’ : uniform weights.  All points in each neighborhood
are weighted equally.</li>
<li>‘distance’ : weight points by the inverse of their distance.
in this case, closer neighbors of a query point will have a
greater influence than neighbors which are further away.</li>
<li>[callable] : a user-defined function which accepts an
array of distances, and returns an array of the same shape
containing the weights.</li>
</ul>
<p class="last">Uniform weights are used by default.</p>
</dd>
<dt>algorithm <span class="classifier-delimiter">:</span> <span class="classifier">{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, optional</span></dt>
<dd><p class="first">Algorithm used to compute the nearest neighbors:</p>
<ul class="simple">
<li>‘ball_tree’ will use <code class="xref py py-class docutils literal"><span class="pre">BallTree</span></code></li>
<li>‘kd_tree’ will use <code class="xref py py-class docutils literal"><span class="pre">KDTree</span></code></li>
<li>‘brute’ will use a brute-force search.</li>
<li>‘auto’ will attempt to decide the most appropriate algorithm
based on the values passed to <code class="xref py py-meth docutils literal"><span class="pre">fit()</span></code> method.</li>
</ul>
<p class="last">Note: fitting on sparse input will override the setting of
this parameter, using brute force.</p>
</dd>
<dt>leaf_size <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default = 30)</span></dt>
<dd>Leaf size passed to BallTree or KDTree.  This can affect the
speed of the construction and query, as well as the memory
required to store the tree.  The optimal value depends on the
nature of the problem.</dd>
<dt>p <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default = 2)</span></dt>
<dd>Power parameter for the Minkowski metric. When p = 1, this is
equivalent to using manhattan_distance (l1), and euclidean_distance
(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.</dd>
<dt>metric <span class="classifier-delimiter">:</span> <span class="classifier">string or callable, default ‘minkowski’</span></dt>
<dd>the distance metric to use for the tree.  The default metric is
minkowski, and with p=2 is equivalent to the standard Euclidean
metric. See the documentation of the DistanceMetric class for a
list of available metrics.</dd>
<dt>outlier_label <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default = None)</span></dt>
<dd>Label, which is given for outlier samples (samples with no
neighbors on given radius).
If set to None, ValueError is raised, when outlier is detected.</dd>
<dt>metric_params <span class="classifier-delimiter">:</span> <span class="classifier">dict, optional (default = None)</span></dt>
<dd>Additional keyword arguments for the metric function.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
<dd>The number of parallel jobs to run for neighbors search.
<code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">RadiusNeighborsClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span> <span class="o">=</span> <span class="n">RadiusNeighborsClassifier</span><span class="p">(</span><span class="n">radius</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">RadiusNeighborsClassifier(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">neigh</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">1.5</span><span class="p">]]))</span>
<span class="go">[0]</span>
</pre></div>
</div>
<p>See also</p>
<p>KNeighborsClassifier
RadiusNeighborsRegressor
KNeighborsRegressor
NearestNeighbors</p>
<p><strong>Notes</strong></p>
<p>See <span class="xref std std-ref">Nearest Neighbors</span> in the online documentation
for a discussion of the choice of <code class="docutils literal"><span class="pre">algorithm</span></code> and <code class="docutils literal"><span class="pre">leaf_size</span></code>.</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm">https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm</a></p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes-module.html#RadiusNeighborsClassifierScikitsLearnNode">RadiusNeighborsClassifierScikitsLearnNode</a></p>
</dd></dl>

<dl class="class">
<dt id="mdp.nodes.RidgeClassifierCVScikitsLearnNode">
<em class="property">class </em><code class="descclassname">mdp.nodes.</code><code class="descname">RidgeClassifierCVScikitsLearnNode</code><a class="headerlink" href="#mdp.nodes.RidgeClassifierCVScikitsLearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Ridge classifier with built-in cross-validation.
This node has been automatically generated by wrapping the <code class="docutils literal"><span class="pre">sklearn.linear_model.ridge.RidgeClassifierCV</span></code> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikits_alg</span></code> attribute.
See glossary entry for <span class="xref std std-term">cross-validation estimator</span>.</p>
<p>By default, it performs Generalized Cross-Validation, which is a form of
efficient Leave-One-Out cross-validation. Currently, only the n_features &gt;
n_samples case is handled efficiently.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alphas <span class="classifier-delimiter">:</span> <span class="classifier">numpy array of shape [n_alphas]</span></dt>
<dd>Array of alpha values to try.
Regularization strength; must be a positive float. Regularization
improves the conditioning of the problem and reduces the variance of
the estimates. Larger values specify stronger regularization.
Alpha corresponds to <code class="docutils literal"><span class="pre">C^-1</span></code> in other linear models such as
LogisticRegression or LinearSVC.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span></dt>
<dd>This parameter is ignored when <code class="docutils literal"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<code class="xref py py-class docutils literal"><span class="pre">sklearn.preprocessing.StandardScaler</span></code> before calling <code class="docutils literal"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal"><span class="pre">normalize=False</span></code>.</dd>
<dt>scoring <span class="classifier-delimiter">:</span> <span class="classifier">string, callable or None, optional, default: None</span></dt>
<dd>A string (see model evaluation documentation) or
a scorer callable object / function with signature
<code class="docutils literal"><span class="pre">scorer(estimator,</span> <span class="pre">X,</span> <span class="pre">y)</span></code>.</dd>
<dt>cv <span class="classifier-delimiter">:</span> <span class="classifier">int, cross-validation generator or an iterable, optional</span></dt>
<dd><p class="first">Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul class="simple">
<li>None, to use the efficient Leave-One-Out cross-validation</li>
<li>integer, to specify the number of folds.</li>
<li><span class="xref std std-term">CV splitter</span>,</li>
<li>An iterable yielding (train, test) splits as arrays of indices.</li>
</ul>
<p class="last">Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
</dd>
<dt>class_weight <span class="classifier-delimiter">:</span> <span class="classifier">dict or ‘balanced’, optional</span></dt>
<dd><p class="first">Weights associated with classes in the form <code class="docutils literal"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
If not given, all classes are supposed to have weight one.</p>
<p class="last">The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
</dd>
<dt>store_cv_values <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=False</span></dt>
<dd>Flag indicating if the cross-validation values corresponding to
each alpha should be stored in the <code class="docutils literal"><span class="pre">cv_values_</span></code> attribute (see
below). This flag is only compatible with <code class="docutils literal"><span class="pre">cv=None</span></code> (i.e. using
Generalized Cross-Validation).</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">cv_values_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_samples, n_targets, n_alphas], optional</span></dt>
<dd>Cross-validation values for each alpha (if <code class="docutils literal"><span class="pre">store_cv_values=True</span></code> and
<code class="docutils literal"><span class="pre">cv=None</span></code>). After <code class="docutils literal"><span class="pre">fit()</span></code> has been called, this attribute will
contain the mean squared errors (by default) or the values of the
<code class="docutils literal"><span class="pre">{loss,score}_func</span></code> function (if provided in the constructor).</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features] or [n_targets, n_features]</span></dt>
<dd>Weight vector(s).</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float | array, shape = (n_targets,)</span></dt>
<dd>Independent term in decision function. Set to 0.0 if
<code class="docutils literal"><span class="pre">fit_intercept</span> <span class="pre">=</span> <span class="pre">False</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">alpha_</span></code> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Estimated regularization parameter</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RidgeClassifierCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">RidgeClassifierCV</span><span class="p">(</span><span class="n">alphas</span><span class="o">=</span><span class="p">[</span><span class="mf">1e-3</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">0.9630...</span>
</pre></div>
</div>
<p>See also</p>
<p>Ridge : Ridge regression
RidgeClassifier : Ridge classifier
RidgeCV : Ridge regression with built-in cross validation</p>
<p><strong>Notes</strong></p>
<p>For multi-class classification, n_class classifiers are trained in
a one-versus-all approach. Concretely, this is implemented by taking
advantage of the multi-variate response support in Ridge.</p>
<p>Full API documentation: <a class="reference external" href="https://mdp-toolkit.github.io/api/mdp.nodes.RidgeClassifierCVScikitsLearnNode-class.html">RidgeClassifierCVScikitsLearnNode</a></p>
</dd></dl>

</div>


          </div>
        </div>
      </div>

      <div class="clearer"></div>
    </div>  
<div class="footer">
    <hr />
    <table>
      <tr>
        <td class="footer-left">
           <a href="https://github.com/mdp-toolkit/mdp-toolkit">
 <img src="https://github.githubassets.com/images/modules/logos_page/GitHub-Logo.png"
      width="60" height="15" border="0"/> </a>
        </td>
        <td class="footer-center">
          Last updated on
             2020-05-17 11:52:12 PM Coordinated Universal Time
        </td>
        <td class="footer-right">
         <form class="search" action="search.html" method="get">
          <input type="submit" value="Search" />
          <input type="text" name="q" size="18" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
         </form>
        </td>
    </table>  
</div>   

  </body>
</html>